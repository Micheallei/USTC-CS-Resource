### 2.1 

主成分分析的基本流程是什么？与特征值有何关系？：  

* 基本流程
  * 对所有样本进行中心化![image-20201223232415602](D:\Typora\photos\image-20201223232415602.png)
  
  * 计算样本的协方差矩阵$XX^T$
  
    注：协方差是衡量两个变量同时变化的变化程度。协方差大于0表示x和y若一个增，另一个也增；小于0表示一个增，一个减。如果ｘ和ｙ是统计独立的，那么二者之间的协方差就是０；但是协方差是０，并不能说明ｘ和ｙ是独立的。协方差绝对值越大，两者对彼此的影响越大，反之越小。
  
  * 对协方差矩阵$XX^T$作特征值分解
  
  * 取最大的m个特征值对应的单位特征向量$w_1,w_2,...,w_m$
  
  * 输出投影矩阵$W=(w_1,w_2,...,w_m)$
  
  * 将样本点投影到选取的特征向量上。就将原始样例的n维特征变成了k维，这k维就是原始特征在k维上的投影
* 与特征值有啥关系
  * 最大特征值对应的特征向量可以最大化投影方差
  * 所以通过对协方差矩阵作特征值分解，可求得前k大的特征值对应的特征向量，从而通过特征向量的线性组合得到新的指标。

### 2.2 

如果从信息检索的视角， 可以将寻找最近邻的过程视作检索最相关的 K 个文档的过程。那么， 这一过程是否可以利用倒排索引的思路加以实现？如何实现？  

* 可以。回顾第6章讲过的向量空间模型（VSM），可以把文档与查询表示成词项的tf-idf权重向量，再计算相似度，按相似度大小排序后返回Top K的文档。
* 如何实现：引入倒排索引的思路，考虑查找样本的k个邻居时，只查找与待分类文本的词条有重叠的文本（由此减少搜索空间）
  1. 倒排索引结构包括词条数组和每个词条的文本链表，词条数组是将所有训练文本分词、特征提取后的所有特征项组成的数组，每个特征项为一个词条。每个词条的文本链表则包含含有该词条的所有文档，每一项表示为文本ID+词条在该文档中的权重(如tf-idf)。并且链表是按权重值排序过的。
  2. 把待分类的查询文本d表示为文档向量V，找到向量V中每个词条的文本链表，将这些链表合并，去除重复文本id的项，最后计算查询与这些文档的距离，返回Top-K（k最近邻）
* 再进一步，考虑通过聚类把训练文档划分为K个区域（簇），每个区域用一个中心向量表示，并建立词条到簇的倒排索引。利用倒排索引找到与查询文本有交集的簇，先计算查询文本与簇中心向量的相似度，超过阈值的簇保留，并让查询文本与簇内每个文档计算相似度，最后返回Top-K。



### 2.3 

无论是 K 最近邻分类还是 K 均值聚类，都涉及到 K 的取值问题。请简述两个问题各自选取合适 K 值的思路，并比较两者在思路上有何不同？  

* 选取K值的思路：

  * K最近邻分类：

    * K要为奇数，便于使用投票法得出分类结果。

    * K值要适中

      * 太小的话分类结果易受到噪声干扰，预测结果会对近邻的实例点非常敏感。如果邻近的实例点恰巧是噪声，预测分类就会出错。

      * 太大的话又可能导致错误涵盖其他类别的样本，导致分类模糊。

        这时与输入实例较远的（不相似的）训练实例也会对预测起作用，使预测发生错误。

    * 经验规则表明k一般低于训练样本数的平方根。

    * 实际操作里也可采用交叉验证法来选取最优的K值

  * K均值聚类：

    * K值过小，生成的簇较大，分类可能太过笼统；

    * K值过大，生成的簇过小，易过拟合、或受到噪声影响。
    
    * 尝试用不同的K值来聚类，检验各自得到聚类结果的质量，从而推测最优的K值。聚类结果的质量可以用类的平均直径来衡量。一般来说，类别数变小时，平均直径会增加；类别数变大超过某个值以后，平均直径会不变，而这个值正是最优的K值
    
    * 此外，可以考虑从降低平方误差和入手来选取K值，比如：
    
      手肘法
    
      核心指标是SSE(误差平方和)。随着聚类数k的增大，样本划分会更加精细，每个簇的聚合程度会逐渐提高，那么误差平方和SSE自然会逐渐变小。并且，当k小于真实聚类数时，由于k的增大会大幅增加每个簇的聚合程度，故SSE的下降幅度会很大，而当k到达真实聚类数时，再增加k所得到的聚合程度回报会迅速变小，所以SSE的下降幅度会骤减，然后随着k值的继续增大而趋于平缓，也就是说SSE和k的关系图是一个手肘的形状，而这个肘部对应的k值就是数据的真实聚类数。

* 两者思路的不同

  * K 最近邻分类里的K是为了找K个最相似样本，以确定待分类样本的类别。K值选取考虑的是要找到合适的K个样本，使得其中的类别众数正好是待分类样本实际的类别，K大了会包含其他类别的样本，小了则易受到噪声干扰。

  * 而K均值聚类里是为了把一堆样本划分为K个不同的簇，使得簇间相异，簇内相似。所以K值的选取，考虑的是如何尽可能按实际情况划分出样本的类别（簇）。一个好的评价指标则是SSE，比如上面的手肘法，则考虑选取使得SSE小且合适的K值。
  * 两者K的含义不同，所以选取思路就有很大区别。



### 2.4 

K-mediods 算法描述：

​	a) 首先随机选取一组聚类样本作为中心点集	
​	b) 每个中心点对应一个簇
​	c) 计算各样本到各个中心点的距离(如欧几里得距离)，将样本点放入距离中心点最短的那个簇中
​	d) 计算各簇中，据簇内各样本点距离的绝对误差最小的点，作为新的中心点
​	e) 如果新的中心点集和原中心点集相同，算法中止；如果新的中心点集与原中心点集不完全相同，返回 b)
试着:

**注意到k-means的质心是各个样本点的平均，可能是样本点中不存在的点。k-medoids的质心一定是某个样本点的值。**

1. 阐述 K-mediods 算法和 K-means 算法相同的缺陷
   * K的含义相同，都需要事先设定簇数目
   * 都可能陷入局部最优解的困境
   * 都是无监督算法，结果不一定具有可解释性
   * 初始聚类中心的选择对聚类结果都有较大的影响
2. 阐述 K-mediods 算法相比于 K-means 算法的优势
   * k-medoids对噪声鲁棒性比较好。比如当一个cluster样本点只有少数几个，如（1,1）（1,2）（2,1）（100,100）。其中（100,100）是噪声。如果按照k-means质心大致会处在（1,1）（100,100）中间，这不是我们期望的，也说明K-means对噪声点敏感。这时k-medoids就可以避免这种情况，他会在（1,1）（1,2）（2,1）（100,100）中选出一个样本点使cluster的绝对误差最小，计算可知一定会在前三个点中选取。
   * K-Means可能导致空簇的出现，但是K-mediods每次都选取的某个样本点作为质心，所以不会出现空簇。
   * K-Means只适用于数值属性聚类(计算均值有意义)，而K-mediods可适用类别（categorical）类型的特征
3. 阐述 K-mediods 算法相比于 K-means 算法的不足  
   
   * k-medoids的运行速度较慢，计算质心的步骤时间复杂度是$O(n^2)$，因为必须计算任意两点之间的距离。而k-means只需平均即可。
   
     当样本数过大时，k-medoids速度太慢，并且此时少数几个噪声对k-means质心影响可以忽略，所以后者应用范围比k-medoids广。
   
     







### 参考文献

* https://www.cnblogs.com/190260995xixi/p/5954921.html
* https://wenku.baidu.com/view/f6d50a22aaea998fcc220e0c.html
* https://zhuanlan.zhihu.com/p/264367144