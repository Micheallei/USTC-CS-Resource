* 每个实验一个月时间完成

* 作业和报告都通过电子版提交

* 课程要解决的问题：检索、抽取、挖掘

  考试只考3-7节，12-16节

* 鼓励实验两人一组

* 无期中考试

### 考试：

题目：~~ch3:33，36； Ch4:15，37，76 ， CH5:27（存在一个非负向量的设定？）；CH5:35~~

~~CH6：https://blog.csdn.net/u014731752/article/details/45224761（PageRank+ppt48）; CH7:30~~

~~CH11：18 ，22~~  ， [FP-Growth][https://blog.csdn.net/baixiangxue/article/details/80335469#:~:text=FP-growth%20%28Frequent,Pattern%20Tree%2C%20%E9%A2%91%E7%B9%81%E6%A8%A1%E5%BC%8F%E6%A0%91%29%2C%E6%98%AF%E9%9F%A9%E5%AE%B6%E7%82%9C%E8%80%81%E5%B8%88%E6%8F%90%E5%87%BA%E7%9A%84%E6%8C%96%E6%8E%98%E9%A2%91%E7%B9%81%E9%A1%B9%E9%9B%86%E7%9A%84%E6%96%B9%E6%B3%95%EF%BC%8C%E6%98%AF%E5%B0%86%E6%95%B0%E6%8D%AE%E9%9B%86%E5%AD%98%E5%82%A8%E5%9C%A8%E4%B8%80%E4%B8%AA%E7%89%B9%E5%AE%9A%E7%9A%84%E7%A7%B0%E4%BD%9CFP%E6%A0%91%E7%9A%84%E7%BB%93%E6%9E%84%E4%B9%8B%E5%90%8E%E5%8F%91%E7%8E%B0%E9%A2%91%E7%B9%81%E9%A1%B9%E9%9B%86%E6%88%96%E9%A2%91%E7%B9%81%E9%A1%B9%E5%AF%B9%EF%BC%8C%E5%8D%B3%E5%B8%B8%E5%9C%A8%E4%B8%80%E5%9D%97%E5%87%BA%E7%8E%B0%E7%9A%84%E5%85%83%E7%B4%A0%E9%A1%B9%E7%9A%84%E9%9B%86%E5%90%88FP%E6%A0%91%E3%80%82]

~~CH12:18 。CH13:27，53，63~~

* 红色+黄颜料：一级目录
* 黑色加黄颜料：二级目录
* 蓝色加黄颜料：三级目录

问题：

* 双向匹配的两种策略(去年期末)：直接合并；取分词结果最短
* 倒排索引优化里，与、或运算的顺序，ppt35(加上非呢)
* 查询的tf-idf怎么算
* 个性化Pagerank，主题敏感Pagerank，Hilltop
* HITS算法的集合是所有边？
* P-R和ROC曲线里的阈值是？返回文档数？
* 实体识别和分词区别？ 为何多一项任务
* PCA实例？ppt62，63
* 为何要做数据离散？
* ch15 p44: N=1不行吧？

## 第一节 绪论

* 解决问题的过程
  * 信息搜索(从海量数据找到可能有用的文档)
  * 信息提取(目标文档中提取和关联价值信息)
  * 信息挖掘(通过挖掘被提取信息完成最终决策)

#### 信息检索概述

* 信息检索：给定用户需求，从数据库中寻找并反馈相关文档

  * Query：查询需求
  * Corpus：待检索的数据库
  * Relevance：文档满足查询需求

* 信息检索的发展史：向量空间算法(鼻祖)

* 信息检索vs数据库

  * 数据库属于标准结构化数据，而信息检索往往面临文本、图像、视频等非结构化或半结构化数据
  * 数据库依赖精确查询条件，但信息检索的查询词更加自由，匹配也相对粗糙
  * 数据库对排序并不强调，但信息检索效果关键在于相关性排序

* 信息检索应用场景

  * 通用搜索：一般web搜索
  * 垂直搜索：限定在特定主题和领域
  * 内部搜索：内网甚至个人电脑中的搜索引擎
  * P2P搜索：由节点构成的网络信息中寻找

* 信息检索的基础问题：

  1. 查询理解：理解用户查询动机

  2. 相关性计算：基于相关性的排序决定了文档呈现顺序

     不同类型的检索模型导致了不同假设的相关性计算

  3. 效果评估：信息检索的质量取决于反馈文档与用户期望的匹配程度

  4. 检索性能

#### 数据挖掘概述

* 概念：从海量数据中提取或挖掘潜在的知识和规律，用于支持当前的判断或未来的决策
  * 数据准备：筛选、清理并整合有待挖掘的数据
  * 数据建模
  * 知识表示
* 数据挖掘的发展历史
* 数据挖掘的基本方法
  * 分类
  * 聚类
  * 关联规则
  * 离群检测
* 数据挖掘典型应用场景
  * 推荐系统
  * 社会网络分析



* 信息搜索：从海量数据中找到有用的文档
* 信息提取：从目标文档中提取和关联价值信息
  * 关联面临着挑战：重名合并
  * 利用知识图谱进行推理：寻找关联性
* 信息挖掘：通过挖掘被提取信息完成最终决策
* 应用
  * 文档搜索
  * 多模态搜索
  * 面向知识的搜索：人不满足于单纯呈现原始的文档，而需要更加精炼的知识表达和更加直观的需求解决
  * 基于分析的搜索：需要从信息中心分析和总结规律
* 课程框架
  * Web信息如何获取-网络爬虫
  * Web信息如何整理和存储-文本处理、索引
  * Web信息如何搜索-查询、排序、评估
  * 提炼价值信息与知识-信息抽取
  * 分析信息和。。。
* **橙色不考**，**绿色会考以及作业**

#### Web起源

* 阿帕网
* 超文本概念（分叉、允许读者作出选择->无限延伸、扩展的非线性文本），超文本传输协议，超文本标记语言
  * B站的互动视频
* 因特网起源
* 互联网的原型**APRANet**-军事连接
  * 推动了TCP/IPxieyi
* NSFNet广域网取代了APRANet
* 1987年，中国与互联网
* 万维网
* Web特点
  * 五个特点
    * 图形化
    * 平台无关
    * 分布式
    * 动态性
  * 信息流视角Web1.0：分为信息提供者、整合者、消费者
    * 门户网站，指通向某类综合性互联网信息资源并提供有关信息服务的应用系统
  * 信息流视角Web2.0：每个人既是信息的生产者也是消费者
    * 推特、微博、博客、油管、bili
    * 出现了信息可信度的问题
  * Web3.0会是怎么样？：更加个性化、智能化、跨越平台与站点的信息的大一统
    * 前两点在前进，大一统在倒退
* 搜索引擎的发展
  * 起源，原型：黄页
  * 黄页的Web化
  * Archie：面向FTP
  * Wanderer：面对互联网上的服务器
    * AliWeb与阿里巴巴无关
  * Yahoo：层次分类，分类目录
  * Lycos:关键词，关键词距离度量
  * Infoseek
  * Metacrawler：整合了其他搜索引擎
  * Altavista：支持自然语言和高级语法
  * Google
    * 打破了文本匹配的概念
    * PageRank技术：衡量网页的权威度
      * 权威的网页一般互联度比较高
  * 天网：北大开发
  * 百度
  * Wolfram Alpha：直接向用户返回答案，而不是返回网页链接
    * 从提供信息到提供知识的转变
* 挑战
  * 来自数据
    * 数据挑战：绝对增长和相对增速
    * 异构数据：网页结构、数据模态的不同，搜索引擎无法搜索视频
    * 数据质量：Web中包含大量未经编辑处理或者权威确认的信息，可能导致错误、无效或者误导
    * 数据不稳定性：网站和文档快速地添加和消亡，导致大量死链的存在
  * 来自用户
    * 查询需求的表达
    * 知识需求和直观表达：用户希望直接从搜索引擎获得答案，而不是阅读文档
    * 个性化需求：大众化的信息需求被个性化、差异化的系信息需求所取代
      * cookie机制：根据历史来判断，但涉及到隐私问题
      * 基于隐私保护的数据挖掘
  * 来自利益
    * SEO（搜索引擎优化）对搜索的干扰
      * 可以提升网站效率，也可能因为滥用搜索算法影响正常使用
    * 竞价排名对于搜索的干扰：基于广告改变排序，对使用者产生误导

#### 信息检索

* 基本含义
  * Query：用户需求
  * Corpus：数据库
  * Relevance：与需求的关联度
* 发展历史
  * 向量空间
  * 标准测评

* 信息检索 vs 数据库
  * 数据的结构化程度
    * 数据库属于标准的结构化数据，而信息检索往往面临文本、图像、视频等非结构化和半结构化数据
  * 查询条件
    * 数据库依赖精确的查询条件，而信息检索的查询词往往更加自由，匹配也相对粗疏
  * 排序方面
    * 数据库对排序不强调，而信息检索的效果关键在于**相关性排序**
* 应用场景
  * 通用搜索
  * 垂直搜索：特定主题和领域
  * 内部搜索：内部网络甚至个人电脑的搜索引擎
  * P2P搜索：节点构成的网络寻找信息，没有集中式控制
* 信息检索的基础问题
  * 查询理解
  * 相关性计算
    * 单纯依赖查询和文档匹配效果不一定好
    * 检索模型
  * 效果评估
    * 准确率、召回率、F值
  * 检索性能
    * 对检索条件和规则进行模块化，效果与效率的均衡

#### 数据挖掘-未来or预测

* 基本含义：从海量数据中提取或。。。（基于信息的规律，产生未来的行为
* KDD：从数据库中发现知识
* 数据挖掘与数据库
  * 前者是未来，后者是现在
* 数据库vs信息检索vs数据挖掘：以一个超市为例
  * 数据库：进货单、价目表
  * 信息检索：最符合顾客需求的是?
  * 数据挖掘：啤酒和尿布-年轻父亲

* 基本方法
  * 分类和聚类
  * 关联规则和离群检测
* 典型应用场景
  * 推荐系统（**实验**
  * 社会网络分析



## 第二节 网络爬虫

##### 网络爬虫定义与需求

* 万维网：文档的网络，以网页为节点、超链接为有向边

* 网页间的互联互通：以科大主页为种子节点 -》理论学习网 -》求是网 -》 央视网 -》前程无忧

  评价网页权威性：两种指标

  * 信息集散地（导向有价值的网页）
  * 本身有很权威的信息

* 爬虫的任务定义：种子节点集合开始，从Web中寻找且下载网页，获取排序需要的相关信息，并且剔除低质量网页。

  常见爬虫类型

  1. 通用网络爬虫：目标为全网Web信息，主要为门户站点搜索引擎和大型Web服务提供商采集数据
  2. 聚焦网络爬虫：选择性爬取与预定主题相关的内容
  3. 增量式网络爬虫：对已下载内容进行增量式更新并只爬取更新内容
     * 周期性更新
     * 周期不定：爬虫周期频率高，还要判断哪部分更新
  4. 深度网络爬虫

* 爬虫的基本用途：数据展示、引擎优化（SEO）、数据分析、特定应用

* 爬虫的基本流程

  1. 爬虫调度端
  2. URL管理器(URL队列管理，防止重复爬取)->网页下载器->网页解析器(提取价值数据、提取新待爬的URL交给URL管理器)

  3. ws

* 网络爬虫的性能衡量

  * 数量覆盖率：全

    * 孤立节点的存在：遍历完备性无法保证
    * 部分“偏远”节点难以遍历（距离中心节点太远）
    * 网络结构动态演化
    * IP/Robots等条件约束

  * 质量覆盖率：好

    * 衡量网页重要性：出入度、基于结构(PageRank/HITS)、信息密度衡量

    * 时效性

* 爬虫的主要需求

  * 速度

  * 可扩展性：多爬虫机制 -》分布式

    * 如何管理多个并发连接
    * 如何任务分配：不同爬虫（再比如有爬虫宕机，如何重新分配？）
    * 过多硬件并行好处并不大：抓取的性能瓶颈出现在通讯和硬盘读写（即爬虫和中心通讯这一块成为了瓶颈，而非爬取网页本身）

  * 友好性

    * 不能显著影响被爬取服务器性能
    * 有些服务器可能不希望被爬取：Robots exclusion

  * 健壮性

    常见服务器陷阱

    * 病态HTML文件（如含大量null）
    * 误导爬虫的网站：CGI程序生成的无限个网页、自动创建很深的路径、HTTP服务器中的路径重映射

  * 时新性

    * 网页年龄
    * 过度重视会增加搜索引擎负担

##### 爬虫基本要素

* 设计的协议和要素：HTML/HTTP、DNS/URL、
* 如何表示与获取网页中的链接结构？-》看HTML源码
* HTML基本概念
  * 基本组成：标记（Tags）+文本内容（Text）
  * 标记的作用：说明网页元数据(<head>部分)；说明文本内容的布局和字体、字号等信息；图片视频超链接（后两者均在body部分）
* HTTP基本概念、应用实例
* URL基本概念
* URL与IP地址的对应关系
  * 一对一
  * 一对多：多个URL映射到同一IP
  * 多对一
* URL链接提取与规范化
  * 目标：得到网页中所含URL的标准型
  * URL不规范现象

* DNS的基本概念和作用

* 提升DNS性能的方法：实际也是对三个方面做权衡

  * 并行

  * 缓存：缓存DNS，下次访问就调访存，不需解析

  * 预取：不用也暂时先缓存（空闲时尽早将主机名投给DNS系统）

    基本步骤：分析刚得到的网页、从<href>属性提取主机名、向缓存服务器提交DNS解析请求、结果放在DNS缓存(可能用得上、可能用不上)

* 站点地图

  * Sitemap：允许协议，为爬虫指明抓取的建议
  * Robots：排斥协议（不允许爬）



##### 面向API的新爬虫任务

* API的基本概念
  * 基于API的数据爬取一般流程
    1. 获取API Key
    2. 确定需要爬取的数据类型
    3. 调用API获取资源ID列表
    4. 针对每个资源ID，调用API获取详细信息
    5. 存储所获得的的结构化数据
  * Token的概念、作用与取得方式
* 结构化数据反馈
  * XML
  * JSON



##### 常见爬虫算法

* n-gram：把文本中每n个词抽成一个词组（PPT例子）
* 广度优先（队列）、深度优先（栈）
* 从遍历到对网页本身重要性的要求



##### 常见反爬虫机制与应对策略

* 常见反爬虫策略：

  * User Agent（一般爬虫请求U-A为空）

    应对：利用python的库允许用户自定义U-A信息

  * IP/账号访问次数/频率

    应对：构造IP代理池、注册多个账号

  * 验证码

  * 动态网页

  * 蜜罐技术

  * 其他反爬虫策略：不同网页结构、多模态呈现(文字转为图像、视频)

    应对策略：OCR、语音识别、图像/视频标签技术

##### 分布式爬虫的基本任务

* M个节点同时执行搜集，如何有效把N个网站的搜集任务分配到M个机器（任务分配得均匀）
* 均匀分配的实现：hash函数（均匀分布）-》将值映射到hash buckets -》将Item放入不同Buckets
* 新问题：节点增、崩溃、重新加入等事件；机器数目变化；Hash函数更改导致重新分配
* 一致性Hash（不是可持续性Hashing）





## 第三节 网页文字处理

* 本节要解决的问题：如何处理网页中的文字信息

#### 问题背景

* 有效解读文档元素，是实现文档检索的第一步

* 文本处理的概念与目的
  * 信息检索的基本组件，为后续应用提供支持
    * 将原始文档转化为词项，以建立索引
    * 使面向查询条件的文档匹配成为可能
  * 文档处理与查询解析相辅相成

#### 词条化处理

##### 分词的概念与挑战

* **词条化(Tokenization)：将给定的字符序列拆分成一系列子序列的过程**

  每个字序列称为一个词条(Token)

* **英文分词的挑战**
  
  * 词与词组的切分
  * 标点符号影响
  * 专有名词的拆分
  
* 中文分词的挑战
  
  * 中文里概念：语素、字、词
  
  * 最大挑战：没有显示分隔符
  
  * 虚词
  
  * 常见的三类中文分词歧义
    * 交集型歧义（交叉歧义）
    * 组合型歧义
    * 真歧义
  
  * 未登录词的影响:
  
    人名、地名、机构名、商品名等，专业领域的术语、新词、变异词语
  
  * 其他类型的字符序列
    * 专业术语中文字与符号结合的部分：C++
    * 新类型的字符序列：快递单号
    * 多种语言混杂的表达方式：awsl

##### 常见分词方法

* 基于字符匹配的方法(机械分词方法)：按一定策略与一个机器词典中的词条进行匹配

  * 最大匹配分词法

    * 正向最大匹配分词(FMM)：

      从左至右尽可能查找最长的词，直到不构成词。（错误率较高）

    * 反向最大匹配分词(RMM)

      从右至左尽可能查找最长的词，直到不构成词（比FMM好）

    * 双向最大匹配分词(BM:FMM+RMM)

      * 综合比较FMM与RMM两法切分效果，通过一些方法选择正确的切分：直接合并/选词数最少

      * 有助于识别分词中的交叉歧义

      **双向匹配的两种策略(去年期末)**

    * 基于匹配分词的一般模型

      ASM（d,a,m)

      * d:匹配方向(+,-)
      * a:每次匹配失败后增/减字符数(+,-)
      * m:最大(+)/最小(-)匹配表示

      注：最大匹配更实用(最小匹配过于琐碎)

  * 最少切分分词法(最短路径分词)

    * 使句子中切出的词目数最少

    * 等价于在有向图中搜索最短路径的问题：字为节点，词成边 （边权重可根据词频决定）

    * 拓展方法：

      N-最短路径方法

      保留N条最短路径，以提供更多分词方案。最后人工复检确定

      24页图中前驱啥意思？（见ipad笔记）

  * **基于匹配分词方法的优劣**

    * 优点：效率高、直观性好
    * 缺点：对词典的依赖性
      * 维护高质量词典需要极大的开支
      * 永远难以应对新生词汇
      * 词汇频率/重要性往往对结果不产生影响

* 基于统计的分词方法

  * 基于深度学习的分词方法

  * 概述

    * 无词典，从海量文档中找答案

    * 若某两个词的组合在统计上出现几率很大，人为分词正确

    * 统计分词的形式化表达（ppt27）:要找到估计概率最大所对应的词串

    * **统计分词的一般化过程**

      1. 建立统计语言模型
      2. 对句子进行分词
      3. 计算概率最大的分词结果

      注：理论上不需词典，但实际第2步用机械分词方法提高效率

  * N-gram模型及其变形

    * N-gram模型：由N个单词组成的集合，各单词具有先后顺序

    * N-gram模型的马尔可夫假设：当前状态出现的概率仅同过去有限的历史状态有关，而与其他状态无关；分词上即文本第N个词出现的概率仅仅依赖于它前面的N-1个词

      常见N-gram模型：

      1. N=1，一元文法模型（最大概率模型）
      2. N=2，Bigram模型
      3. N=3，Trigram模型

    * N-gram模型的概率估计

      基于最大似然估计![image-20201023205133759](D:\Typora\photos\image-20201023205133759.png)

    * N-gram模型的分词过程

      ![image-20201023205323566](D:\Typora\photos\image-20201023205323566.png)

    * Bigram模型实例：ppt33-34

    * 特殊形式：一元文法模型(N=1)，词与词间独立

  * 基于统计文法模型的优劣

    * 优点：减轻了对于词典的依赖性
      * 但并未消除，取决于性能与效率的平衡
    * 缺点：依赖已有数据中词频的统计，对于新生词汇或专业词汇不友好
      * 冷门领域稀有词汇难以准确划分
      * 易受数据集先验偏差影响

  * HMM与CRF：基于序列标注的分词方法

    * 统计分词的进一步抽象

    * 四类标注：B（词的开始）、M（词的中间）、E（结束）、S（单字词）

    * 隐马尔科夫模型（HMM）

      * 基本思想：根据观测值序列(字符序列)，找到真正的隐藏状态值序列(标签序列)

      * 5个核心要素：

        * 两个集合：观测值集合、隐藏状态集合(BEMS)

        * 三个矩阵

          1. 初始状态概率矩阵：第一个字属于某种隐含状态(BMES)的概率

          2. 隐含状态转移概率矩阵：各种隐含状态(标签)之间的转移概率
          3. 观测状态概率矩阵：从隐含状态到观测值的转移概率

      * 中文分词问题在HMM下的表述：![image-20201023210603230](D:\Typora\photos\image-20201023210603230.png)

      * HMM的两个基本假设：

        * 齐次假设：当前隐藏状态只与上一个状态有关（但实际上可能只看前一个状态是不够的，需要看多个状态）![image-20201023210643278](D:\Typora\photos\image-20201023210643278.png)
        * 观测独立性假设：观测值之间互相独立，只与生成它的状态有关![image-20201023210712561](D:\Typora\photos\image-20201023210712561.png)

      * 综上，表述变为![image-20201023210919442](D:\Typora\photos\image-20201023210919442.png)

        用维特比算法求解

        1. 初始化
        2. 递归
        3. 终止
        4. 回溯

    * 条件随机场模型(CRF)（相关工具包：ppt43）

      * 隐马尔可夫模型的独立性假设难以描述字词之间的复杂关联
      * CRF
        * 具有表达长距离依赖性和交叠性特征的能力
        * 所有特征可以进行全局归一化，能够求得全局的最优解
      * 比如一句话开头的词对结尾的词也有影响，CRF就能处理这种情况

    * 长短时记忆模型(LSTM)

      * 传统神经网络中：每层内部的节点间无连接，所以无法利用上下文关系
    * 循环神经网络（RNN）可利用上下文关系。但对信息依赖的学习能力有限(较长上下文关系时不行)
    
    相比于RNN，LSTM有多个门(四层神经网络，见ppt图)，拥有了增加和减少信息的能力
    
      ![image-20201010173647261](D:\Typora\photos\image-20201010173647261.png)
    
      中间的绿色块里有4个黄色的门，用来筛选数据
    
      * 新增了保存长期信息的单元状态，以及控制保存、输出长期状态和输入顺势状态的“门"
      * LSTM与CRF的结合：
        * 传统思路：LSTM+SoftMax分类
          * 忽略了预测序列的标签之间的关联性
          * 可能导致错误标签序列的出现
        * 结合LSTM和CRF：有效利用句子级别的标记信息,输出的将不再是相互独立的标签，而是最佳且合理的标签序列

##### 常用分词工具

（每页附带链接）

* 中文分词工具：
  * NLPIR-ICTCLAS：基于HMM技术
  * 结巴分词：
    * 三种分词模式，支持自定义词典
    * 基于HMM，有python库支持
  * HanLP：完全开源，语料时新，可自定义
  * THULAC
  * PKUSeg：支持多领域分词，支持用全新的标注数据来训练模型

* 英文分词工具：
  * Stanford NLP
    * 支持多种语言的完整文本分析管道（包括分词、词性标注、词形归并、依存关系解析等任务）

* 分词可能带来的隐患

  * 分词带来的大量低频词，导致严重数据稀疏
  * 越来越多的未登录词(OOV)
  * 分词中难免的错漏将导致额外的噪声
  * 深度学习发展，分词受益愈发有限(神经网络黑盒子丢进去就有很好效果，虽然解释性差，但效果好，所以也就不需要分词了)

#### 停用词处理

* 概念：**指文档中频繁出现或对实际语义影响不大的词语**

  如 The,of,的，是；数字、副词等

* 为何要去除停用词

  * 重复率高，会造成索引中的倒排表很长，影响查询性能
  * 对最后结果的排序没啥贡献

* **停用词类型与识别**

  * 停用词的设置与语料库的性质有关
    * 通用停用词表
    * 专用停用词表(特定学科/领域)
  * 常用的停用词识别方法
    * 文本频率、词频统计、熵计算
    * 结合统计与句法或内容分析
  * 常用的停用词表：ppt54，如NLTK停用词表

* 去除停用词可能导致的隐患

  * 某些停用词在特定场景下有意义
  * 有些停用词的组合有意义

* 未来停用词的使用趋势

  * 逐渐减少使用
  * 现代搜索引擎更关注利用语言的统计特性来处理常见词问题
    * 压缩技术：降低停用词表的存储开支
    * 引入词项权重，将高频词的影响降至最低
    * 索引去除技术，将低于权重的词项排除

#### 规范化处理

##### 归一化处理

* 文本规范化的意义

  * 文本处理的主要目标在于优化查询词与索引词之间的匹配

    但两方的文本内容都可能出现各种干扰

    * 大小写、缩写、标点
    * 不同时态的词形
    * 同义词/相关词
    * 用户个性化表述方式，如方言

  * 规范化的目的在于尽量保证索引词项符合用户查询输入

* 规范化需要考虑的问题

  * 大小写、标点、缩写等规则化处理
  * 词根化处理：词干提取与词形还原
  * 拼写错误检查与修正
  * 同义词/相关词识别与处理
  * 其他类型的文本规范化问题
    * 特殊文本形式：9月19日与9/19
    * 跨语言问题

* 词根化处理

  * **词根化（英文才需）：指还原词语的特殊形式的过程**

    可有效降低词项的数量并减少歧义

  * 词干提取：去掉单词前后缀，获得词根的过程（常见前后词缀：复数、过去分词、进行时等）

  * 词形还原：单词转变成最基本形态(基于词典)
  
    如is,are,been变为be
  
  * 词干提取与词形还原异同
  
    * 相同点：
      1. 目标一致
      2. 结果交叠
      3. 方法类似：主流方法均是利用语言中的规则或词典实现
    * 不同点
      1. 原理上：词干提取：缩减； 词形还原：转变
      2. 复杂性上：词形还原还需考虑词缀转化、词性识别等
      3. 实现上：词干提取：利用规则变化；词形还原：更依赖词典
      4. 结果上：词干提取：不一定得到完整单词；词形还原：完整单词
  
  * 基本词干提取方法：（网址）
  
    Porter Stemming
  
    * 基本流程

##### 错误拼写检查

* 采用基于词典或编辑距离的方式进行检查和校对
* 编辑距离
  * 指两个字符串之间转换所需最少编辑操作步数
    * 一步编辑操作：替换、插入、删除一个字符
* 不必要的拼写错误处理将影响用户体验

##### 同义词/相关词处理

* 比词根化和拼写错误检查更难处理，借助人工维护的知识库来获取各种词项间的关系
* 常见的词与词间关系
  * 同义词
  * Is a关系
  * Is-part-of关系
  * 反义词
* 人工维护的知识库
  * **WordNet（英文）**
  * 中文：HowNet、Chinese WordNet、大词林
* 相关应用：基于同义词/相关词，拓展查询条件





## 第四节 网页索引

#### 上节回顾

* 文档处理完整流程

**本节要解决的问题**：预处理后的词项，如何有效支持查询任务？

#### 问题背景

* 网页索引的目的：索引的质量，关系着整个搜索引擎系统的精度与效率（查询速度、存储空间）

  **关键词查询-》索引与匹配-》返回匹配文档**

* **索引词项的选择**

  * 选择范围：
    * 人工索引：质量好，查询有针对性，更准确反应文档核心内容，但不够全面，依赖人力(大规模文档不行)
    * 自动索引：依赖算法自动分析文档
      * 部分索引：如标题、摘要、关键词等
      * 全文索引：对文档中所有词都进行索引(普遍用)
      * 算法提取索引词，高效全面，但存在误导，或限于算法效果而出错
  * 选择原则
    * 理想状态：表达文档核心内容的语义单位
    * 依赖文本处理技术进行处理，如分词、规范化等

#### 布尔检索

* 概念：
  * 文档被表示为关键词的集合
  * 查询式被表示为关键词的布尔组合:与或非
  * **相关度计算**
    * 当且仅当文档满足布尔查询式时才会被检索出来（完全、准确匹配）
    * 检索策略是二值匹配
* 优点：
  * 查询简单，易于理解
  * 使用布尔表达式，可以很方便的控制查询结果
  * 可通过扩展来包含更多功能（如文档类型、文档领域、词频、位置信息等）
* 缺点
  * 功能弱，不支持部分匹配
  * 所有匹配文档均返回，不考虑权重和排序
  * 很难进行自动的相关性反馈(如根据用户点击量来进行排序)
* 布尔检索的应用场景
  * 利用与或非操作符将词项连接起来，操作符可连续使用
  * 如OR：可以利用同义词表自动扩展
  * 实例：
    1. 笨方法：从头到尾扫描所有剧本
       * 缺陷：速度慢(尤其对大型文档)
       * NOT操作的处理：需遍历全文
       * 不支持检索结果的排序
    2. 关联矩阵
       * 是非线性的扫描方式，需事先给文档建立索引（尤其是需要常作查询任务时，一次建表标注以后就方便了）
       * 实例：转化为行向量运算（ppt15）
       * 缺陷：
         * 文档集很大：矩阵规模大：M(文档数)*N(词汇表大小)
         * 高度稀疏的矩阵(1的值很少)

#### 倒排索引（速度）

* 概念与意义

  * 是基于词项的基础文本索引

  * **包括两部分结构**

    * 词汇表：词项的集合
    * 倒排表：文档ID列表，列举词项在哪些文档出现

  * 正排索引VS倒排索引![image-20201024094907245](D:\Typora\photos\image-20201024094907245.png)

    索引有一定规模时，可压低其更新频率来减少维护开销，另一方面，搜索频率远高于文档更新频率，所以倒排更优

##### 倒排表的构建与查询

* **建立：需建立索引的文档-》词条化-》规范化-》倒排索引建立**
* 实例：建立倒排表的流程
  1. 检索每篇文档，获得<词项，文档ID>对，写入临时索引
  2. 对临时索引中的词项进行排序
  3. 遍历临时索引，对相同词项的文档ID进行合并
* 查询实例(ppt26,27)：本质是倒排记录表的“合并过程”，复杂度O(x+y)

* 动态索引问题

  * 前面假设文档集是静态的，但实际上

    * 文档会不断加入
    * 文档也会被删除或修改

    此时需要

    * 对已在词典中的词项更新倒排记录(尤其是文档的删除)

    * 将新的词项加入词典

  * 最简单方法：主从索引

    * 维护一个大的主索引
    * 新文档信息存储在一个小的辅助索引中
    * 检索时同时遍历两个索引，并进行合并
    * 如需删除操作，可利用一个新的无效位向量，用其过滤结果（比如对删除的词也用一个倒排表记录该词在哪个文档中的记录已无效）

    * 定期将辅助索引合并到主索引中

    * 问题：

      1. 频繁合并导致大开销
      2. 合并过程效率很低
         * 考虑每个词项的倒排表单独形成一个文件，这时合并较为简单，但此时文件读写开销大（压力转到O/S上）

      现实里考虑折中方案

      * 对大索引记录表切分，小的索引记录表合并

  * 索引拆分：将热门词的Web文档放在缓存server中

##### 倒排表的优化与扩展

倒排索引的优化问题

* 问题

  * 含NOT操作的布尔查询，是否仍O(x+y)复杂度？
  * 任意组合的布尔查询呢？可否在线性时间完成？

  核心：提升倒排索引的效率

* **处理查询的最佳顺序**

  * 使用AND连接的查询，本质是倒排表的合并操作(O(x+y))

    * 按文档频率的顺序进行处理：

      先处理文档频率小的，再处理大的

  * 更一般的：任意组合的布尔查询(一般是AND+OR)

    * 按文档频率的顺序进行处理
      1. 获得所有词项的文档频率
      2. 保守估计每个OR操作后的结果大小(考虑x+y最坏情况)
      3. 按结果从小到大的顺序执行AND

  * 合并操作的优化：超越O(x+y)

    * **设置跳表指针实现快速合并**
1. 如何快速合并：ppt37
   
2. 何种策略设置跳表指针（(ppt38，39)）
   
   * 指针数与存储空间权衡（p38）
     
   * 启发式策略及其缺陷（p39）

倒排表的扩展性问题

* 除了文档ID，在倒排表中加入**词项频率、词项类型**等

* 除了基本的词查询：词组查询、词项临近、文档区域

* 短词查询的需求：stanford university看为一个整体

  * 双引号的短语查询

  * 隐式的短语查询(无双引号)

  * 解决方案

    1. 二元词索引

       * 文档中每个连续词对看成一个短语，将这些二元词对作为词典中的词项
       * 以此构建面向二元词的倒排表并处理两个词构成的短语查询(可扩展至多个词)
       * 更长的短语查询：
         * 拆分成多个二元词短查询来处理（ppt44）
         * 或采用多元词索引来处理

    2. 位置信息索引

       * n元词索引最大问题：词汇表迅速增长

       * 考虑记录词项的同时，记录其在文档中出现的位置![image-20201024104456349](D:\Typora\photos\image-20201024104456349.png)

         对于短语查询，仍采用AND连接每个词，查找符合的文档（但不是简单判断两词是否出现在同一文档，还要检查其出现的位置情况是否符合要求）

       * 代价：存储太大

       * 好处：

         * 能知道短语是否出现并确定位置
         * 能用于领近搜索(如要找的两个词间间隔k个词)

#### 索引存储（空间）

* 词典与倒排表一起存储

  * 便于同时读取，但文档规模大会导致索引过大，影响性能

* 两者分开存储：

  * 存为不同文件，通过页指针关联（倒排表文件可分布存储）
  * 优点：性能大幅提升
    * 词典可以常驻内存
    * 可支持并行、分布式查询

* 常见的词汇表存储结构：

  * 顺序存储

    * 词汇表按字典序排列，查找采用二分查找法
    * 优点：简单粗暴
    * 缺点：效率一般
      * 索引构建效率(文档插入需反复调用查找和排序)
      * 索引检索效率(二分查找O(logN))

  * 哈希存储

    * 对词汇表进行哈希，把词项散列成一个整数，用该整数作为词项访问地址
    * 优点：实现简单、检索速度快，理论时间O(1)
    * 缺点：
      * 冲突过多时效率下降
      * 关键要找到一个好的散列函数(词项数量太多，易冲突，而且会有新词加入)

  * B+-树：多叉平衡有序树

    * 优点：性能好且稳定，查找次数=层数

    * 缺点：

      * 维护代价高

      * 实现相对复杂

  * Trie树

    * 又称前缀树，利用字符串公共前缀来节约存储空间
    * 是快速检索的多叉树结构
    * ppt54
    * 优点：
      * 查找效率高，与词表长度无关
      * 索引的插入，合并速度快
    * 缺点：空间大，“空间换时间”，叶子节点数=词项数
      * 若是完全m叉数，节点数指数级增长
      * Trie树虽不是完全m叉树，但尤其当词项公共前缀少时，空间需求很大

#### 索引压缩（空间）

* 意义

  * 节省磁盘空间，提高效率(内存利用率或数据传输速度)

    前提：快速的解压缩算法

    * 对词典：压缩的足够小，词典可直接放内存，效率高
    * 对倒排记录表：
      * 减少所需磁盘空间，更多移入内存
      * 减少从磁盘读取倒排表所需时间

* 文档集中词项的分布情况

  * Zipf定律:排名第i多的词项的文档集频率与1/i成正比（即高频词项很少，大部分是生僻词项）

* 两种索引压缩策略

  * 从词典和倒排表两个维度入手

  * 词典的基本存储方式：定长存储

    词项本身20B，文档频率和倒排表指针各4Byte，共28B

    词项为20B定长:极大空间浪费

    * 压缩词项列表：将词典视为一个字符串

      * 词项间用指针分割![image-20201024112816650](D:\Typora\photos\image-20201024112816650.png)
      * 可压缩至一个词项平均占19Byte

    * 按块存储

      * 单一字符串在词项指针上占用较多空间

      * 为每k个词项存储一个指针，但需为每个词项额外一个字节表示词项长度

      * k的选取:

        二分法只能在块外进行，块内是线性查找

        k太大，线性查找部分增多，效率更低

    * 前端编码
      * 按照词典顺序排列的连续词项间，往往有公共前缀
      * 用特殊字符表示前缀使用

  * 倒排表存储

    * 问题：倒排表所需空间远大于词典本身（存文档ID）

      800K个文档至少需要20bits来表示文档ID

    * 倒排表存储的两种相反需求

      词项的频率决定了对于倒排表存储的不同需求（ppt72）

    * 采用间距代替文档ID：间距数值必小于文档ID数值

    * 可变长度编码：用最少的字节数来表示间距值

      对一个间距值G，用整数个字节来对其编码（ppt74-75）

      可变字节码在小数字上的短码可以节省更多空间

  



## 第五节 查询

#### 问题背景

* 布尔检索的重要局限性
* 更一般的检索方式：用排序代替严格匹配模式，着眼于Top  N
  * 相关性排序要点
    * 理解用户查询条件
    * 相关性的评估方式
* 本章问题：如何有效理解与响应用户的查询条件

#### 查询表达理解

* 查询条件的重要性
* 查询条件难以理解的原因
* 理解用户查询的方式
  * 最基本途径：基于查询的自然语言处理
  * 引入相关性反馈：用户对查询结果作评价-》被动
  * 引导用户表达意图：查询建议、查询扩展
  * 借助其他信息：用户间接性反馈、情境信息与情境建模

#### 相关性反馈

* 基本流程p15：核心即迭代、为词项添加权重等

##### 常用技术

* 向量空间模型(VSM)：查询和文档表征为词项权重构成的向量——》通过向量间的相似度评估相关性

  * 基于VSM的Rocchio算法：质心概念

    ppt26

* 罗基奥算法与正负反馈

  * 正反馈价值更大、收集负反馈很难

* 相关性反馈中的两个基本假设

  * 用户知道在文档集中应该用哪些词项来表达
  * 相关文档中出现的词项类似，因此可通过相似文档来相互搜寻

* 相关性反馈存在的问题：

  * 可能影响用户体验
  * 等

##### 反馈分类

* 常见的相关性反馈类型

  * 显示反馈

  * 隐式反馈：鼠标键盘动作、用户眼球动作

    对应的优缺点

  * 伪反馈：直接根据检索结果自动反馈

#### 查询扩展

* 拼写错误处理：基于词典或编辑距离的方式进行校对检查
* 查询扩展中，用户针对词项的合适程度给出反馈，并用之来构建更为完整的查询条件
* 利用同义词辞典，实现查询条件的扩展(可考虑给扩展的词以更小的权重)
  * 同义词词典的维护、建立：ppt45，46
    * 自动构建同义词词典的两种思路：ppt48
    * 同义词词典质量的讨论
* 基于搜索日志的查询扩展

#### 情境感知的查询理解

* 查询上下文有助于判断用户真实意图
* 情境信息-》提供更精确的信息检索和过滤服务
* 查询上下文能更好理解用户查询词：但如何拆分查询会话(如连续多个搜索记录是否真的都相关，哪些才是相关的?)
* 基于上下文感知的搜索：
  * 基本流程
    * 线下训练阶段(模型准备)
      * 查询词归纳为查询概念
      * 建立模型描述查询概念间的关联关系
    * 线上服务阶段（感知查询阶段）
      * 切分会话，判断与当前查询相关的上下文查询与点击记录
      * 根据已有查询记录理解用户当前意图，进行精确查询
  * 查询概念
    * 定义：一组有着相同语义的查询词
    * 启发式方法:Query-URL的二部图聚类
  * 基础的上下文感知方法：p63
  * 进阶的上下文感知方法：p64
  * 上下文感知的不同类型(细节见ppt)
    * 查询重组
    * 查询特化
    * 查询泛华
    * 一般关联
* 更复杂的情境信息：移动智能终端
* 情景感知的用户需求







## 第六节 排序

* 更一般的检索方式：采用排序代替严格匹配
  * 按相关性排序
  * 自由文本查询
* 本章问题：如何对网页进行排序，使得用户能最快获得所需信息？

#### 问题背景

* 排序问题难点
* 好的排序要求：
  * 网页内容匹配程度
  * 网页内容的质量

#### 信息检索与相关度计算

##### 信息检索模型概述

* 信息检索模型是用来描述文档与查询的表示形式与相关性的框架
* 模型的形式化表述：[D,Q,F,R(Di,q)]
* 信息检索模型分类
  * 基于集合论的模型：布尔模型（与形式化表述的对应：ppt13）
  * 基于代数论的模型：向量空间模型VSM
  * 基于概率论的模型：概率模型、语言模型、推断网络模型等

##### 向量空间模型

* 布尔模型的根本局限性
* 好的信息检索系统：相关度高的文档排名更高，必须有区分
* 量化方法1：Jaccard系数，计算两个集合(仍将文档视作词项的集合)重合度 ppt16-18
  * 缺点
* 量化方法2：词项频率TF(t,d)
  * 指词项t在文档d中出现的次数
  * 存在的问题：
  * 改进：引入对数词频
* 量化方法3：文档频率DF
  * 因为停用词等词频也很高 ->什么样的词真正有区分度：罕见词
  * $df_t$指出现词项t的文档数量
  * 逆文档频率：$idf_t$:显然$idf_t$越大，表明词项t的df越小，即越罕见(而不是the这种词)
* 量化方法4：tf-idf
  * 衡量文档相关性的词：在少数文档内多次出现的词
* 量化方法5：VSM
  * 每个文档和查询视为一个词项权重构成的向量
  * 查询时通过比较向量间相似性来进行匹配
  * 形式化表述：ppt30
  * 如何计算查询与文档间的相似度
    * 基本方法：欧式距离  -》可考虑归一化来去掉向量长度的干扰
    * 余弦相似度
  * VSM过程总结，及其优缺点：ppt36
* 其他模型：概率模型

#### 网页权威性计算

#### 排序学习问题

ch6，7均有一部分，未看



## 第 7 节 结果评价

#### 问题背景

* 信息检索模型中的相关性
  * 主题相关
  * 用户相关
* 问题：如何评估所得网页排序的质量？
* 评价前提：公平，控制干扰因素
* 结果评价的常见内容
  * 性能（本章更关注）
  * 效率
  * 多样性、权威性、时新性、更新频率
* 评价效率的常见指标：ppt7 表
* 评价性能的常见指标
  * 面向单个查询：
    * 无序/二元结果：Precision、Recall、F-value
    * 有序/多元结果：P@N、R@N、AP、NDCG
  * 面向多个查询
    * MAP、MRR及各种扩展指标

#### 单查询评价

##### 无序结果评价

* 只看相不相关
* 数据集切分角度： TP，FN，FP，TN
* 准确率Precision：TP/(TP+FP)，查准率
* 召回率Recall：TP/(TP+FN)，查全率
* Accuracy
* 准确率和召回率的平衡
* 召回率的近似计算：缓冲池方法
* F值：P-R值的加权调和平均数
* ROC曲线：接受者操作特征曲线，兼顾正负样例
  * 真正率
  * 假正率
* P-R曲线：平衡点，只考虑正例
* AUC：曲线下面积

##### 有序结果评价

* P@N：Precision@N ,前N个检索结果文档的准确率
* R@N：Recall@N，前N个检索结果找回的相关文档比例，上限往往远小于1
* R-Precision：在所有相关文档总数位置上的正确率
* P-R曲线：通过改变N得到不同的(P，R)值，连接起来成为曲线
* AP：平均准确率
  * 未插值AP
  * 插值AP：计算https://www.jianshu.com/p/8848a1441c0f
  * 简化AP

##### 相关度分级

*  分级的必要性和相关度加和
* 累计增益（CG）：衡量位于位置1到p的检索结果的相关度之和
  * 但并未考虑文档位置，不能体现靠前部分文档的质量
* 折损累计增益（DCG）：两种表达式
  * 缺点：随长度单调非减（与返回文档列表的长度有关）
* 归一化折损累计增益（NDCG）
  * NDCG=DCG/iDCG

#### 多查询评价

* 对多查询的结果进行综合
* MAP：Mean AP，求算数平均
  * 缺点：不同查询难度不平衡时会有误导
* GMAP：引入几何平均值，提升相对强弱的影响
* MRR：RR、Mean RR ，只关心返回的文档中的第一个相关文档位置
* ERR：
  * 一篇文档可能被用户点击的概率：$PR_r$
  * 预期的倒数排序ERR，表示用户需求被满足时停止的位置的倒数的期望
* 方差：一个检索系统往往对有的查询表现好，有的则表现差
  * 通常一个检索系统对不同查询的方差，往往大于多个检索系统对相同查询的方差

#### 结果多样化评价

* 为何考虑多样性
* 多样性的形式化定义
* 核心思想：降低用户无法获得所需信息的风险。尽可能确保排序靠前结果里至少有一个满足用户需求
* 多样性的衡量方式：衡量不同文档间的主题差异性==？？？？？下面两个模型没看懂==
  1. 隐式模型
     * 只计算文档间差异性
     * 代表：最大边界相关性（MMR）
  2. 显式模型
     * 考量文档对应的用户意图
     * FM-LDA，考虑文档的不同子主题



## 第 9 节 实体识别

#### 问题背景

* 信息检索模型的形式化表述
* 问题：如何从文档中提取信息和知识

#### 信息抽取概述

* 信息抽取的含义
* 信息抽取与信息检索的比较
  * 功能不同：
    * 抽取：从文本中获取用户感兴趣的事实信息
    * 检索：从文档集合中找出文档子集
  * 处理技术不同
    * 检索：通常利用统计与关键词等技术
    * 抽取：借助于自然语言处理技术
  * 使用领域不同
    * 检索：领域无关
    * 抽取：领域相关（借助于领域知识辅助抽取）
* 信息抽取的内容：抽取实体，确定关系
  * 实体、属性、关系、事件
* 信息抽取的基本任务：NE、TE、CR、TR、ST， ppt19有例子
  * 命名实体NE（实体抽取）
  * 模板元素TE（属性抽取）
    * 槽
  * 共指关系CR：等价概念
    * 实体对齐
  * 模板关系TR（关系抽取）
    * 将实体关联起来
  * 场景模板ST（事件抽取）：实体发生的事件
* 基于信息抽取的推理

#### 知识图谱概述

* 语义网络：有向图结构
* 本体论
  * 五元组表示法 O={C,R,F,A,I},具体见ppt27
* 语义网：对机器
* 知识图谱：除了显示网页文档的连接列表外，还提供结构化的、详细的有关主题的信息
  * 优点：找到最想要的信息、提供最全面的摘要、让搜索更有深度和广度
  * 基本形式：结点（概念、实体）+边（关系、属性）， 是一个有向图
  * 基本元素
    * 节点
    * 边
    * 基本单位：三元组（实体-关系-实体）
  * 相关应用：
    * 语义搜索
    * 问答系统
    * 推荐系统
  * 公开的知识图谱数据
* 事理图谱：研究谓词性事件及其内外联系
* 多模态知识图谱
  * 属性多模态
  * 实体多模态

#### 命名实体识别

* 基本概念
  * NER命名实体识别：识别出人名地名等，并加以归类
  * 核心两个子任务：判别实体边界、判别实体类型
* 命名实体识别的内容：实体类（人、地、机构名）、时间类、数值类（货币、百分比）
  * 不是命名实体的：ppt47
* 难点：与分词类似
  * 新命名实体的不断出现
  * 命名实体存在歧义
  * 构成结构复杂（别名、缩略词、音译）
  * 类型多样
* 命名实体识别的性能评价
  * Precision，Recall，F-value，计算见ppt49
* 识别方法
  * 基于词典
    * 优点、缺点
  * 基于规则：手工构造规则模板
    * 模式与字符串匹配为主要手段
    * 优点、缺点
  * 基于统计：具体类别见ppt54
    * 序列标注问题：B，M，E，S
    * **分支1：基于分类的命名实体识别方法**
      * 视作多分类问题：设计特征（如词性、字符类型、单词）和分类标签，来训练分类器
      * 得到的是高维稀疏向量（因为是0/1向量，所以如当前词是哪个，就需要n维，n是词总数）
      * 词性标注问题
        * 基本方法：
          * 基于规则（人工或通过大规模语料学习规则）
          * 基于统计模型的标注方法(如HMM)
          * 统计与规则方法相结合
    * **分支2：基于模型序列的命名实体识别方法**
      * 引入了更多更细致的标签种类
      * 常用模型：HMM、CRF、深度学习（LSTM）等
    * 特点：特征选取要求高（特征工程）、对语料依赖大、人工标注训练数据
    * 模型改进：更多领域知识(如汉字部首)，见ppt65-68
* 与命名实体识别相关问题：实体对齐
  * 利用实体的属性信息
  * 跨知识图谱的实体对齐任务：还利用关系三元组(实体间关系)
  * 进阶：多模态知识图谱里的实体对齐，需有效处理多模态特性
* 与命名实体识别相关问题：实体消歧
  * 一个单词可能有多个意思
  * 对不同含义-》抽取对应描述文本 -》 建立关键词表 -》 语义表征 -》 得到不同语义的表征向量
* 命名实体识别常用工具

## 第 10 节 关系抽取

问题：如何从文档中提取关系，实现知识实体的关联？

#### 关系抽取

* 从文本中识别出两个或多个实体间存在的关系
* 意义：
  * 是搜索引擎发现和关联知识的重要渠道
  * 是知识库构建与知识关联的基础性手段
  * 是支持问答系统、推荐系统等应用的有力工具
* 关系如何表达
  * 二元组：特定领域关系抽取
  * 三元组：多类型关系抽取
  * 多元组：加入时态等外部信息
* 常见的关系数据源

##### 关系抽取方法

* 基于规则的关系抽取

  * 纯手工定制规则（词法、句法、语义模式规则），通过匹配从文本寻找关系
  * 优缺点

* 基于模式的关系抽取

  * 从种子关系中获得关系模式，再由关系模式抽取新的关系，从中选择可信度高的关系作为新种子，迭代优化寻找新的模式和新的关系，直到没有新的关系或新的模式产生
  * 法1：DIPRE:双重迭代模式关系抽取
    * 基本元素：元组(关系实例）、模式（包含常量和变量）
    * 基本假设
    * 缺陷
    * 改进：考虑模式和三元组实例间的双重影响关系
    * 算法流程：ppt26，生成模式的基本步骤见ppt32
    * Occurrence的概念与实例：要避免因间隔太远而导致的语义不相关问题
    * 模式的概念与实例：同一关系的不同实例在网页上的不同Occurrence，保留相同内容，用通配符代替不同内容 -》 得到近似的模式
    * URL前缀的作用、 非从属组织网页
  * 法2：Snowball
    * 仅信任支持度和置信度较高的模式
    * 支持度、置信度计算：ppt34
  * 基于模式方法的优缺点

* 基于机器学习的关系抽取

  * 转化为分类问题

  * 基于特征的方法

    * 基于特征向量， 使用SVM，最大熵等进行分类

      * 难点：特征集确定，常用特征有单词本身、词性、分析树或依存树

      * 句法分析树：ppt39
      * 句法依存树：ppt40

    * 缺陷：ppt41

  * 基于深度学习技术：如CNN技术，ppt42 图

  * **基于核函数的方法**

    * 不需构建特征向量，而是利用核函数来计算两个关系实例的相关性
    * 利用句子的结构特性进行抽取：如将关系实例表示为某种结构的树，计算与结构相关的核函数的值来计算实例间的相似度
    * 浅层树核、依存树核、最短依存树核、卷积树核

##### 开发关系抽取

* 前面所讲往往是预先定义好的关系
* 基于知识监督的开放关系抽取
  * 通过wiki等结构化知识库
* 基于句法的开放关系抽取：
  * 通过识别表达语义关系的短语来抽取实体间关系
  * 关系短语
  * 通过对关系短语的句法结构约束来抽取关系
    * 优缺点

##### 远程监督方法

* 面向文本的关系抽取方法，最大难点在于获取足够数量的、高质量的标注
* 远程监督思想：若某实体对间有某种关系，则所有包含这个实体对的句子都是用于描述这种关系
* 类似于DIPRE算法的迭代思想
* 局限性：语义漂移现象 -》 人工校验
* 优化方案：
  * 动态转移矩阵：描述各个类之间相互标错的概率
  * 规则学习: ppt56
  * 注意力机制：不同句子对关系判别模型的贡献度不同

#### 关系补全

* 知识图谱的补全
* 知识图谱的关系补全问题可转换为 链接预测问题
  * 已知图结构中节点和部分边，推测其他可能存在的边，ppt61
  * 二者的差异性
* 基于知识图谱嵌入的补全方法
  * 代表：TransE模型，思路借鉴了word2vec，利用词向量
  * 其他Teans系列模型概述ppt64

#### 事件抽取

* 场景模板ST（事件抽取，见第9章）

  * 把语义要素填入槽(ch9 ppt14)中

* 事件抽取是构建事理图谱的基础

* 事件抽取的概念

  * 信息呈现为句子级别

  * 是推理任务的前提

  * 基本要素

    * 事件触发词：动、名词 （事件发生的核心词）

      对应 事件触发词的检测与分类是基本任务

    * 事件类型

    * 事件元素：如时间、实体等

    * 事件元素角色

* 事件抽取的模板：参考 模板元素TE（属性抽取） -》 槽

  具体流程： 

  * 触发词识别和分类-》 判定事件及其类型
  * 选定相应模板，并识别事件元素、事件元素角色 -》 将识别结果填入模板合适的槽内

* 限定域事件抽取：类似于预定义关系抽取

  * 可采用基于模式匹配的方法实现
    * 基于规则（完全人工标注）
    * 弱监督的模式匹配 -》 如种子模式（DIPRE方法）
  * 基于机器学习方法：多分类问题
    * 基于特征工程
    * 基于神经网络
  * 远程监督等弱监督方式

* 开放域事件抽取

  * 无监督方法：聚类
  * 类比 开放关系抽取里的 ”知识监督“方案

* 从知识图谱到事理图谱

  

## 第 11 节 数据准备

* 数据挖掘定义
* 数据挖掘的目的：满足以下条件的模式或模型
  * 有效性
  * 实用性
  * 解释性
  * 意外性

#### 数据挖掘方法

* 分类
* 聚类

##### 关联规则

参考https://www.cnblogs.com/lsqin/p/9342926.html

* 事务型数据：一条记录对应一个项目的集合(无序)

* 关联规则

  * 基本形式：A->B
  * 评价指标：支持度、置信度

* 频繁项集

  频繁项集的生成方法

  * 穷举
  * Apriori：先验原理：非频繁的项集，其所有超集也是非频繁的
    * 算法
  * FP-Growth：？？？？？？

##### 异常检测

* 基于统计的方法
* 基于度量：距离或密度
* 基于聚类

#### 数据预处理

* 为何要数据预处理

* 常见数据质量问题

  * 数据测量、采集等过程中出现的错误
  * 噪声、离群点、缺失值等问题
  * 重复数据

* 噪声数据

* 离群数据

* 缺失数据：删除+填补并重

* 重复数据：知识图谱中的实体对齐

* 常见预处理方法

  * 数据聚合

  * 数据采样：采样要有代表性

    * 启发式的采样规模确定方法
    * 采样不应该具有倾向性

  * 维度归约：删除不具有区分度的特征

    * 维度灾难

    * 主成分分析（PCA）：思路：通过正交变换将一组可能存在相关性的变量转换为一组线性不相关的变量。（也是一个降维的过程）

      https://zhuanlan.zhihu.com/p/150097862，参考 随机向量的主成分 一节

      * 选择轴的标准：轴上的投影方差尽可能大
      * 最大特征值对应的特征向量可以最大化投影方差（要求掌握算特征值的方法）
      * 特征向量：对现有属性的线性组合，以生成新的特征
    * ？？？？？读https://blog.csdn.net/lyl771857509/article/details/79435402
  
* 数据离散
  
    * Pointwise类排序算法：有序回归方法：数轴区间与文档标签对应起来
    * 最基本的离散化：二元化（二进制）
    * 非监督离散化：根据数据本身的特性进行离散（不使用类别信息）
    * 有监督离散化



## 第 12 节 分类



## 第 13 节 聚类

##### 不同的蔟类型

* 明显分离的簇
* 基于中心的簇
* 基于邻近的簇
* 基于密度的簇
* 基于概念的簇

##### 常见的聚类方法

* K均值聚类
* 层次聚类
  * 叶结点是单个样本
  * 相应层数的划分对应K值
  * 分类：
    1. 凝聚式聚类（自下而上）
       * 邻近度矩阵
       * 邻近度的定义
         * 单链
         * 全链
         * 组平均
         * 中心距离
    2. 分裂式聚类（自上而下）
       * 二分K均值聚类
       * 最小生成树聚类（MST）
         * 差异度矩阵
         * 断边
         * 有趣的现象：
  * 层次聚类的局限性
* 基于密度的聚类
  * 密度：指定样本一定半径的样本数量
  * BDSCAN算法：核心点、边界点、噪声点
* 模糊聚类初阶
  * 从属度
  * 模糊K均值聚类

##### 聚类问题的评估

* 非监督评估（内部评估）
  * 基于邻近度矩阵
  * 凝聚度与分离度
    * 基于图和基于原型(中心)的两种度量方式
* 有监督评估（外部评估）
  * 有外部标签
    * 分类手段
    * 邻近度矩阵
* 相对评估：比较两个簇或者两个聚类结果











## 第 14 节 推荐系统

### 问题背景

* 常见的推荐类型：长尾现象、马太效应

* 推荐系统的数学形式：

  基本要素

  * 用户集合
  * 物品集合
  * 评分集合
  * 效用函数

  推荐系统本质是一个矩阵补全问题

* 推荐系统3个问题：收集 -》 推断 -》 评估 （ppt16）

  * 收集评分数据：显式数据、隐式数据(从行为等)

    负样本问题

  * 推断未知评分：效用矩阵的严重稀疏性

    冷启动问题

  * 推荐效果评估：

    评估方式

    * 均方根误差RMSE
    * 分类问题：0/1效用矩阵 -> Precision/Recall/F值等评估
    * 排序后返回TopN：用Pre@N, Rec@N等评估
    * 排序问题：基于预测的评分对物品排序，并基于Ch7关于排序评估方式

    评估的注意事项

    * 推荐的顺序值得研究
    * 推荐的多样性 ：避免茧房效应

### 基于内容的推荐

* 用户偏好相对稳定：推荐其以前喜欢的

* 基本流程（ppt26）

* 物品画像（向量格式）

  用户画像由其评分过的物品画像估计（如加权平均）

  利用相似度度量（如余弦相似度）来对新物品评分

* 优点：

  * 每个人推荐相互独立，冷启动没那么严重
  * 不受大众热度影响
  * 可以推荐新物品、非热门物品
  * 可解释性强

* 缺点：

  * 找到合适的特征很难
  * 新用户画像如何建立
  * 过度特化现象

* 多样化问题的解决

  * 评估方式里引入多样性评估：最大边界相关性(MMR)

* 衍生：基于路径推荐

  采用知识图谱代替向量化的画像方式，基于图谱上的游走实现推荐（概率最大的路径）



### 基于协同过滤的推荐Collaborative Filtering

* 基本思想：其他用户的浏览记录也有借鉴作用，所以回到矩阵补全问题：基于矩阵的其他行，协助补全本行

#### 基于内存

##### 基于用户推荐

* 有相似偏好的用户，过去与未来的评分行为相似

* 所以寻找最近邻用户（从用户评分的相似趋势，不是绝对分值，即用户对共同物品的评分趋势相似）

* **相似度度量公式：ppt40**

* 得到用户的相似性后，把相似性作为加权来根据历史上其他用户对该物品的评分来预测物品的评分

  **公式见ppt41**

  * 进一步，考虑K-最近邻的相似用户（当然无评分的跳过）

##### 基于物品推荐

* 相似的物品，往往在大众眼中评分也比较接近
* 衡量相似物品的标准：不同物品的评分历史（同一个人给两个物品打出相似分数，说明其认为两物品相似，越多这样的人，两物品越相似）

* 两类方法对比：物品推荐效果更好

* 基于内存推荐的优缺点
  * 优：适用于任意种类物品，效果好
  * 缺：冷启动、稀疏性、热度偏差等问题
* 衍生问题（ppt50-51）
  * 用户冷启动问题
  * 物品冷启动问题

#### 基于模型

* 基本思路：用户偏好与物品属性间的相似度

  * 基于内存的推荐的假设（ppt54）
  * 考虑利用潜在的主题（潜在因素）来表示用户的偏好与物品的属性

* 潜在因子

* 基本形式：矩阵分解(MF)

  *   评分矩阵R拆分为物品属性矩阵Q，用户偏好矩阵P

    P、Q的维度取决于物品/用户数量，以及潜在因子的数量

  * 评分预测

  * 估计潜在因子矩阵P, Q:利用历史评分记录：ppt58式子（误差平方和SSE）

  * 过拟合与正则化：将参数往原点拉

* 衍生模型

  * 非负矩阵分解：根据实际情况，添加非负约束，利用梯度下降法优化，引入噪声矩阵E

    ==迭代式不太懂，感觉矩阵乘法维度对应不上(ppt64)==

  * 概率矩阵分解：先假设参数、噪声均符合高斯分布

  * 进一步在模型中加入更多信息？ -> ppt68

  * 社交约束：好友在偏好与行为上十分相似

    但：用户在不同社交关系中体现出不同偏好影响

#### 应用示例:Netflix Prize

* 集成学习：线性集成方式（各个推荐模型的结果加权整合）
* 全局与局部信息的融合：个人打分习惯与总体趋势的偏离
* 捕捉情景化规律：不同时间的评分行为（工作日、周末）； 旧的偏好会随着时间流逝而衰减







## 第15节 社会网络(上)

本节讨论无向图（朋友、合作关系）

### 基本概念

* 社会网络的基本元素：图、节点、边（超边、超图）
  * 邻居、度
  * 大V ： 幂律分布（ppt13）
  * 连通性：图的连通性（有向图分强弱）、两节点的连通性
  * 六度空间理论
  * 连通组件（连通子图，有向图分强弱）

### 节点角色

* 强/弱关系理论
* 节点的角色不同：
  * 意见领袖：参考HITS算法以及权威网页（信息生产者）、枢纽网页（转化信息）的区分
  * 结构洞：为组织引入外部信息（不同部门间的信息沟通）
    * 判断方式：  1.ppt33     2. 聚集系数（越低越中介）

### 链接预测

* 主要预测网络中的新边

* 关系的嵌入性：一条边两端节点共同的好友数

* 链接预测方法

  1. 最基本的链接预测：ppt39式子，两个节点共同好友越多，越可能存在一条边
  2. 改进版：（避免海王）对式子进行Normalization ppt40
  3. 另一种链接预测思路：无尺度网络，有更多好友的节点更易获得新好友ppt41

  4. 同时考虑共同好友数量、以及共同好友的好友数（交友精力问题） ppt42

  5. 从局部到全局：基于多跳关系的链接预测方法 ppt44

  6. 更一般化的基于全局网络结构的链接预测方法：RWR指标，==ppt46-47？？没太看懂==

  7. 有监督学习：Random Walk(全局网络结构) + Pairwise LTR(训练偏好)

  8. 网络表征 ： 图结构 -> 转化为节点序列，采用类似文本的表征方法（Word2Vec)

     Nodevec：从DFS和BFS路径获得节点序列

* 网络的符号性：敌对、朋友关系

  ​	图的平衡性ppt55

  ​	引入二阶约束

* 链接的稳定匹配问题

### 社团挖掘

只考虑无向图

* 社团的广泛存在

* 聚类技术：

  * 层次聚类

    * 分裂式聚类（自上而下，eg：最小生成树聚类）

      考虑逐步去掉弱连接的过程

    * 边介数，计算实例ppt66

      Girvan-Newman算法ppt67

      算法局限性：无法确定合适的社团数（可考虑通过凝聚度、分离度等层次聚类指标来选择，见ch13）

    * Q-Modularity指标：衡量社区划分的通用标准(ppt71) ,     ==式子的理解ppt72（不懂==

      选取Q峰值时对应的层次聚类结果

  * 划分聚类

    * 如K均值聚类：依赖样本属性的相似度 -》 

      节点的属性：1. 额外引入   2. 用邻接矩阵作为节点属性（n维0-1向量）

    * 谱聚类





## 第16节 社会网络（下）

本节讨论有向图（通讯、关注关系）

本节问题：社会网络中的信息如何流动？

### 问题背景

* 信息传播：定义ppt9

  关注 信息传播过程的模型设计

* 信息传播的元素

  * 发送者
  * 接受者
  * 媒介（平台）

* 信息传播的类型：ppt12图

以下均讲的local information

### 基本传播模型

* 信息级联：每个节点仅能将信息传递给与其直接相连的节点
  * 网络中节点状态是二元的：激活（收到并转发了信息）/未激活

#### 独立级联模型（ICM)

* 每次激活都是一次独立事件，互不影响
* 每个已激活节点，仅一次机会尝试激活其的未激活邻居节点
* 过程：ppt18，19
* 例子：ppt20

#### 线性阈值模型 (LTM)

* 将信息传递过程视作多人影响的叠加过程，ppt21，22（有确定性）
* 例子：ppt23

### 传播最大化问题

* 初始种子节点集合S，预期激活的节点集合f(S)
* 最大化问题的目的：选取限定规模的集合S，最大化f(S)的规模
* 启发式思路：寻找网络中最具影响力的节点
  * PageRank及其衍生模型
  * 核心性度量：
    * 度：节点邻居越多，影响力越大，ppt29计算公式、归一化公式
    * 紧密度：某个节点到其他节点越近（最短路径长度），影响力越大  ，ppt30，31计算公式、例子
    * 介数：越多最短路径经过某节点，计算公式ppt32
  * 计算单个节点所能激活的邻居数量，再进行排序（但节点影响范围可能存在重叠）：ppt34，有缺点
* 一般化方法：传播最大化问题转化为一个带约束的最优化问题：ppt35
  * f(S)的性质：ppt36，子模特性
  * 在ICM/LTM等模型定义下，此问题是一个NP难问题
  * 但可以考虑贪心算法近似求解，算法ppt38，39

### 衍生传播模型

* 独立级联模型的局限性ppt41

  改进：稳定状态传播模型（SSS）（节点状态不再二元化；节点可以持续对外输出影响）

  核心公式：ppt43

  隐患：不合理的反向传播，解决方案见ppt46

* 独立级联模型的另一局限性ppt47：信息传播与接收的捆绑

  解绑的衍生模型

  * 信息覆盖最大化问题

  * 单独对信息接受过程进行建模:ppt51，描述节点接受信息的概率

    引出信息接受最大化问题： 问题求解的转换ppt53，论文思路ppt52







