问题：机器学习：

* 书p28

* 其中试卷第一题：实验方法： 数据集划分、根据不同任务选择算法、对结果作假设检验
* 第14章作业14.3可以参考李航的统计学习方法，但是略有区别



## 导言

* 大三下有一门深度学习
* 参考书：统计机器学习(李航，清华大学出版社)； Pattern Recognition and Machine Learning

* 笔试：50%
* 20%作业，15次作业，平均每次2道题，每三次提交一次作业
* 上机：占比30%，约6次实验



## 第一次课

#### 机器学习的基本概念

* 数据(经验E)：训练集、测试集、特征、标记、训练样本（特征+标记）

* 任务T：按标记区分：

  * 分类：标记为离散值（二分类、多分类）
  * 回归：标记为连续值
  * 聚类：没有标记

  前两个是监督学习，聚类是无监督学习

  监督+无监督学习=半监督学习（eg：有些西瓜有标记，有些没有）

* 泛化能力：模型适应新样本的能力。样本 i.i.d。训练样本越多，越可能通过学习获得强泛化能力的模型

* 假设空间：在假设空间中搜索不违背训练集的假设

* 归纳偏好：奥卡姆剃刀：若有多个假设与观察一致，选最简单的那个
* NFL

#### 机器学习的部分数学基础

矩阵（https://zhuanlan.zhihu.com/p/24709748）

* 增广矩阵

* 加减、标量乘法

* 转置

* 矩阵乘法

* 矩阵类型：对角矩阵、(反)对称矩阵、正交矩阵、方阵（对角矩阵不一定是方阵，长方形的矩阵也可能是对角矩阵。但是所有的对称矩阵和正交矩阵一定是方阵。）、单位矩阵

* 矩阵逆元:如存在一个矩阵B使得AB = BA = I，则称方阵A为非奇异的（nonsingular）或者可逆的（invertible），矩阵B称为矩阵A的乘法逆元; 方阵不存在乘法逆元，则称为奇异的

* 行列式

* 特征值与特征向量(不是每一个矩阵都可以分解成特征值和特征向量。但每个实对称矩阵都可以分解成实特征向量和实特征值。)

* 正定性：如果**对称方阵A**满足对所有非零向量x，对应的二次型：$Q(x)=x^TAx$ 函数值都是正数，就称A为正定矩阵（positive definite matrix）。对称矩阵的正定性与其特征值密切相关。矩阵是正定的当且仅当其特征值都是正数。

  另外：

  - 所有特征值都是非负数的矩阵被称为半正定（positive semidefinite）矩阵。
  - 所有特征值都是负数的矩阵被称为负定（negative definite）矩阵。
  - 所有特征值都是非正数的矩阵被称为 半负定（negative semidefinite）矩阵。

* 奇异值分解

* 向量空间

  子空间

* 线性无关：如果一组向量中的任意一个向量都不能表示成其他向量的线性组合，那么称这组向量为线性无关 （linearly independent）。如果某个向量是一组向量中某些向量的线性组合，那么我们将这个向量加入这组向量后不会增加这组向量的生成子空间。

* 基和维数：标准正交基

* 行空间、列空间

* 矩阵的秩：矩阵A线性无关的列的极大数目，称之为矩阵A的列秩。类似地，矩阵A线性无关的行的极大数目，称之为矩阵A的行秩。矩阵的行秩和列秩总是相等的。因此可以简称为矩阵的秩（rank），通常记为r(A)r(A) 或者rank(A)rank(A)

* 线性变换

* 矩阵范数
  * F范数
  * 诱导范数
  
* 海森矩阵

* 雅克比矩阵

* 伴随矩阵：矩阵中全部元素的代数余子式所构成的矩阵就为伴随矩阵

* 矩阵求导

  * 小写字母x表示标量，粗体小写字母![[公式]](https://www.zhihu.com/equation?tex=%5Cboldsymbol%7Bx%7D+)表示（列）向量，大写字母X表示矩阵。

  * 标量f对矩阵X的导数，定义为![[公式]](https://www.zhihu.com/equation?tex=%5Cfrac%7B%5Cpartial+f%7D%7B%5Cpartial+X%7D+%3D+%5Cleft%5B%5Cfrac%7B%5Cpartial+f+%7D%7B%5Cpartial+X_%7Bij%7D%7D%5Cright%5D)，即f对X逐元素求导排成与X尺寸相同的矩阵

    <u>将矩阵导数与微分建立联系：![[公式]](https://www.zhihu.com/equation?tex=df+%3D+%5Csum_%7Bi%3D1%7D%5Em+%5Csum_%7Bj%3D1%7D%5En+%5Cfrac%7B%5Cpartial+f%7D%7B%5Cpartial+X_%7Bij%7D%7DdX_%7Bij%7D+%3D+%5Ctext%7Btr%7D%5Cleft%28%5Cfrac%7B%5Cpartial+f%7D%7B%5Cpartial+X%7D%5ET+dX%5Cright%29+)</u>

    其中tr代表迹(trace)是方阵对角线元素之和，满足性质：对尺寸相同的矩阵A,B，![[公式]](https://www.zhihu.com/equation?tex=%5Ctext%7Btr%7D%28A%5ETB%29+%3D+%5Csum_%7Bi%2Cj%7DA_%7Bij%7DB_%7Bij%7D)，即![[公式]](https://www.zhihu.com/equation?tex=%5Ctext%7Btr%7D%28A%5ETB%29)是矩阵A,B的内积（矩阵内积是矩阵对应元素乘积之和）

    全微分![[公式]](https://www.zhihu.com/equation?tex=df)是导数![[公式]](https://www.zhihu.com/equation?tex=%5Cfrac%7B%5Cpartial+f%7D%7B%5Cpartial+X%7D)(m×n)与微分矩阵![[公式]](https://www.zhihu.com/equation?tex=dX)(m×n)的内积。

    * <u>矩阵微分运算法则</u>

      1. 加减法：![[公式]](https://www.zhihu.com/equation?tex=d%28X%5Cpm+Y%29+%3D+dX+%5Cpm+dY)；矩阵乘法：![[公式]](https://www.zhihu.com/equation?tex=d%28XY%29+%3D+%28dX%29Y+%2B+X+dY+)；转置：![[公式]](https://www.zhihu.com/equation?tex=d%28X%5ET%29+%3D+%28dX%29%5ET)；迹：![[公式]](https://www.zhihu.com/equation?tex=d%5Ctext%7Btr%7D%28X%29+%3D+%5Ctext%7Btr%7D%28dX%29)。
      2. 逆：![[公式]](https://www.zhihu.com/equation?tex=dX%5E%7B-1%7D+%3D+-X%5E%7B-1%7DdX+X%5E%7B-1%7D)。此式可在![[公式]](https://www.zhihu.com/equation?tex=XX%5E%7B-1%7D%3DI)两侧求微分来证明。
      3. 行列式：![[公式]](https://www.zhihu.com/equation?tex=d%7CX%7C+%3D+%5Ctext%7Btr%7D%28X%5E%7B%5C%23%7DdX%29+)，其中![[公式]](https://www.zhihu.com/equation?tex=X%5E%7B%5C%23%7D)表示X的伴随矩阵，在X可逆时又可以写作![[公式]](https://www.zhihu.com/equation?tex=d%7CX%7C%3D+%7CX%7C%5Ctext%7Btr%7D%28X%5E%7B-1%7DdX%29)。此式可用Laplace展开来证明，详见张贤达《矩阵分析与应用》第279页。
      4. 逐元素乘法：![[公式]](https://www.zhihu.com/equation?tex=d%28X%5Codot+Y%29+%3D+dX%5Codot+Y+%2B+X%5Codot+dY)，![[公式]](https://www.zhihu.com/equation?tex=%5Codot)表示尺寸相同的矩阵X,Y逐元素相乘。
      5. 逐元素函数：![[公式]](https://www.zhihu.com/equation?tex=d%5Csigma%28X%29+%3D+%5Csigma%27%28X%29%5Codot+dX+)，![[公式]](https://www.zhihu.com/equation?tex=%5Csigma%28X%29+%3D+%5Cleft%5B%5Csigma%28X_%7Bij%7D%29%5Cright%5D)是逐元素标量函数运算， ![[公式]](https://www.zhihu.com/equation?tex=%5Csigma%27%28X%29%3D%5B%5Csigma%27%28X_%7Bij%7D%29%5D)是逐元素求导数。例如![[公式]](https://www.zhihu.com/equation?tex=X%3D%5Cleft%5B%5Cbegin%7Bmatrix%7DX_%7B11%7D+%26+X_%7B12%7D+%5C%5C+X_%7B21%7D+%26+X_%7B22%7D%5Cend%7Bmatrix%7D%5Cright%5D%2C+d+%5Csin%28X%29+%3D+%5Cleft%5B%5Cbegin%7Bmatrix%7D%5Ccos+X_%7B11%7D+dX_%7B11%7D+%26+%5Ccos+X_%7B12%7D+d+X_%7B12%7D%5C%5C+%5Ccos+X_%7B21%7D+d+X_%7B21%7D%26+%5Ccos+X_%7B22%7D+dX_%7B22%7D%5Cend%7Bmatrix%7D%5Cright%5D+%3D+%5Ccos%28X%29%5Codot+dX)

    * <u>迹的技巧</u>

      1. 标量套上迹：![[公式]](https://www.zhihu.com/equation?tex=a+%3D+%5Ctext%7Btr%7D%28a%29)
      2. 转置：![[公式]](https://www.zhihu.com/equation?tex=%5Cmathrm%7Btr%7D%28A%5ET%29+%3D+%5Cmathrm%7Btr%7D%28A%29)。
      3. 线性：![[公式]](https://www.zhihu.com/equation?tex=%5Ctext%7Btr%7D%28A%5Cpm+B%29+%3D+%5Ctext%7Btr%7D%28A%29%5Cpm+%5Ctext%7Btr%7D%28B%29)。
      4. 矩阵乘法交换：![[公式]](https://www.zhihu.com/equation?tex=%5Ctext%7Btr%7D%28AB%29+%3D+%5Ctext%7Btr%7D%28BA%29)，其中![[公式]](https://www.zhihu.com/equation?tex=A)与![[公式]](https://www.zhihu.com/equation?tex=B%5ET)尺寸相同。两侧都等于![[公式]](https://www.zhihu.com/equation?tex=%5Csum_%7Bi%2Cj%7DA_%7Bij%7DB_%7Bji%7D)。
      5. 矩阵乘法/逐元素乘法交换：![[公式]](https://www.zhihu.com/equation?tex=%5Ctext%7Btr%7D%28A%5ET%28B%5Codot+C%29%29+%3D+%5Ctext%7Btr%7D%28%28A%5Codot+B%29%5ETC%29)，其中![[公式]](https://www.zhihu.com/equation?tex=A%2C+B%2C+C)尺寸相同。两侧都等于![[公式]](https://www.zhihu.com/equation?tex=%5Csum_%7Bi%2Cj%7DA_%7Bij%7DB_%7Bij%7DC_%7Bij%7D)

    * 复合运算

      假设已求得![[公式]](https://www.zhihu.com/equation?tex=%5Cfrac%7B%5Cpartial+f%7D%7B%5Cpartial+Y%7D)，而Y是X的函数，如何求![[公式]](https://www.zhihu.com/equation?tex=%5Cfrac%7B%5Cpartial+f%7D%7B%5Cpartial+X%7D)呢？在微积分中有标量求导的链式法则![[公式]](https://www.zhihu.com/equation?tex=%5Cfrac%7B%5Cpartial+f%7D%7B%5Cpartial+x%7D+%3D+%5Cfrac%7B%5Cpartial+f%7D%7B%5Cpartial+y%7D+%5Cfrac%7B%5Cpartial+y%7D%7B%5Cpartial+x%7D)，但这里我们**不能随意沿用标量的链式法则**，因为矩阵对矩阵的导数![[公式]](https://www.zhihu.com/equation?tex=%5Cfrac%7B%5Cpartial+Y%7D%7B%5Cpartial+X%7D)截至目前仍是未定义的。于是我们继续追本溯源，链式法则是从何而来？源头仍然是微分。我们直接从微分入手建立复合法则：先写出![[公式]](https://www.zhihu.com/equation?tex=df+%3D+%5Ctext%7Btr%7D%5Cleft%28%5Cfrac%7B%5Cpartial+f%7D%7B%5Cpartial+Y%7D%5ET+dY%5Cright%29)，再将dY用dX表示出来代入，并使用迹技巧将其他项交换至dX左侧，即可得到![[公式]](https://www.zhihu.com/equation?tex=%5Cfrac%7B%5Cpartial+f%7D%7B%5Cpartial+X%7D)。

      最常见的情形是![[公式]](https://www.zhihu.com/equation?tex=Y+%3D+AXB)，此时 ![[公式]](https://www.zhihu.com/equation?tex=df+%3D+%5Ctext%7Btr%7D%5Cleft%28%5Cfrac%7B%5Cpartial+f%7D%7B%5Cpartial+Y%7D%5ET+dY%5Cright%29+%3D+%5Ctext%7Btr%7D%5Cleft%28%5Cfrac%7B%5Cpartial+f%7D%7B%5Cpartial+Y%7D%5ET+AdXB%5Cright%29+%3D++%5Ctext%7Btr%7D%5Cleft%28B%5Cfrac%7B%5Cpartial+f%7D%7B%5Cpartial+Y%7D%5ET+AdX%5Cright%29+%3D+%5Ctext%7Btr%7D%5Cleft%28%28A%5ET%5Cfrac%7B%5Cpartial+f%7D%7B%5Cpartial+Y%7DB%5ET%29%5ET+dX%5Cright%29) ，可得到![[公式]](https://www.zhihu.com/equation?tex=%5Cfrac%7B%5Cpartial+f%7D%7B%5Cpartial+X%7D%3DA%5ET%5Cfrac%7B%5Cpartial+f%7D%7B%5Cpartial+Y%7DB%5ET)。注意这里![[公式]](https://www.zhihu.com/equation?tex=dY+%3D+%28dA%29XB+%2B+AdXB+%2B+AXdB+%3D+AdXB)，由于![[公式]](https://www.zhihu.com/equation?tex=A%2CB)是常量，![[公式]](https://www.zhihu.com/equation?tex=dA%3D0%2CdB%3D0)，以及我们使用矩阵乘法交换的迹技巧交换了![[公式]](https://www.zhihu.com/equation?tex=%5Cfrac%7B%5Cpartial+f%7D%7B%5Cpartial+Y%7D%5ET+AdX)与![[公式]](https://www.zhihu.com/equation?tex=B)。

    * 总结：**若标量函数f是矩阵X经加减乘法、逆、行列式、逐元素函数等运算构成，则使用相应的运算法则对f求微分，再使用迹技巧给df套上迹并将其它项交换至dX左侧，对照导数与微分的联系**![[公式]](https://www.zhihu.com/equation?tex=df+%3D+%5Ctext%7Btr%7D%5Cleft%28%5Cfrac%7B%5Cpartial+f%7D%7B%5Cpartial+X%7D%5ET+dX%5Cright%29+)**，即能得到导数。**

      **特别地，若矩阵退化为向量，对照导数与微分的联系**![[公式]](https://www.zhihu.com/equation?tex=df+%3D+%5Cfrac%7B%5Cpartial+f%7D%7B%5Cpartial+%5Cboldsymbol%7Bx%7D%7D%5ET+d%5Cboldsymbol%7Bx%7D+)**，即能得到导数。**
    
      * 标量f对向量x求导即是求标量的梯度，f(x)情况下，逐元素求导即可

  * 矩阵对矩阵求导

    * 矩阵F(p×q)对矩阵X(m×n)的导数应包含所有mnpq个偏导数![[公式]](https://www.zhihu.com/equation?tex=%5Cfrac%7B%5Cpartial+F_%7Bkl%7D%7D%7B%5Cpartial+X_%7Bij%7D%7D)，从而不损失信息

    * 先定义向量![[公式]](https://www.zhihu.com/equation?tex=%5Cboldsymbol%7Bf%7D)(p×1)对向量![[公式]](https://www.zhihu.com/equation?tex=%5Cboldsymbol%7Bx%7D)(m×1)的导数![[公式]](https://www.zhihu.com/equation?tex=%5Cfrac%7B%5Cpartial+%5Cboldsymbol%7Bf%7D%7D%7B%5Cpartial+%5Cboldsymbol%7Bx%7D%7D+%3D+%5Cbegin%7Bbmatrix%7D+%5Cfrac%7B%5Cpartial+f_1%7D%7B%5Cpartial+x_1%7D+%26+%5Cfrac%7B%5Cpartial+f_2%7D%7B%5Cpartial+x_1%7D+%26+%5Ccdots+%26+%5Cfrac%7B%5Cpartial+f_p%7D%7B%5Cpartial+x_1%7D%5C%5C+%5Cfrac%7B%5Cpartial+f_1%7D%7B%5Cpartial+x_2%7D+%26+%5Cfrac%7B%5Cpartial+f_2%7D%7B%5Cpartial+x_2%7D+%26+%5Ccdots+%26+%5Cfrac%7B%5Cpartial+f_p%7D%7B%5Cpartial+x_2%7D%5C%5C+%5Cvdots+%26+%5Cvdots+%26+%5Cddots+%26+%5Cvdots%5C%5C+%5Cfrac%7B%5Cpartial+f_1%7D%7B%5Cpartial+x_m%7D+%26+%5Cfrac%7B%5Cpartial+f_2%7D%7B%5Cpartial+x_m%7D+%26+%5Ccdots+%26+%5Cfrac%7B%5Cpartial+f_p%7D%7B%5Cpartial+x_m%7D%5C%5C+%5Cend%7Bbmatrix%7D)(m×p)，有![[公式]](https://www.zhihu.com/equation?tex=d%5Cboldsymbol%7Bf%7D+%3D+%5Cfrac%7B%5Cpartial+%5Cboldsymbol%7Bf%7D+%7D%7B%5Cpartial+%5Cboldsymbol%7Bx%7D+%7D%5ET+d%5Cboldsymbol%7Bx%7D+)

    * 再定义矩阵的（按列优先）向量化![[公式]](https://www.zhihu.com/equation?tex=%5Cmathrm%7Bvec%7D%28X%29+%3D+%5BX_%7B11%7D%2C+%5Cldots%2C+X_%7Bm1%7D%2C+X_%7B12%7D%2C+%5Cldots%2C+X_%7Bm2%7D%2C+%5Cldots%2C+X_%7B1n%7D%2C+%5Cldots%2C+X_%7Bmn%7D%5D%5ET)(mn×1)，并定义矩阵F对矩阵X的导数![[公式]](https://www.zhihu.com/equation?tex=%5Cfrac%7B%5Cpartial+F%7D%7B%5Cpartial+X%7D+%3D+%5Cfrac%7B%5Cpartial+%5Cmathrm%7Bvec%7D%28F%29%7D%7B%5Cpartial+%5Cmathrm%7Bvec%7D%28X%29%7D)(mn×pq)

      导数与微分有联系![[公式]](https://www.zhihu.com/equation?tex=%5Cmathrm%7Bvec%7D%28dF%29+%3D+%5Cfrac%7B%5Cpartial+F%7D%7B%5Cpartial+X%7D%5ET+%5Cmathrm%7Bvec%7D%28dX%29)

    * 说明：

      1. 用记号![[公式]](https://www.zhihu.com/equation?tex=%5Cnabla_X+f)表示上篇定义的m×n矩阵，则有![[公式]](https://www.zhihu.com/equation?tex=%5Cfrac%7B%5Cpartial+f%7D%7B%5Cpartial+X%7D%3D%5Cmathrm%7Bvec%7D%28%5Cnabla_X+f%29)。虽然本篇的技术可以用于标量对矩阵求导这种特殊情况，但使用上篇中的技术更方便。读者可以通过上篇中的算例试验两种方法的等价转换。

      2. **标量对矩阵的二阶导数，又称Hessian矩阵**，定义为![[公式]](https://www.zhihu.com/equation?tex=%5Cnabla%5E2_X+f+%3D+%5Cfrac%7B%5Cpartial%5E2+f%7D%7B%5Cpartial+X%5E2%7D+%3D+%5Cfrac%7B%5Cpartial+%5Cnabla_X+f%7D%7B%5Cpartial+X%7D)(mn×mn)，是对称矩阵。对向量![[公式]](https://www.zhihu.com/equation?tex=%5Cfrac%7B%5Cpartial+f%7D%7B%5Cpartial+X%7D)或矩阵![[公式]](https://www.zhihu.com/equation?tex=%5Cnabla_X+f)求导都可以得到Hessian矩阵，但从矩阵![[公式]](https://www.zhihu.com/equation?tex=%5Cnabla_X+f)出发更方便。

      3. ![[公式]](https://www.zhihu.com/equation?tex=%5Cfrac%7B%5Cpartial+F%7D%7B%5Cpartial+X%7D+%3D+%5Cfrac%7B%5Cpartial%5Cmathrm%7Bvec%7D+%28F%29%7D%7B%5Cpartial+X%7D+%3D+%5Cfrac%7B%5Cpartial+F%7D%7B%5Cpartial+%5Cmathrm%7Bvec%7D%28X%29%7D+%3D+%5Cfrac%7B%5Cpartial%5Cmathrm%7Bvec%7D%28F%29%7D%7B%5Cpartial+%5Cmathrm%7Bvec%7D%28X%29%7D)，求导时矩阵被向量化，弊端是这在一定程度破坏了矩阵的结构，会导致结果变得形式复杂；好处是多元微积分中关于梯度、Hessian矩阵的结论可以沿用过来，只需将矩阵向量化

      4. 有分子布局和分母布局两种定义，其中向量对向量的导数的排布有所不同。本文使用的是分母布局，机器学习和优化中的梯度矩阵采用此定义

         而控制论等领域中的Jacobian矩阵采用分子布局，向量![[公式]](https://www.zhihu.com/equation?tex=%5Cboldsymbol%7Bf%7D)对向量![[公式]](https://www.zhihu.com/equation?tex=%5Cboldsymbol%7Bx%7D)的导数定义是![[公式]](https://www.zhihu.com/equation?tex=%5Cfrac%7B%5Cpartial+%5Cboldsymbol%7Bf%7D%7D%7B%5Cpartial+%5Cboldsymbol%7Bx%7D%7D+%3D+%5Cbegin%7Bbmatrix%7D+%5Cfrac%7B%5Cpartial+f_1%7D%7B%5Cpartial+x_1%7D+%26+%5Cfrac%7B%5Cpartial+f_1%7D%7B%5Cpartial+x_2%7D+%26+%5Ccdots+%26+%5Cfrac%7B%5Cpartial+f_1%7D%7B%5Cpartial+x_m%7D%5C%5C+%5Cfrac%7B%5Cpartial+f_2%7D%7B%5Cpartial+x_1%7D+%26+%5Cfrac%7B%5Cpartial+f_2%7D%7B%5Cpartial+x_2%7D+%26+%5Ccdots+%26+%5Cfrac%7B%5Cpartial+f_2%7D%7B%5Cpartial+x_m%7D%5C%5C+%5Cvdots+%26+%5Cvdots+%26+%5Cddots+%26+%5Cvdots%5C%5C+%5Cfrac%7B%5Cpartial+f_p%7D%7B%5Cpartial+x_1%7D+%26+%5Cfrac%7B%5Cpartial+f_p%7D%7B%5Cpartial+x_2%7D+%26+%5Ccdots+%26+%5Cfrac%7B%5Cpartial+f_p%7D%7B%5Cpartial+x_m%7D%5C%5C+%5Cend%7Bbmatrix%7D)，对应地导数与微分的联系是![[公式]](https://www.zhihu.com/equation?tex=d%5Cboldsymbol%7Bf%7D+%3D+%5Cfrac%7B%5Cpartial+%5Cboldsymbol%7Bf%7D+%7D%7B%5Cpartial+%5Cboldsymbol%7Bx%7D%7D++d%5Cboldsymbol%7Bx%7D+)；同样通过向量化定义矩阵F对矩阵X的导数![[公式]](https://www.zhihu.com/equation?tex=%5Cfrac%7B%5Cpartial+F%7D%7B%5Cpartial+X%7D+%3D+%5Cfrac%7B%5Cpartial+%5Cmathrm%7Bvec%7D%28F%29%7D%7B%5Cpartial+%5Cmathrm%7Bvec%7D%28X%29%7D)，有![[公式]](https://www.zhihu.com/equation?tex=%5Cmathrm%7Bvec%7D%28dF%29+%3D+%5Cfrac%7B%5Cpartial+F%7D%7B%5Cpartial+X%7D+%5Cmathrm%7Bvec%7D%28dX%29)。两种布局下的导数互为转置，二者求微分的步骤是相同的，仅在对照导数与微分的联系时有一个转置的区别，读者可根据所在领域的习惯选定一种布局。

    * 从微分得到导数需要一些向量化的技巧：
  
      1. 线性：![[公式]](https://www.zhihu.com/equation?tex=%5Cmathrm%7Bvec%7D%28A%2BB%29+%3D+%5Cmathrm%7Bvec%7D%28A%29+%2B+%5Cmathrm%7Bvec%7D%28B%29)。
      2. 矩阵乘法：![[公式]](https://www.zhihu.com/equation?tex=%5Cmathrm%7Bvec%7D%28AXB%29+%3D+%28B%5ET+%5Cotimes+A%29+%5Cmathrm%7Bvec%7D%28X%29)，其中![[公式]](https://www.zhihu.com/equation?tex=%5Cotimes)表示Kronecker积，A(m×n)与B(p×q)的Kronecker积是![[公式]](https://www.zhihu.com/equation?tex=A%5Cotimes+B+%3D+%5BA_%7Bij%7DB%5D)(mp×nq)。此式证明见张贤达《矩阵分析与应用》第107-108页。
    3. 转置：![[公式]](https://www.zhihu.com/equation?tex=%5Cmathrm%7Bvec%7D%28A%5ET%29+%3D+K_%7Bmn%7D%5Cmathrm%7Bvec%7D%28A%29)，A是m×n矩阵，其中![[公式]](https://www.zhihu.com/equation?tex=K_%7Bmn%7D)(mn×mn)是交换矩阵(commutation matrix)，将按列优先的向量化变为按行优先的向量化。例如![[公式]](https://www.zhihu.com/equation?tex=K_%7B22%7D+%3D+%5Cbegin%7Bbmatrix%7D1+%26+0+%26+0%26+0+%5C%5C+0%26+0%26+1%26+0+%5C%5C+0%26+1%26+0%26+0+%5C%5C+0%26+0+%260+%26+1%5Cend%7Bbmatrix%7D%2C+%5Ctext%7Bvec%7D%28A%5ET%29%3D+%5Cbegin%7Bbmatrix%7D+A_%7B11%7D+%5C%5C+A_%7B12%7D+%5C%5C+A_%7B21%7D+%5C%5C+A_%7B22%7D%5Cend%7Bbmatrix%7D%2C+%5Ctext%7Bvec%7D%28A%29%3D%5Cbegin%7Bbmatrix%7D+A_%7B11%7D+%5C%5C+A_%7B21%7D+%5C%5C+A_%7B12%7D+%5C%5C+A_%7B22%7D%5Cend%7Bbmatrix%7D)。
      4. 逐元素乘法：![[公式]](https://www.zhihu.com/equation?tex=%5Cmathrm%7Bvec%7D%28A%5Codot+X%29+%3D+%5Cmathrm%7Bdiag%7D%28A%29%5Cmathrm%7Bvec%7D%28X%29)，其中![[公式]](https://www.zhihu.com/equation?tex=%5Cmathrm%7Bdiag%7D%28A%29)(mn×mn)是用A的元素（按列优先）排成的对角阵。

    * 复合：假设已求得![[公式]](https://www.zhihu.com/equation?tex=%5Cfrac%7B%5Cpartial+F%7D%7B%5Cpartial+Y%7D)，而Y是X的函数，如何求![[公式]](https://www.zhihu.com/equation?tex=%5Cfrac%7B%5Cpartial+F%7D%7B%5Cpartial+X%7D)呢？从导数与微分的联系入手，![[公式]](https://www.zhihu.com/equation?tex=%5Cmathrm%7Bvec%7D%28dF%29+%3D+%5Cfrac%7B%5Cpartial+F%7D%7B%5Cpartial+Y%7D%5ET%5Cmathrm%7Bvec%7D%28dY%29+%3D+%5Cfrac%7B%5Cpartial+F%7D%7B%5Cpartial+Y%7D%5ET%5Cfrac%7B%5Cpartial+Y%7D%7B%5Cpartial+X%7D%5ET%5Cmathrm%7Bvec%7D%28dX%29+)，可以推出链式法则![[公式]](https://www.zhihu.com/equation?tex=%5Cfrac%7B%5Cpartial+F%7D%7B%5Cpartial+X%7D+%3D+%5Cfrac%7B%5Cpartial+Y%7D%7B%5Cpartial+X%7D%5Cfrac%7B%5Cpartial+F%7D%7B%5Cpartial+Y%7D)。

      和标量对矩阵的导数相比，矩阵对矩阵的导数形式更加复杂，从不同角度出发常会得到形式不同的结果

    * 有一些Kronecker积和交换矩阵相关的恒等式，可用来做等价变形：
  
      1. ![[公式]](https://www.zhihu.com/equation?tex=%28A%5Cotimes+B%29%5ET+%3D+A%5ET+%5Cotimes+B%5ET)。
      2. ![[公式]](https://www.zhihu.com/equation?tex=%5Cmathrm%7Bvec%7D%28%5Cboldsymbol%7Bab%7D%5ET%29+%3D+%5Cboldsymbol%7Bb%7D%5Cotimes%5Cboldsymbol%7Ba%7D)。
      3. ![[公式]](https://www.zhihu.com/equation?tex=%28A%5Cotimes+B%29%28C%5Cotimes+D%29+%3D+%28AC%29%5Cotimes+%28BD%29)。可以对![[公式]](https://www.zhihu.com/equation?tex=F+%3D+D%5ETB%5ETXAC)求导来证明，一方面，直接求导得到![[公式]](https://www.zhihu.com/equation?tex=%5Cfrac%7B%5Cpartial+F%7D%7B%5Cpartial+X%7D+%3D+%28AC%29+%5Cotimes+%28BD%29)；另一方面，引入![[公式]](https://www.zhihu.com/equation?tex=Y+%3D+B%5ET+X+A)，有![[公式]](https://www.zhihu.com/equation?tex=%5Cfrac%7B%5Cpartial+F%7D%7B%5Cpartial+Y%7D+%3D+C+%5Cotimes+D%2C+%5Cfrac%7B%5Cpartial+Y%7D%7B%5Cpartial+X%7D+%3D+A+%5Cotimes+B)，用链式法则得到![[公式]](https://www.zhihu.com/equation?tex=%5Cfrac%7B%5Cpartial+F%7D%7B%5Cpartial+X%7D+%3D+%28A%5Cotimes+B%29%28C+%5Cotimes+D%29)。
    4. ![[公式]](https://www.zhihu.com/equation?tex=K_%7Bmn%7D+%3D+K_%7Bnm%7D%5ET%2C+K_%7Bmn%7DK_%7Bnm%7D+%3D+I)。
      5. ![[公式]](https://www.zhihu.com/equation?tex=K_%7Bpm%7D%28A%5Cotimes+B%29+K_%7Bnq%7D+%3D+B%5Cotimes+A)，A是m×n矩阵，B是p×q矩阵。可以对![[公式]](https://www.zhihu.com/equation?tex=AXB%5ET)做向量化来证明，一方面，![[公式]](https://www.zhihu.com/equation?tex=%5Cmathrm%7Bvec%7D%28AXB%5ET%29+%3D+%28B%5Cotimes+A%29%5Cmathrm%7Bvec%7D%28X%29)；另一方面，![[公式]](https://www.zhihu.com/equation?tex=%5Cmathrm%7Bvec%7D%28AXB%5ET%29+%3D+K_%7Bpm%7D%5Cmathrm%7Bvec%7D%28BX%5ETA%5ET%29+%3D+K_%7Bpm%7D%28A%5Cotimes+B%29%5Cmathrm%7Bvec%7D%28X%5ET%29+%3D+K_%7Bpm%7D%28A%5Cotimes+B%29+K_%7Bnq%7D%5Cmathrm%7Bvec%7D%28X%29)

    * 总结：若矩阵函数F是矩阵X经加减乘法、逆、行列式、逐元素函数等运算构成，则使用相应的运算法则对F求微分，再做向量化并使用技巧将其它项交换至vec(dX)左侧，对照导数与微分的联系![[公式]](https://www.zhihu.com/equation?tex=%5Cmathrm%7Bvec%7D%28dF%29+%3D+%5Cfrac%7B%5Cpartial+F%7D%7B%5Cpartial+X%7D%5ET+%5Cmathrm%7Bvec%7D%28dX%29)，即能得到导数。
  
      **特别地，若矩阵退化为向量，对照导数与微分的联系**![[公式]](https://www.zhihu.com/equation?tex=d%5Cboldsymbol%7Bf%7D+%3D+%5Cfrac%7B%5Cpartial+%5Cboldsymbol%7Bf%7D+%7D%7B%5Cpartial+%5Cboldsymbol%7Bx%7D+%7D%5ET+d%5Cboldsymbol%7Bx%7D+)**，即能得到导数。**



## 第二次课

#### 第一次课数学补充

* 强对偶性

  Slater条件：原问题为凸优化问题、且可行域中至少有一个点使得不等式严格成立

* 最优解的必要条件：KKT条件

  等高线：最优解在可行域/不在可行域内

  对偶性

  互补松弛性



#### 第二章 模型评估与选择

* 实验的方法学


##### 模型评估:给定一个数据集，如何评估一个模型的泛化能力

* 经验误差与过拟合
  * 误差：样本真实输出与预测输出间的差异，可以是错误率

    （训练误差(经验误差)、测试误差）

    泛化误差：除训练集外的所有样本上的误差

  * 过拟合(overfit)

    * 解决方法：优化目标加正则项；early stop

  * 欠拟合(underfit)

    * 解决方法：
      * 决策树：拓展分支
      * 神经网络：增加训练轮数

* 评估方法：需要测试集来测试学习机器对新样本的判别能力（假设测试集从样本真实分布中独立采样而得，则以测试误差作为泛化误差的近似）

  1. 留出法（分层采样）
  2. 交叉验证法：特例：留一法(k=1,数据集较大时，计算开销难以忍受；但结果往往准确)
  3. 自助法(Bootstrap)

  4. 调参和最终模型

    * 不同参数设置会导致学得的模型有显著差别
    * 模型选择，包括学习算法选择和参数配置的设定，后者称为调参
    * 调参的一般过程
      * 将训练集分为训练集和验证集
      * 通过网格法(在一定范围内设置一定步长来选取多个参数值比较)或随机法进行参数搜索，计算出验证集上的误差
      * 选出最佳的参数配置，在训练集上重新训练，再来做测试

* 模型选择：给定一个数据集，如何根据泛化能力，选出最好的模型或选出最好的参数配置（超参数，反映一种先验）

* 性能度量：反应任务需求。不同问题有不同的标准

  * 均方误差(RMS)（回归任务最常用）

  * 绝对误差(MAE)

  * 对分类任务

    * 错误率：分类错误的样本数占样本总数的比例
    * 精度：分类正确的样本数占样本总数的比例

    信息检索等场景经常需要衡量正例被预测出来的比率       或者预测出来的正例中正确的比率

    * 查准率P：预测出来的正例中正确的比率
    * 查全率R：正例被预测出来的比率  

    TP、FN、FP、TN

    ​	注：查准率和查全率是矛盾的度量

  * 如何衡量查准率和查全率

    * P-R曲线
  * F1度量
    
  * $F_{\beta}$度量
    
  * 多个二分类混淆矩阵：多次训练/测试，多个数据集训练/测试，多分类中每两两类别的组合

    要在n个二分类混淆矩阵中综合考虑查准率和查全率

    * 宏查准率、宏查全率、宏F1:对查全率等取平均
    * 微查准率、微查全率、微F1：先对数据(如TP)取平均，再计算

  * ROC曲线：受试者工作特征曲线

    * 真正例率TPR: 真正例中被预测为正例的概率（和查全率相同）
    * 假正例率FPR：真反例中被预测为正例的概率
    * 判断优劣
      * 包住别人的为优
      * AUC值（面积大为优）

  * 代价敏感错误率、代价曲线

    * 假反例率FNR：真正例中被预测为反例的概率

       FNR=1-TPR

* 比较检验

  * 关于性能比较
    * 测试性能不等于泛化性能
    * 测试性能随着测试集的变化而变化
    * 很多机器学习算法本身有一定的随机性
    
  * 对单个学习器泛化性能的假设检验
  
    * 二项检验
    * t检验
  
  * 对不同学习器性能进行比较： 
  
    单数据集上比较两个算法：
  
    * 交叉验证t检验
    * McNemar检验
  
    N个数据集上比较k个算法：
  
    * Friedman检验和Nemenyi后续检验
  
* 偏差与方差





## 第三次课

### 第三章 线性模型

##### 3.1 基本形式

* $f(x)=\omega^Tx+b$
* 线性模型优点
  * 形式简单、易于建模
  * 好的可解释性
  * 分线性模型的基础：引入层级结构或高维映射

##### 3.2 线性回归

* 线性回归试图学得一个线性模型以尽可能准确的预测实值输出标记

* 离散属性处理：

  * 有“序”关系：连续值
  * 无“序”关系：独热向量

* 常用性能度量：最小化均方误差

  对应 最小二乘“参数估计”
  
* 多元线性回归：一个xi有d个属性(即d维向量)

  1. $X^TX$满秩时得到3.11和3.12式

  2. 不是满秩：归纳偏好决定，如引入正则化项

  一元线性回归则是多元的特例

* 广义线性模型、联系函数：如对数线性回归

##### 3.3 对数几率回归

* 对分类任务

* 单位阶跃函数的替代：对数几率函数（3.17-3.19）

* 几率、对数几率、对数几率回归

* 对数几率回归的优点：

  1. 无需事先假设数据分布
  2. 可得到“类别”的近似概率预测
  3. 对率函数任意阶可导的凸函数，可直接应用现有数值优化方法求取最优解

* 采用极大似然法估计$\omega和b$,即最小化对数似然3.27

  需用到数值优化算法

  1. 梯度下降法
  2. 牛顿法

##### 3.4 线性判别分析（LDA）

* 二分类任务：投影到直线上，使得同类样例投影点尽可能接近，异类样例投影点尽可能远离
  * 均值向量、协方差矩阵
  * 最大化目标$J=\frac{w^TS_bw}{w^TS_ww}$,又称$S_b$与$S_w$的广义瑞利商
    * 类内散度矩阵$S_w$
    * 类间散度矩阵$S_b$
    * 利用拉格朗日乘子法得到w的解：等价转化为式3.36

* 多分类任务

  * 全局散度矩阵$S_t$

  * 类内散度矩阵$S_w$

  * 类间散度矩阵$S_b=S_t-S_w$

  * 优化目标![](D:\Typora\photos\image-20201008165417653.png)

    最后得到解W（d*(N-1)维矩阵）

* LDA又视为监督降维技术

  

知识补充

* 协方差：用来度量两个随机变量关系的统计量

  我们可以仿照方差的定义：

   ![img](https://images0.cnblogs.com/blog/397158/201307/24152520-57efb2d1a89446f1ac4691a88bea7d8e.jpg)

  协方差可以这么来定义：

   ![img](https://images0.cnblogs.com/blog/397158/201307/24152533-db0b66a3311c4f03899fad76aeb12d74.jpg)

  协方差的结果有什么意义呢？如果结果为正值，则说明两者是正相关的(从协方差可以引出“相关系数”的定义)。结果为负值就说明负相关的。如果为0，也是就是统计上说的“相互独立”。

* 协方差矩阵：协方差只能处理二维问题，那维数多了自然就需要计算多个协方差，比如n维的数据集就需要计算 n! / ((n-2)!*2) 个协方差，那自然而然的我们会想到使用矩阵来组织这些数据。给出协方差矩阵的定义：

   ![img](https://images0.cnblogs.com/blog/397158/201307/24152802-82166a021c784b12a2705e9929d70c07.jpg)

  这个定义还是很容易理解的，我们可以举一个简单的三维的例子，假设数据集有三个维度，则协方差矩阵为

   ![img](https://images0.cnblogs.com/blog/397158/201307/24152812-b6c5c191189e4ced8201d4c3569dc8f7.jpg)

  可见，协方差矩阵是一个对称的矩阵，而且对角线是各个维度上的方差

  需要明确：**协方差矩阵计算的是不同维度之间的协方差，而不是不同样本之间的**

  * 巧妙的协方差矩阵的计算方法：先让样本矩阵中心化，即每一维度减去该维度的均值，使每一维度上的均值为0，然后直接用新的到的样本矩阵乘上它的转置，然后除以(N-1)即可

##### 3.5 多分类学习

* 思想：利用二分类学习器解决多分类问题

  将多分类任务拆为若干个二分类任务，分别训练一个分类器，最后再对每个分类器的预测结果集成以获得最终多分类结果

* 拆分策略

  1. OvO（一对一）：N个类别两两配对，产生N(N-1)/2个二分类任务（每个分类器训练只用对应两类样例）；测试结果：看哪个类别被预测的最多

  2. OvR（一对其余）：一个类作为正例，其余作反例，共N个分类器；

     预测结果看：仅一个分类器预测为正类，则应为该类别；多个分类器预测为正类，则取预测置信度最大的那个

  3. MvM（多对多）：每次将若干类作正类，其他类作反类。

     正反类构造特殊设计：

     * 纠错输出码(ECOC),具有容错性
       * 工作流程
         1. 编码（二元码、三元码）：N个类别作M次划分，训练M个分类器
         2. 解码：各个分类器的结果联合起来形成测试示例的编码，选码距最小的那个作为预测结果
       * 一般ECOC编码越长，纠错能力越强
       * 同等长度编码，理论上任意两个类别间编码距离越远，纠错能力越强
     * 汉明距离：汉明距离是一个概念，它表示两个（相同长度）字对应位不同的数量，我们以d（x,y）表示两个字x,y之间的汉明距离。对两个字符串进行异或运算，并统计结果为1的个数，那么这个数就是汉明距离

##### 3.6 类别不平衡问题

* 分类任务中不同类别的训练样例数目差别很大

* 假设训练集是真实样本总体的无偏采样：$\frac{y}{1-y}>\frac{m^+}{m_-}$则预测为正例

  此称为  再缩放

* 但上述假设（训练集是真实样本总体的无偏采样）往往不成立。现在大体三类做法

  1. 欠采样：去掉一些反例（集成学习机制：反例划分为若干个集合供不同学习器使用）
  2. 过采样：增加一些正例（如插值）
  3. 阈值移动：还是利用上述假设，将其嵌入到最后的决策过程



## 第四次课

### 第四章 决策树

#### 4.1 基本流程

* 递归生成决策树算法

* 三种递归返回类型：对应把当前结点标记为叶结点
  * 当前结点包含的样本全为同一类别，无需划分：叶结点标记为该类别
  * 当前属性集为空，或所有样本在所有属性取值上取值相同，无法划分：叶结点标记为该结点所含样本数最多的类别
  * 当前结点包含的样本集合为空，不能划分：叶结点标记为其父结点所含样本最多的类别
  
* 回归树(ppt50-51):若标记为连续值（即从分类问题变成了回归问题，数据（**x**,y）中的y取值变为连续值而非离散的几类）

  叶子节点根据目标优化函数，可标记为均值、中位数等值，具体的决策树生成算法还是同书p74，但是最优划分属性选择，以及对叶节点的标记参见ppt50-51

#### 4.2 划分选择

* 如何选择最优划分属性

  目的：希望决策树的分支结点所包含的样本尽可能属于同一类别，即结点“纯度”越来越高

##### 4.2.1 信息增益

* 信息熵：度量样本集合纯度的指标（均匀分布的熵最大，见ppt6页）

  ![image-20201013220740855](D:\Typora\photos\image-20201013220740855.png)

* 属性a对样本集D划分所得的"信息增益":a有V种取值![image-20201013221134849](D:\Typora\photos\image-20201013221134849.png)

  Gain(D,a)越大，用a划分后“纯度”提升越大

* ![image-20201013221221861](D:\Typora\photos\image-20201013221221861.png)

  多个最大任选其一

* 信息增益的解释：ppt7-8 ？？？7页的变形不会

* 存在的问题：

  * “编号”也作为划分属性时，其信息增益远大于其他属性，但显然这样的决策树无泛化能力
  * 信息增益对可取值数目较多的属性有所偏好

##### 4.2.2 增益率

* 定义![image-20201013223003373](D:\Typora\photos\image-20201013223003373.png)

  a的可能取值数目越多(V越大)，IV(a)通常越大

* 增益率准则对可取值数目较少的属性有所偏好

* 实际算法考虑先从候选划分属性中找出信息增益高于平均水平的属性，再从中选择增益率最高的

##### 4.2.3 基尼指数

* 数据集D的纯度可用基尼值来度量

  ![image-20201013223923731](D:\Typora\photos\image-20201013223923731.png)

* 属性a的基尼指数：![image-20201013223946250](D:\Typora\photos\image-20201013223946250.png)

* 选择![image-20201013223957615](D:\Typora\photos\image-20201013223957615.png)

* 基尼系数：ppt17

#### 4.3 剪枝处理

* 避免过拟合

* 泛化性能提升的判断：2.2节性能评估

  如：留出法

##### 4.3.1 预剪枝

* 决策树生成过程中，对每个节点在划分前进行估计，若划分不能让决策树泛化性能提升，则停止划分，该结点标记为叶结点
* 划分的细节详见书对应节的红笔勾画
* 决策树桩：仅有一层划分的决策树
* 预剪枝：降低过拟合风险；显著减少决策树训练时间开销和测试时间开销；但带来欠拟合风险

##### 4.3.2 后剪枝

* 先从训练集生成一棵完整决策树，再自底向上(并自左向右)对非叶结点考察，若该结点对应子树替换为叶结点能让决策树泛化性能提升，则该子树换为叶结点
* 欠拟合风险小，泛化性能比预剪枝好；但训练时间开销比未剪枝和预剪枝都要大得多

#### 4.4 连续与缺失值

##### 4.4.1 连续值处理

* 二分法：

  样本集中连续属性a有n个取值，则考虑t的候选划分点集合![image-20201014191006391](D:\Typora\photos\image-20201014191006391.png)

  选择最优划分点来进行样本集合划分：![image-20201014191040572](D:\Typora\photos\image-20201014191040572.png)

  选最大化Gain(D,a,t)的t值，将其作为属性a的信息增益，参与决策树的划分选择

* 注意，连续属性在一个结点作为划分属性后，在其后代结点里仍可作为划分属性(因为结点里仍有多个取值)

##### 4.4.2 缺失值处理

* 问题

  1. 如何在属性缺失的情况下进行划分属性选择

     为每一个样本x赋予一个权重$w_x$，考虑对某一属性a，只用该属性上无缺失值的样本子集，计算方式同前面信息增益，只需最后多乘一个权重$\rho$

     ![image-20201014192109991](D:\Typora\photos\image-20201014192109991.png)

     具体参数详见书p86-87

  2. 给定划分属性，若样本在该属性上值缺失，该怎么对样本进行划分

     若不确实，该样本划入对应子结点，权重$w_x$不变；若有缺失，该样本同时划入所有子结点，样本权值变为![image-20201014192508123](D:\Typora\photos\image-20201014192508123.png)

     (对应属性值$a^v$)

#### 4.5 多变量决策树

* 单变量决策树：分类边界由若干与坐标轴平行的分段组成

* 多变量决策树(斜决策树)：每个非叶结点是一个线性分类器，是属性的线性组合，这样可以降低决策树的复杂度(高度)

* 中位数：中位数是按顺序排列的一组数据中居于中间位置的数，即在这组数据中，有一半的数据比他大，有一半的数据比他小，这里用![img](https://bkimg.cdn.bcebos.com/formula/37851974cda93192441ec18f750ea5fa.svg)来表示中位数

  则当N为奇数时，![img](https://bkimg.cdn.bcebos.com/formula/369410329320c73428e6e00bfda20122.svg)；当N为偶数时，![img](https://bkimg.cdn.bcebos.com/formula/f0f3f443e3bc3e7a8bacf371ae430bda.svg)

  一个数集中最多有一半的数值小于中位数，也最多有一半的数值大于中位数。如果大于和小于中位数的数值个数均少于一半，那么数集中必有若干值等同于中位数。

  设连续随机变量X的分布函数为![img](https://bkimg.cdn.bcebos.com/formula/4a0e5c39135afbe789dd61992e7bb0b2.svg)，那么满足条件![img](https://bkimg.cdn.bcebos.com/formula/1d6a6f6717bab8b95f2acbc0bcd0eb79.svg)的数称为X或分布F的中位数



### 第五章 神经网络

#### 5.1神经元模型

* 定义：书p97
* 神经元模型（最基本成分）
* 阈值
* M-P神经元模型；带权重的连接
* 激活函数：阶跃函数、Sigmoid函数(如对数几率函数)

#### 5.2 感知机与多层网络

* 感知机：由两层神经元组成：

  * 输入层：只是接受输入信号
  * 输出层：M-P神经元（又称“阈值逻辑单元”）
  * 能实现 与或非 等运算
  * 一般而言，给定数据集，权重和阈值可通过学习得到（也可把阈值看为一个固定输入为1.0的哑结点）
    * 学习率$\eta$; 
    * 训练过程：式5.1,5.2
  * 感知机只有一层功能神经元（有激活函数），只能处理线性可分的问题，否则感知机学习过程会振荡

* 解决非线性问题：多层感知机（多层功能神经元）

  * 隐层、隐含层：也具有激活函数

* 多层前馈神经网络

  * 表示能力：

    万能近似定理：只需要一个包含足够多神经元且具有任何“挤压”性质的激活函数(如Sigmoid函数)的隐层，多层前馈神经网络就能以任意精度逼近有界闭集上的任意连续函数

* 神经网络的学习过程，就是根据训练数据来调整神经元之间的“连接权”以及每个功能神经元的阈值

#### 5.3 误差逆传播算法

* BP算法：用于训练多层前馈神经网络(当然也可用于训练如 递归神经网络)

* BP算法的描述及其推导（书p101-104）

  * 前向预测、后向传播（参见ppt24-28）

  * 更新公式:$\Delta w_{hj}、\Delta \theta_j、\Delta v_{ih} 、\Delta \gamma_h$

  * 用到的梯度
    * 输出层神经元梯度项$g_j$
    * 隐层神经元梯度项$e_h$

* 图5.8：标准BP算法的流程

  * 目标是最小化训练集D上的累计误差
  * 但每次仅针对一个训练样例更新连接权和阈值
  * 例子：ppt28

* 累计误差逆传播算法

  * 目标是最小化训练集D上的累计误差
  * 读取整个训练集D一遍后才对参数更新，参数迭代频率更低
  * 但累积误差下降到一定程度后，进一步下降会很慢，此时标准BP算法往往会得到更好的解，尤其当训练集非常大的时候

* 小批量随机梯度下降法：ppt33（标准BP与累计BP的权衡）

* 如何设置隐层神经元的个数：靠试错法

* BP神经网络的过拟合问题的两个解决策略

  1. 早停
  2. 正则化：在误差目标函数中增加一个用于描述网络复杂度的部分，如连接权和阈值

#### ppt补充（p35页开始）

* 激活函数：Sigmoid、tanh、ReLU、Leaky ReLU、Maxout、ELU
* 损失函数：ppt38？？？？？

##### 深度学习

* 深度学习将一切表示为嵌套的层次概念体系

  * 简单概念间联系 => 复杂概念
  * 一般抽象概括 => 高级抽象表示
  * 上层信息是底层信息的组合

* 深度卷积网络(CNN)

  * 一维卷积

  * 二维卷积

  * 互关联函数

  * 卷积的计算例子、公式：ppt45

    * 步幅：框每次移动(右移、下移)的格数

    * 卷积核：每个卷积核里的内容是要学习的参数
    * 卷积层：可用不同卷积核，得到不同结果
    * 池化层：最大池化、平均池化等；步幅

* 循环神经网络（RNN）

  * 建模序列数据，如文本、时间序列等

  * 分类

    * 有限响应模型（eg：马尔科夫模型）：今天的信息只对未来N天内预测有用
    * 无限响应模型：今天的信息对未来任何时刻的预测都有用

  * 用隐状态变量$h_t$存储直到t时刻的历史数据，并用状态转移方程进行描述

    $h_t=f_w(h_{t-1},x_t)$

  * 应用：p55  一对一、多对多等

  

## 第六章 支持向量机

#### 6.1 间隔与支持向量

* 寻找划分超平面：最鲁棒、泛化能力最强

* 划分超平面的线性方程描述

  $\omega^T x+b=0$

  * $\omega$:法向量，决定超平面方向
  * b：位移项：决定超平面与原点间距离
  * 样本空间中点x到超平面$(\omega,b)$的距离：r(式6.2)

* 支持向量、间隔、式6.3

* 目标：找到具有“最大间隔”的划分超平面

  可写成6.5或6.6的形式：支持向量机(SVM)的基本型



#### 6.2 对偶问题

* 将问题(式6.6)转化为其对偶问题(6.11)

  * 依顺序求解出$\alpha_i,\omega,b$

  * KTT条件
  * 支持向量机的稀疏性：训练完成后，大部分训练样本都不需保留，最终模型只与支持向量有关
  * 最终模型：式6.12（b的确定见后）

* 求解对偶问题(6.11):二次规划算法(低效)或SMO算法

  SMO：

  * 思路
    1. 选取一对需更新的变量$\alpha_i,\alpha_j$
    2. 固定$\alpha_i,\alpha_j$外的参数，求解6.11式(max)，获得更新后的$\alpha_i,\alpha_j$

  * $\alpha_i,\alpha_j$两个变量的选择：

    ppt22（对应书p125上部）
    
  * 是要在迭代过程中使得最终所有参数（如$\alpha_i$）完全满足KKT要求

* 偏移项b的确定：6.17和6.18式

#### 6.3 核函数

* 若原始样本空间里不存在能正确划分两类样本的超平面，考虑映射至更高维的特征空间：若原始空间是有限维，即属性数有限，则一定存在一个高维特征空间使样本可分

* 映射后有$f(x)=\omega^T \phi(x)+b$

  类似6.2求得解

* 核函数：$k(x_i,x_j)$来替代$x_i,x_j$在特征空间的内积

* 支持向量展式:6.24

* 考虑到一般情况下映射$\phi$形式不知，所以核函数也无法写出

  * 什么样的函数能做核函数：定义6.1

    * 只要一个对称函数所对应的核矩阵半正定，其即可作为核函数。

    * 对一个半正定核矩阵，总能找到一个对应的映射$\phi$；即任一个核函数都隐式的定义了一个特征空间，称为“再生核希尔伯特空间”(RKHS)

  * 合适的核函数是否存在：考虑到我们并不知道特征映射的形式，所以只能尝试----核函数选择

* 常用核函数：表6.1(p128)

  组合得到的核函数

  * 线性组合
  * 直积组合
  * 结合其他函数g(x)

#### 6.4 软间隔与正则化

* 现实任务中很难确定合适的核函数使得训练样本在特征空间线性可分

* 即使恰好找到这样的核函数，也不能断定线性可分的结果不是由于过拟合造成的

* 软间隔：允许某些样本不满足约束

  硬间隔

* 优化目标变为式6.29

  * 损失函数：
    * 0/1损失函数
    * hinge损失函数
    * 指数损失
    * 对率损失
  * 用hinge损失得到的式6.35:软间隔支持向量机
    * 对应对偶问题6.40
    * 分析见p132页倒数第三段
  * 使用其他替代损失函数
    * 对率损失：几乎得到对率回归模型。与支持向量机性能相当，但也有区别(书p132-133)
  * 式6.42：结构风险与经验风险；正则化问题

#### 6.5 支持向量回归（SVR）

* 考虑回归问题,容忍宽度为$2\epsilon$的误差，在此范围内认为预测正确，不计算误差
* SVR问题化为6.43
  * 引入松弛变量得6.45
  * 再引入拉格朗日乘子得对偶问题6.51

#### 6.6 核方法

* 定理6.2 表示定理

* 发展出一系列基于核函数的学习方法，统称为“核方法”

  * 如通过"核化"(引入核函数)来将线性学习器扩展为非线性学习器
* eg：核线性判别分析(KLDA)



## 第七章 贝叶斯分类器

#### 7.1 贝叶斯决策论

* 对分类任务而言，在所有相关概率已知的理想情形下，贝叶斯决策论考虑如何基于这些概率和误判损失来选择最优的类别标记。

* 误分类损失$\lambda_{ij}$，后验概率$P(c|x)$，样本x上的条件风险$R(c_i | x)$

  目标是寻找判定准则h来最小化总体风险R(h)

  * 只需对每个样本x最小化其条件风险即可，此为贝叶斯判定准则(式7.3)
  * 贝叶斯最优分类器、贝叶斯风险:理论上限
  * 若对应二分类问题(0/1分类)，则改写为式7.5,7.6

* 现实里后验概率$P(c|x)$难以获得，要在有限训练集上估计，两种策略

  * 判别式模型：直接预测$P(c|x)$，如决策树、BP神经网络、SVM
    * 直接面对预测，准确率高，学习简单
  * 生成式模型：建模联合概率$P(x,c)$,再求$P(c|x)$，见式7.7，7.8。
    * 如朴素贝叶斯等
    * 先验概率P(c)：可由样本空间中各样本占比估计(即出现频率)
    * 类条件概率(似然)P(x|c)
    * 证据因子P(x)：与类标记无关

#### 7.2 极大似然估计

* 估计**P(x|c)**:先假设其具有某种形式的分布并且被参数向量$\theta_c$确定，则由此得到其似然：式7.9
* 对数似然：式7.10
* 例子：正态分布：p150页
* 缺陷：分布形式是假定的，这通常需要根据任务本身经验来选择所假定的分布，若假设的分布与真实分布偏差较大，那么会产生误导性结果

#### 7.3 朴素贝叶斯分类器

* 核心是根据7.6式，7.8式

* 属性条件独立性假设：所有属性相互独立，见式7.14,判定准则变为7.15
* 是对一个具体测试例子的各个属性取值来具体计算，比较简单，分为离散时的用频率估计概率，连续属性的概率密度函数
* 拉普拉斯修正：避免0的出现
* 应用：
  * 速度要求高：查表
  * 任务数据更替频繁：懒惰学习
  * 数据不断增加：增量学习

#### 7.4 半朴素贝叶斯分类器

* 核心是根据7.6,7.8式
* 仅考虑一部分属性间的相互依赖关系
* 独依赖估计ODE：超父SPODE、TAN（基于最大带权生成树）、AODE（基于集成学习）

#### 7.5 贝叶斯网

* 也称信念网，借助于
  * 有向无环图DAG来刻画属性间依赖关系
  * 用条件概率表CPT描述属性间联合概率分布
* 贝叶斯网组成：$B=<G,\Theta>$
* 结构：表达属性间的条件独立性（另一概念：边际独立性）
  * 同父结构
  * V型结构
  * 顺序结构
  * 道德图、有向分离：快速找到变量间的条件独立性
* 学习：
  * 网络结构已知：对训练样本计数即可（利用条件概率表）
  * 寻找网络结构：评分搜索（对应评分函数）
    * MDL准则
    * 式7.28：评分函数
      * AIC
      * BIC
    * 网络空间里搜索最优贝叶斯网结构：NP难
      * 贪心法
      * 给网络结构施加约束来削减搜索空间
* 推断：通过已知变量观测值来推测带查询变量的过程
  * 吉布斯采样法

#### 7.6 EM算法

* 隐变量：未观测的变量
* 最大化已观测数据的对数“边际似然”：7.35式
* 迭代步骤：p163页，两种情况，都是分为E步和M步
* 是一种非梯度优化方法



## 第八章 集成学习

#### 8.1 个体与集成

* 一组“个体学习器” -> 再结合

  * 同质：基学习器、基学习算法
  * 异质：组件学习器

* 个体学习器应好而不同：准确性、多样性

* 简单分析集成中个体分类器错误率相互独立时的集成错误率：式8.3

  但现实里不可能相互独立

* 集成学习的两大方法

  * 个体学习器间存在强依赖关系，必须串行生成的序列化方法：Boosting
  * 不存在强依赖关系，可同时生成的并行化方法：Bagging和随机森林

#### 8.2 Boosting

* 一般描述

* AdaBoost算法及其推导

  * 基本思路是：用一个基学习算法训练一个分类器$h_{t-1}$,并为该分类器确定一个权重$\alpha_{t-1}$以最小化指数损失函数，然后考虑基于当前的集成分类器(加性模型，式8.4)，在下一轮训练里使得$h_{t}$尽可能可以纠正H(x)的全部错误，即最小化8.12式，从而发现理想的$h_t$需要在分布$D_t$下最小化分类误差，所以需根据8.19式更新样本分布，再重新用基学习器算法训练

* Boosting算法

  * 重赋权法
  * 重采样法：有重启动机会
  * 基本条件的筛选

  * 此算法主要关注降低方差，对基于泛化性能相当弱的学习器能构建出很强的集成

* 梯度提升树：基于决策树的Boosting集成

  * 比较Adaboost与Gradient Boosting：ppt25
  * ppt25-32，不太懂，需网上查资料再学学

#### 8.3 Bagging与随机森林

* 主要考虑尽可能能使得基学习器有较大差异(尽管不能独立)
* Bagging
  * 利用自助采样法得到T个训练集
  * 并形式集成学习方法
  * 复杂度与基学习器同阶
  * 结合策略：分类任务：简单投票；  回归任务：简单平均
  * 且可以直接用于多分类、回归任务(标准AdaBoost只适于二分类，多分类等需做修改)
  * 包外估计的优点、用途与实现
  * Bagging主要关注降低方差

* 随机森林RF
  * 在Bagging的基础上引入了随机属性选择
  * 除了样本扰动，还带来了属性扰动，加大了学习器间的差异度
  * 训练效率优于Bagging

#### 8.4 结合策略

* 学习器组合的三方面好处
* 平均法：数值型输出(回归任务)的结合策略
  * 简单平均法
  * 加权平均法
* 投票法：分类任务的结合策略
  * 绝对多数投票法
  * 相对多数投票法
  * 加权投票法
  * 根据学习器输出值的类型，分为硬投票(类标记)和软投票(类概率)

* 学习法

  * eg：Stacking
  * 初级学习器、次级学习器(元学习器)

  * 次级学习器的训练集：通过交叉验证或留一法得到
  * 影响Stacking集成的泛化性能的因素
    * 次级学习器的输入属性表示
    * 次级学习算法
  * 多响应线性回归MLR

* 贝叶斯模型平均BMA：加权平均法的一种特殊实现

  * 与Stacking的比较

#### 8.5 多样性

* 误差-分歧分解
  * 一系列推导
  * 主要包括 学习器的分歧、学习器的平方误差
  * 最后得式子8.36，表明结论：个体学习器准确性越高，多样性越大，则集成越好
  * 但实际难以使用
* 多样性度量
  * 典型做法：考虑个体分类器的两两相似性
  * 不合度量
  * 相关系数
  * Q-统计量
  * k-统计量
  * 成对型 多样性度量的二维图表示
* 多样性增强
  * 数据样本扰动
  * 输入属性扰动
  * 输出表示扰动
  * 算法参数扰动

## 第九章 聚类

#### 9.1 聚类任务

* 一点简介

#### 9.2 性能度量

* 即想知道聚类结果的好坏
* 目标：簇内相似度高，簇间相似度低
* 外部指标：用到参考模型
  * Jaccard系数（JC)
  * FM指数（FMI）
  * Rand指数（RI)
* 内部指标
  * 一些定义：$avg(C),diam(C),d_{min}(C_i,C_j),d_{cen}(C_i,C_j)$
  * DB指数(DBI)
  * Dunn指数(DI)

#### 9.3 距离计算

* 距离度量

  需满足

  * 非负性
  * 同一性
  * 对称性
  * 直递性

* 闵可夫斯基距离：式9.18
  
  * 具体：欧式距离、曼哈顿距离(街区距离)
* 连续属性、离散属性
* 属性是否有 序关系： 有序属性(可用闵可夫斯基距离)、无序属性
* VDM：对无序属性的距离度量：式9.21
* 合并处理有序、无序属性：式9.22
* 加权距离
* 相似度度量里的距离未必满足距离度量所有性质 -> 非度量距离
* 距离度量学习

#### 9.4 原型聚类

* 假设聚类结构能通过一组原型(样本空间中具有代表性的点)刻画
* K均值算法：书p203图

#### 9.5 密度聚类

#### 9.6 层次聚类



## 第十章 降维与度量学习

#### 10.1 k近邻学习

* kNN，距离最近的k个样本
* 是懒惰学习，在训练阶段仅把样本保存起来(对比急切学习)
* 最近邻分类器的泛化错误率不超过贝叶斯最优分类器错误率的2倍

#### 10.2 低维嵌入

* 10.1里考虑的理想情况：密采样 -》 在高维属性时会导致维数灾难
* 多维缩放（MDS）算法
  * 把高维空间映射到低维空间，并且保持样本两两之间距离尽可能与原来接近
  * 具体算法即利用原高维空间的距离矩阵D得到低维空间的内积矩阵B
  * 再特征值分解得到低维表示Z
  * 算法见书p229
* 线性降维方法：书p229 式10.13
  * 新空间里的属性是原空间中属性的线性组合
  * 对低维子空间的性质要求不同，则需对W施加不同的约束

#### 10.3 主成分分析

* 是最常用的一种线性降维方式

* 找到这样的超平面，满足最近重构性和最大可分性

  分别基于上述两种性质可以得到等价的主成分分析的优化目标

* 算法见书p231

* 维度$d'$选择：用户指定；k近邻分类器的交叉验证；设置重构阈值

#### 10.4 核化线性降维

*  有时需非线性降维才可找到恰当的低维嵌入
*  核主成分分析KPCA
*  具体思路类似10.3 ：10.19式出发，目的是求W，由核函数的高维映射转而先求$\alpha$和K，再由10.20反过来求W

#### 10.5 流形学习

* 流形嵌入 -》 高维空间局部仍有欧式空间的性质

##### 等度量映射（Isomap）

* 考虑把测地线距离转变为计算 近邻连接图上两点的最短路径问题-》Djikstra，Floyd算法 -》得到距离矩阵后，由10.2节MDS算法得到低维嵌入
* 整体算法：p235 ：Isomap算法

##### 局部线性嵌入（LLE）

未看

#### 10.6 度量学习

* 尝试学习出一个合适的距离度量

* 平方欧式距离10.32 -》 每一维属性权重10.33 -》马氏距离10.34（W替换为半正定对称矩阵M，又称度量矩阵，以表示属性间的关系）

* 于是考虑对M进行学习 -》 定个目标，如希望提高近邻分类器的性能

  以NCA（近邻成分分析）为例

  * 概率投票法10.35, i样本被分到哪个类别的概率和最大
  * 最后得到优化目标 10.38

* 更进一步，考虑引入领域知识作为度量学习的优化目标：必连与勿连约束集合



## 第11章 特征选择与稀疏学习

#### 11.1 子集搜索与评价

* 特征选择：是一个重要的“数据预处理”过程 （相关特征、无关特征）

* 特征选择 和 降维 是处理高维数据（维数灾难）的两大主流技术

* 冗余特征（本章不涉及）

* 特征选择方法：产生“候选子集” -》 评价其好坏，根据评价结果产生下一个候选子集，再评价，如此反复，直到无法找到更好的候选子集

  * 子集搜索问题：根据评价结果获取下一个候选特征子集

    * 前向搜索：从只选一个特征加入候选子集开始
    * 后向搜索：每次去掉一个无关特征
    * 双向搜索

    缺点：以上方法都是贪心的

  * 子集评价问题：如何评价候选子集的好坏

    基于信息增益指标

* 决策树也可用于特征选择，实际上也算是 前向搜索与信息熵相结合的方法

* 常见的三类特征选择方法：过滤式、包裹式、嵌入式

#### 11.2 过滤式选择

* 先做特征选择，再训练学习器，两者完全分离

* Relief方法（为2分类设计）：用 相关统计量 来度量特征的重要性

  关键:如何确定相关统计量，式11.3及其后面两段话的解释

  猜中近邻、猜错近邻

* Relief-F（可处理多分类问题）：11.4

#### 11.3 包裹式选择

* 直接把最终要使用的学习器的性能作为特征子集的评价准则
* LVW算法：算法见p251图， 但若有时间限制，很可能给不出解
* 另一种随机化方法：蒙特卡罗方法（p251 旁白）

#### 11.4 嵌入式选择与$L_1$正则化

* 考虑把特征选择过程与学习器训练过程融为一体

* 考虑LR模型，损失函数为平方误差

  * 样本特征很多，样本数较少时，容易过拟合
  * L2范数正则化 （11.6，称为 岭回归），能显著降低过拟合风险 
  * L1范数 11.7，称为LASSO
  * 也可用任意$L_p$范数
  * 正则化可参考书p133 6.5节上方最后三行话

* L1范数优势：易于获得稀疏解，即求得的w有更少的非零分量，这样相当于做出了特征选择

  形式证明参考p253

* L1正则化问题的求解：近端梯度下降（PGD）

  * 优化目标：11.8
  * 迭代公式：11.12，11.13   -》 迭代闭式解11.14



#### 11.5 稀疏表示与字典学习

#### 11.6 压缩感知

* 根据部分信息来恢复全部信息
* Nyquist采样定理



## 第13章 半监督学习

#### 13.1 未标记样本

* 主动学习：引入额外的专家知识，通过与外界的交互将部分未标记样本转变为有标记样本

* 若未标记样本与有标记样本是从同样的数据源独立同分布采样得到  -> 半监督学习

  基本思路：相似的样本有相似的输出

  * ”聚类假设“ 以利用未标记样本
  * ”流形假设“

* 半监督学习 分为 纯半监督学习 和 直推学习

#### 13.2 生成式方法

* 基于生成式模型的方法：假设所有数据是由同一个潜在模型生成的

  模型假设不同，则方法也不同

* 高斯混合模型：未标记数据的样本标记 作为 模型的缺失参数 ，通过EM算法迭代至收敛 从而求解

  对数似然式：13.4

* 此外，混合专家模型、朴素贝叶斯模型都是生成式模型

* 关键：模型假设必须准确，与真实数据分布吻合

#### 13.3 半监督SVM

* S3VM：找到能把有标记样本分开， 且穿过数据低密度区域的划分超平面 

* TSVM: 尝试将每个未标记样本作为正例或反例

  式13.9 ，仍是二分类问题

  * 求解方式：局部搜索，算法见p300图

#### 13.4 图半监督学习

* 把数据集每个样本看为图上一个结点，样本间相似度作为两结点间的权重，有标记样本染了色，未标记样本没染色 -》 图上颜色扩散问题 -》 引入矩阵运算

* 亲和矩阵的定义：式13.11

* 目标：从图G学得一个实值函数 -》 由sign()映射到标记

* 针对二分类问题的标记传播

  考虑到 相似的样本具有相似的标记 ，定义关于f的能量函数(式13.12),

  现要考虑最小化能量函数：有标记的则取等，未标记的取极值点，求解得到式13.17，于是可预测未标记样本的值

* 适用于多分类问题的标记传播

  基本类似，只是把F从 (l+u)*1 扩展到了 (l+u) * Y

  标记传播矩阵S，迭代计算式13.19 -》 收敛至13.20，迭代求解的算法见p303图

  迭代求解的原理：p303底部 - p304顶部两段话

* 图半监督学习方法的缺陷

## 第14章 概率图模型

#### 14.1 隐马尔可夫模型（HMM)

* 推断 的定义
* 生成式模型、判别式模型的任务
* 概率图模型分类：有向无环图（贝叶斯网，变量间存在显式因果关系）；无向图（马尔科夫网，变量间无显式因果关系）
* 隐马尔可夫模型HMM 是动态贝叶斯网，用于时序数据建模 （静态贝叶斯网见7.5节）
  * 变量：隐变量（状态变量，状态空间离散）、观测变量（本节讨论离散型）
  * 变量间依赖关系：图14.1（书320），所有变量的联合概率分布式14.1
  * HMM的3组参数：状态转移概率矩阵A、输出观测概率矩阵B、初始状态概率（向量$\pi$）
  * 观测序列的产生：p321
  * HMM是一个生成模型
* 人们关注的HMM的三个基本问题（ppt11）
  * 概率计算问题：给定模型，如何计算产生某个观测序列的概率,参考https://blog.csdn.net/Joyliness/article/details/79593485

    以下若有不直观的式子推导，参考ppt p14-15 笔记

    * 目标：求解$P(O∣λ),其中O为观测序列，\lambda为模型参数 $

    * 定义前向概率  $α_t(i)=P(o_1,...,o_t,i_t=q_i∣λ)$

      递推公式：$α_{t+1}(j)=b_j(o_{t+1})∑_{i=1}^Na_{ij}α_t(i)$

      从而$P(O∣λ)=∑_{i=1}^Nα_T(i)$    -》 前向算法

      计算量减少的原因在于，每一次计算直接引用前一个时刻的计算结果，避免重复计算

    * 定义后向概率![image-20210226101717925](D:\科大\大三上\机器学习概论\image\image-20210226101717925.png)

      递推式：![image-20210226102141930](D:\科大\大三上\机器学习概论\image\image-20210226102141930.png)

      从而![image-20210226102329945](D:\科大\大三上\机器学习概论\image\image-20210226102329945.png) -》后向算法

    * 前向-后向算法

      ![image-20210226102646704](D:\科大\大三上\机器学习概论\image\image-20210226102646704.png)

    * 其他概率的计算，其中第二项又记为 $\gamma_t(i)$

      ![image-20210226103405311](D:\科大\大三上\机器学习概论\image\image-20210226103405311.png)

  * 预测问题：如何根据模型、观测序列（$O_T$），推断出隐藏模型状态序列($S_T$)

    * 近似算法：直接由![image-20210226104943639](D:\科大\大三上\机器学习概论\image\image-20210226104943639.png)

      取每个时刻下概率最大的那个状态即可

    * 维特比算法：（动态规划求概率最大路径）

      * 具体思路即：若最大路径$S_T$上有t时刻状态为i时 整个路径最大，那么只需再分别求 (1,t) 和(t+1,T)的最大概率路径，公式见ppt p20

      * 计算方法：自底向上法

        * 局部最佳路径、局部概率

        * ![image-20210226111056976](D:\科大\大三上\机器学习概论\image\image-20210226111056976.png)

          ![image-20210226111107495](D:\科大\大三上\机器学习概论\image\image-20210226111107495.png)

          计算示例参考https://blog.csdn.net/zb1165048017/article/details/48578183

  * 学习问题：已知观测序列，如何调整模型参数使得该序列出现概率最大

    若训练数据包含观测序列和状态序列，则HMM的学习问题非常简单，是监督学习算法。
    若训练数据只包含观测序列，则HMM的学习问题需要使用EM算法求解，是非监督学习算法。

    * 用EM算法求解HMM学习问题 之  Baum-Welch算法

      **EM算法参考书7.6节**

      EM 算法是求参数极大似然估计的一种方法，它可以从非完整数据集中对参数进行 MLE 估计，是一种非常简单实用的学习算法。这种方法可以广泛地应用于处理缺损数据，截尾数据，带有讨厌数据等所谓的不完全数据(incomplete data)。

      假定集合Z = (X,Y)由观测数据 X 和未观测数据Y 组成，Z = (X,Y)和 X 分别称为完整数据和不完整数据。假设Z的联合概率密度被参数化地定义为P(X，Y|Θ)，其中Θ 表示要被估计的参数。Θ 的最大似然估计是求不完整数据的对数似然函数L(X;Θ)的最大值而得到的：

      $L(Θ; X )= log P(X |Θ) = ∫log P(X ,Y |Θ)dY ；(1)$

      EM算法包括两个步骤：由E步和M步组成，它是通过迭代地最大化完整数据的对数似然函数$Lc( X;Θ )$的期望来最大化不完整数据的对数似然函数，其中：

      　　　$Lc(X;Θ) =log P(X，Y |Θ) ； (2)$

      假设在算法第t次迭代后Θ 获得的估计记为Θ(t ) ，则在（t+1）次迭代时，

      　　E-步：计算完整数据的对数似然函数的期望，记为：

      　　　$Q(Θ |Θ (t) ) = E[Lc(Θ;Z)|X;Θ(t) ]； (3)$

      　　M-步：通过最大化Q(Θ |Θ(t) ) 来获得新的Θ 。

      　　通过交替使用这两个步骤，EM算法逐步改进模型的参数，使参数和训练样本的似然概率逐渐增大，最后终止于一个极大点。

      **Baum-Welch算法**具体步骤（参考ppt和https://blog.csdn.net/u014688145/article/details/53046765）

      1. E步：初始化参数值$\pi$, 并计算对数似然关于隐状态序列Y的期望$Q(\lambda | \lambda^t)$， 参考ppt24

         得到![image-20210226120211013](D:\科大\大三上\机器学习概论\image\image-20210226120211013.png)

      2. M步：极大化Q函数$Q(\lambda | \lambda^t)$求模型参数A,B,π

         由于要极大化的参数在上式中单独地出现在3个项中，所以只需要对各项分别极大化。

         利用拉格朗日乘子法求解三个概率参数 ，参考ppt25

#### 14.2 马尔可夫随机场（MRF）

* 是典型的马尔可夫网

* 势函数（因子）、团、极大团、每个结点至少出现在一个极大团中

* 联合概率定义：式14.2

* 变量数目太多- -》 团数太多 -》 基于极大团定义式14.3

* 分离集、全局马尔可夫性，验证见p324

  推论：

  * 局部马尔可夫性
  * 成对马尔可夫性

* 势函数的定义：p325

#### 14.3 条件随机场(CRF)

* 是判别式无向图模型，与MRF比较
  * 前者对条件概率建模，有观测变量； 后者对联合概率建模
  * 但二者均通过团上的势函数定义概率
  
* 预测的标记变量可以是结构型变量：如词性标注里的线性序列结构（对应链式条件随机场，p326)；语法分析里的树形结构

* 条件概率的定义：p327；    
  * 转移特征函数
  * 状态特征函数

* 条件随机场的内积形式以及矩阵形式  ,内积和矩阵形式的区别仅仅在于对于书式14.11中的二重求和的求和顺序不同2

  统计学习方法》中的 阐述：

  ![img](https://pic2.zhimg.com/80/v2-cfdd9d5b43b8d5b793065cc3bed2ea71_720w.jpg)

  ![img](https://pic3.zhimg.com/80/v2-1ec4a66dc1ed53598317e1bfc7641dce_720w.jpg)

* 参考  https://www.cnblogs.com/pinard/p/7055072.html以及李航统计学习方法

* 代码实现：https://zhuanlan.zhihu.com/p/44042528

* 前向-后向算法

* 预测问题

* 学习问题

#### 14.4 学习与推断

#### 14.5 近似推断

#### 14.6 话题模型

* 词、文档、话题、词袋

* 隐狄利克雷分配模型

  * K个话题，T篇文档，N个词的词典

  * 图14.12

    LDA从生成式角度看待：如何由话题生成文档

* 由极大似然估计寻找LDA模型参数 ：吉布斯采样或变分法求解

* 若模型参数已知——》吉布斯采样、变分法的近似推断

#### 14.7（补充）

* LDA涉及到的先验知识有：二项分布、Gamma函数、Beta分布、多项分布、Dirichlet分布、马尔科夫链、MCMC、Gibs Sampling、EM算法等
* 话题模型补充参考lab5报告及其后面的两个参考文献





## 第 16 章 强化学习

#### 16.1 任务与奖赏

* 强化学习通常用马尔科夫决策过程MDP来描述

  四元组E=<X,A,P,R> : 状态空间X，动作空间A，转移函数P，奖赏函数R

* 区分机器与环境：在环境中状态的转移、奖赏的返回是不受机器控制的，机器只能通过选择要执行的动作来影响环境，也只能通过观察转移后的状态和返回的奖赏来感知环境

* 机器学得一个策略：在状态x下知道要执行的动作。确定性策略和随机性策略

  学习的目的就是找到能使长期累积奖赏最大化的策略，常用的有"T步累积奖赏"，"$\gamma$折扣累积奖赏"

* 强化学习与监督学习的差别

#### 16.2 K-摇臂赌博机

* 最简单的情形：最大化单步奖赏，即K-摇臂赌博机模型，目标是最大化自己的奖赏

* 方法：仅探索法(把尝试机会平均分给所有摇臂，为了获取每个摇臂的期望奖赏)； 仅利用法(为了执行奖赏最大的动作)

  但都无法使得最终的累积奖赏最大化

* 探索-利用窘境

* $\epsilon$-贪心：以概率$\epsilon$来平衡探索与利用，并采用增量式更新，算法见p375

* Softmax：引入温度，并且基于当前已知的摇臂平均奖赏来对探索和利用进行折中

* 简单扩展到多步强化学习任务：最后一段话(p376)

#### 16.3 有模型学习

* 机器已对环境建模，即16.1里MDP四元组已知

* 策略评估：状态值函数V(), 状态-动作值函数Q(),各自对应的累积奖赏计算公式：16.7和16.8

  根据公式的递归性，考虑用动态规划算法（自底向上），p379算法

* 策略改进：

  * 理想化策略：能最大化累积奖赏16.11
  * 最优策略对应的值函数V()称为最优值函数，注意策略空间可能有约束
  * 最优Bellman等式
  * 具体改进方式：
    * 首先最大化16.11的累积奖赏，得到一个的策略（随机性策略)
    * 此后，对策略的每一次选择确定化，选择当前最优的动作

* 策略迭代与值迭代

#### 16.4 免模型学习

* 蒙特卡罗强化学习
  * p384：同策略蒙特卡罗强化学习算法
  * 重要性采样
  * 异策略蒙特卡罗强化算法
* 时序差分学习：结合动态规划与蒙特卡罗方法，提高效率
  * Sarsa算法（同策略)
  * Q-learning算法：异策略

#### 16.5 值函数近似

#### 16.6 模仿学习

## 附录

### B 优化

#### B.1 拉格朗日乘子法

* https://zhuanlan.zhihu.com/p/154517678
* 拉格朗日乘子法（Lagrange multipliers）是一种寻找多元函数**在一组约束下**的**极值**的方法。
* 通过引入拉格朗日乘子，可将有 d 个变量与 k 个约束条件的最优化问题转化为具有 d+k 个变量的无约束优化问题求解。
* 分为等式约束和不等式约束

##### 等式约束

当约束条件是等式的时候，例子：

- 求 ![[公式]](https://www.zhihu.com/equation?tex=f%28%5Ctextbf+x%29%3Dx_1%5E2%2Bx_2%5E2) 的最小值，约束条件： ![[公式]](https://www.zhihu.com/equation?tex=g%28%5Ctextbf+x%29+%3D+x_1%5E2x_2-3%3D0)
- ![[公式]](https://www.zhihu.com/equation?tex=minf%28%5Ctextbf+x%29%5C+%5C+%5C+%5C+%5C+s.t.+g%28%5Ctextbf+x%29%3D0)

直观操作步骤：

- 画出约束条件曲线 ![[公式]](https://www.zhihu.com/equation?tex=g%28%5Ctextbf+x%29+%3D+x_1%5E2x_2-3%3D0)
- 画出 ![[公式]](https://www.zhihu.com/equation?tex=f%28%5Ctextbf+x%29%3Dx_1%5E2%2Bx_2%5E2) 的等高线
- 找到 ![[公式]](https://www.zhihu.com/equation?tex=f%28%5Ctextbf+x%29) 和 ![[公式]](https://www.zhihu.com/equation?tex=g%28%5Ctextbf+x%29+) 相交的点中的 ![[公式]](https://www.zhihu.com/equation?tex=f%28%5Ctextbf+x%29) 取得最小值的点（**相切的位置**），输出此时的 ![[公式]](https://www.zhihu.com/equation?tex=f%28%5Ctextbf+x%29) 值。

![img](https://pic3.zhimg.com/80/v2-4d3f117cf37a4975d6989927c4a3a596_1440w.jpg)

那么，我们能得到什么信息呢？

- 约束曲线与极值曲线**相切**的点为极值点 ![[公式]](https://www.zhihu.com/equation?tex=%5Ctextbf+x%5E%2A) 。

- - 对于约束曲面上的任意点 ![[公式]](https://www.zhihu.com/equation?tex=%5Ctextbf+x) ，该点的梯度 ![[公式]](https://www.zhihu.com/equation?tex=%5Cnabla+g%28%5Ctextbf+x%29) 正交于约束曲面。
  - 在最优点 ![[公式]](https://www.zhihu.com/equation?tex=%5Ctextbf+x%5E%2A) ，目标函数在该点的梯度 ![[公式]](https://www.zhihu.com/equation?tex=%5Cnabla+f%28%5Ctextbf+x%5E%2A%29) 正交于约束曲面。



由此可知，在最优点 ![[公式]](https://www.zhihu.com/equation?tex=%5Ctextbf+x%5E%2A) ，梯度 ![[公式]](https://www.zhihu.com/equation?tex=%5Cnabla+g%28%5Ctextbf+x%29) 和 ![[公式]](https://www.zhihu.com/equation?tex=%5Cnabla+f%28%5Ctextbf+x%29) 的方向必相同或相反，即存在 ![[公式]](https://www.zhihu.com/equation?tex=%5Clambda+%5Cne+0) ，使得： ![[公式]](https://www.zhihu.com/equation?tex=%5Cnabla+f%28%5Ctextbf+x%5E%2A%29+%2B+%5Clambda%5Cnabla+g%28%5Ctextbf+x%5E%2A%29%3D0) ， ![[公式]](https://www.zhihu.com/equation?tex=%5Clambda) 称之为**拉格朗日乘子**。

所以在求解 ![[公式]](https://www.zhihu.com/equation?tex=f%28x%29) 极值的问题上，我们相当于有两个条件了：

![[公式]](https://www.zhihu.com/equation?tex=%5Cbegin%7Bequation%7D%5Cleft%5C%7B%5Cbegin%7Barray%7D%7Bl%7D++%5Cnabla+f%28%5Ctextbf+x%29+%2B+%5Clambda%5Cnabla+g%28%5Ctextbf+x%29%3D0+%5C%5C+g%28%5Ctextbf+x%29+%3D+x_1%5E2x_2-3%3D0+%5Cend%7Barray%7D%5Cright.%5Cend%7Bequation%7D)



对于上面的具体例子，我们这两个条件，可联立方程： ![[公式]](https://www.zhihu.com/equation?tex=%5Cbegin%7Bequation%7D%5Cleft%5C%7B%5Cbegin%7Barray%7D%7Bl%7D+%5Cnabla+f%28%5Ctextbf+x%29%3D%5Clambda+%5Cnabla+g%28%5Ctextbf+x%29+%5C%5C+g%28%5Ctextbf+x%29%3Dx_1%5E%7B2%7D+x_2-3%3D0+%5Cend%7Barray%7D%5Cright.%5Cend%7Bequation%7D) ，求解得到： ![[公式]](https://www.zhihu.com/equation?tex=%5Cbegin%7Bequation%7D%5Cleft%5C%7B%5Cbegin%7Barray%7D%7Bl%7D+%5Cleft%28%5Cbegin%7Barray%7D%7Bc%7D+2+x_1+%5C%5C+2+x_2+%5Cend%7Barray%7D%5Cright%29%3D%5Clambda%5Cleft%28%5Cbegin%7Barray%7D%7Bc%7D+2+x_1+x_2+%5C%5C+x_1%5E%7B2%7D+%5Cend%7Barray%7D%5Cright%29+%5C%5C+x_1%5E%7B2%7D+x_2-3%3D0+%5Cend%7Barray%7D+%5CLongrightarrow%5Cleft%5C%7B%5Cbegin%7Barray%7D%7Bl%7D+x_1+%5Capprox+%5Cpm+1.61+%5C%5C+x_2+%5Capprox+1.1+%5C%5C+%5Clambda+%5Capprox+0.87+%5Cend%7Barray%7D%5Cright.%5Cright.%5Cend%7Bequation%7D)

上面的步骤就是用了拉格朗日乘子法进行求解的，最终求得了极值点。

* 仔细观察上面推出极值点的两个条件：

  ![[公式]](https://www.zhihu.com/equation?tex=%5Cbegin%7Bequation%7D%5Cleft%5C%7B%5Cbegin%7Barray%7D%7Bl%7D++%5Cnabla+f%28%5Ctextbf+x%29+%2B+%5Clambda%5Cnabla+g%28%5Ctextbf+x%29%3D0+%5C%5C+g%28%5Ctextbf+x%29+%3D0+%5Cend%7Barray%7D%5Cright.%5Cend%7Bequation%7D)

  - 他最简单的原函数是不是就是： ![[公式]](https://www.zhihu.com/equation?tex=f%28%5Ctextbf+x%29%2B%5Clambda+g%28%5Ctextbf+x%29) ，上面第一行是对 ![[公式]](https://www.zhihu.com/equation?tex=%5Ctextbf+x) 的求导等于0，第二行是对 ![[公式]](https://www.zhihu.com/equation?tex=%5Clambda) 的求导等于0，哎呦，正合适哦是不是。
  - 所以我们定义的拉格朗日函数为： ![[公式]](https://www.zhihu.com/equation?tex=L%28%5Ctextbf+x%2C%5Clambda%29%3Df%28x%29%2B%5Clambda+g%28x%29)
  - 将其对 ![[公式]](https://www.zhihu.com/equation?tex=%5Ctextbf+x) 的偏导数 ![[公式]](https://www.zhihu.com/equation?tex=%5Cnabla+_xL%28%5Ctextbf+x%2C%5Clambda%29) 置零，就得到 ![[公式]](https://www.zhihu.com/equation?tex=%5Cnabla+g%28%5Ctextbf+x%5E%2A%29+%2B+%5Clambda%5Cnabla+f%28%5Ctextbf+x%5E%2A%29%3D0)
  - 将其对 ![[公式]](https://www.zhihu.com/equation?tex=%5Clambda) 的偏导数 ![[公式]](https://www.zhihu.com/equation?tex=%5Cnabla+_%5Clambda+L%28%5Ctextbf+x%2C%5Clambda%29) 置零，就得到 ![[公式]](https://www.zhihu.com/equation?tex=g%28x%29%3D0)
  - 都对拉格朗日函数的偏导置零，能求出满足条件的极值点，说明什么？这就是在求 ![[公式]](https://www.zhihu.com/equation?tex=L%28%5Ctextbf+x%2C%5Clambda%29) 的极值点

##### 不等式约束

* 当约束条件是不等式的时候，存在不等式约束时，最优解的位置只有两种情况，一种是最优解在不等式约束的边界上，另一种就是不等式约束的区域内，需要分为两种情况讨论

* **情况一**：最优解在 ![[公式]](https://www.zhihu.com/equation?tex=g%28%5Ctextbf+x%29%3C0) 区域内，条件无作用，直接令 ![[公式]](https://www.zhihu.com/equation?tex=%5Cnabla+f%28%5Ctextbf+x%29%3D0) 求解。

* - 这等价于将拉格朗日函数的 ![[公式]](https://www.zhihu.com/equation?tex=%5Clambda) 置零，即 ![[公式]](https://www.zhihu.com/equation?tex=L%28%5Ctextbf+x%2C%5Clambda%29%3Df%28%5Ctextbf+x%29%2B%5Clambda+g%28%5Ctextbf+x%29%3Df%28%5Ctextbf+x%29) ，再对 ![[公式]](https://www.zhihu.com/equation?tex=L%28%5Ctextbf+x%2C%5Clambda%29) 求极值。

* **情况二**：最优解在 ![[公式]](https://www.zhihu.com/equation?tex=g%28%5Ctextbf+x%29%3D0) 上，相当于等式约束，曲线相切处为最优解。

* - 即满足 存在一个 ![[公式]](https://www.zhihu.com/equation?tex=%5Clambda) 使得![[公式]](https://www.zhihu.com/equation?tex=%5Cnabla+f%28x%5E%2A%29+%2B+%5Clambda%5Cnabla+g%28x%5E%2A%29%3D0+) ，但是这里的 ![[公式]](https://www.zhihu.com/equation?tex=%5Clambda) 的取值范围就不是不等于0了，这个时候最优解处 ![[公式]](https://www.zhihu.com/equation?tex=%5Cnabla+f%28x%5E%2A%29) 的方向必须与 ![[公式]](https://www.zhihu.com/equation?tex=%5Cnabla+g%28x%5E%2A%29) 的**相反**，即存在常数 ![[公式]](https://www.zhihu.com/equation?tex=%5Clambda%3E0) ，使得 ![[公式]](https://www.zhihu.com/equation?tex=%5Cnabla+f%28x%5E%2A%29+%2B+%5Clambda%5Cnabla+g%28x%5E%2A%29%3D0+)。

- **整合情况**：第一种 ![[公式]](https://www.zhihu.com/equation?tex=%5Clambda%3D0) ，第二种 ![[公式]](https://www.zhihu.com/equation?tex=g%28%5Ctextbf+x%29%3D0) ，所以两种情况都满足 ![[公式]](https://www.zhihu.com/equation?tex=%5Clambda+g%28%5Ctextbf+x%29%3D0)。

- 因此，在约束条件 ![[公式]](https://www.zhihu.com/equation?tex=g%28%5Ctextbf+x%29%5Cleq0) 下最小化 ![[公式]](https://www.zhihu.com/equation?tex=f%28%5Ctextbf+x%29) ，可转化成在如下三个约束下最小化拉格朗日函数 ![[公式]](https://www.zhihu.com/equation?tex=L%28%5Ctextbf+x%2C%5Clambda%29%3Df%28%5Ctextbf+x%29%2B%5Clambda+g%28%5Ctextbf+x%29) ：

- - ![[公式]](https://www.zhihu.com/equation?tex=%5Cbegin%7Bequation%7D%5Cleft%5C%7B%5Cbegin%7Barray%7D%7Bl%7D+g%28%5Ctextbf%7Bx%7D%29+%5Cleqslant+0+%5C%5C+%5Clambda+%5Cgeqslant+0+%5C%5C+%5Clambda+g%28%5Ctextbf+x%29%3D0+%5Cend%7Barray%7D%5Cright.%5Cend%7Bequation%7D) ，这个式子成为 ![[公式]](https://www.zhihu.com/equation?tex=Karush-Kuhn-Tucker%28KKT%29) 条件。

## 作业解答与疑问

==3.2 ：3.18式为啥求二阶导后为一个数而不是一个矩阵？？==

3.7：

首先原书中提到：

> 对同等长度的编码，理论上来说，任意两个类别之间的编码距离越远，则纠错能力越强。因此，在码长较小时可根据这个原则计算出理论最优编码。

其实这一点在论文中也提到，“假设任意两个类别之间最小的海明距离为 ![[公式]](https://www.zhihu.com/equation?tex=d) ，那么此纠错输出码最少能矫正 ![[公式]](https://www.zhihu.com/equation?tex=%5Cleft%5B+%5Cfrac%7Bd-1%7D%7B2%7D+%5Cright%5D) 位的错误。

此外，一个好的纠错输出码应该满足两个条件：

1. 行分离。任意两个类别之间的codeword距离应该足够大。
2. 列分离。任意两个分类器 ![[公式]](https://www.zhihu.com/equation?tex=f_%7Bi%7D%2Cf_%7Bj%7D) 的输出应相互独立，无关联。这一点可以通过使分类器 ![[公式]](https://www.zhihu.com/equation?tex=f_%7Bi%7D) 编码与其他分类编码的海明距离足够大实现，且与其他分类**编码的反码的海明距离也足够大（有点绕。）。**

对上述第二点：

> 如果两个分类器的编码类似或者完全一致，很多算法（比如C4.5）会有相同或者类似的错误分类，如果这种同时发生的错误过多，会导致纠错输出码失效。（翻译原论文）

个人理解就是：若增加两个类似的编码，那么当误分类时，就从原来的1变成3，导致与真实类别的codeword海明距离增长。极端情况，假设增加两个相同的编码，此时任意两个类别之间最小的海明距离不会变化依然为 ![[公式]](https://www.zhihu.com/equation?tex=d) ，而纠错输出码输出的codeword与真实类别的codeword的海明距离激增（从1变成3）。所以如果有过多同时发出的错误分类，会导致纠错输出码失效。

另外，两个分类器的编码也不应该互为反码，因为很多算法（比如C4.5，逻辑回归）对待0-1分类其实是对称的，即将0-1类互换，最终训练出的模型是一样的。也就是说两个编码互为补码的分类器是会同时犯错的。同样也会导致纠错输出码失效。

当然当类别较少时，很难满足上面这些条件。如上图中，一共有三类，那么只有 ![[公式]](https://www.zhihu.com/equation?tex=2%5E%7B3%7D%3D8) 中可能的分类器编码（ ![[公式]](https://www.zhihu.com/equation?tex=f_%7B0%7D-f%7B7%7D) ），其中后四种（ ![[公式]](https://www.zhihu.com/equation?tex=f_%7B4%7D-f_%7B7%7D) ）是前四种的反码，都应去除，再去掉全为0的![[公式]](https://www.zhihu.com/equation?tex=f_%7B0%7D)，就只剩下三种编码选择了，所以很难满足上述的条件。事实上，对于 ![[公式]](https://www.zhihu.com/equation?tex=k) 种类别的分类，再去除反码和全是0或者1的编码后，就剩下 ![[公式]](https://www.zhihu.com/equation?tex=2%5E%7Bk%7D-1) 中可行的编码。