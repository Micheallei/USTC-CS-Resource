## 前馈神经网络

#### 神经元

* MP神经元：激活函数为0/1阶跃函数
* 激活函数要求的性质
* sigmoid型
  * 两端饱和
  * 常用的有Logistic和Tanh
  * 非零中心化和零中心化
* Hard-Sigmoid型
* ReLU
  * Leaky ReLu
  * PReLU
  * ELU
  * Softplus
* Swish
* GELU
* Maxout
* 常见激活函数及其导数：ppt23

#### 网络结构

* 前馈网络：全连接前馈网络、卷积神经网络

* 记忆网络：循环神经网络、Hopfield网络、玻尔兹曼机、受限玻尔兹曼机

  记忆增强神经网络

* 图网络：图卷积网络、图注意力网络、消息传递神经网络

* 网络结构：神经网络有多少单元，以及这些单元如何连接；网络深度与每一层的宽度

  更深的网络通常每一层使用更少的单元数和更少的参数，且有更强的泛化能力，但也更难以优化

#### 前馈神经网络(FNN)

又叫做全连接神经网络、多层感知机

* 输入层、隐藏层、输出层

* 公式及图像表示：书p92

* 通用近似定理

* 应用到机器学习：把神经网络看做特征转换方法，并且最后一层来进行Logistic回归(二分类)/softmax回归(多分类)

  softmax以及交叉熵损失函数参考书p28以及https://zhuanlan.zhihu.com/p/25723112

* ==单峰回归与多峰回归：ppt38-39==不太懂

* 参数学习：通过定义损失函数，给定训练集，通过梯度下降法来优化前馈神经网络的参数，公式为4.46,4.48

  但每个参数求偏导太低效，一般考虑用反向传播算法

* 随机梯度下降、小批量随机梯度下降

#### 反向传播算法

* 参考书对应章节

#### 自动梯度计算

* 自动计算梯度的方法
  * 数值微分
  * 符号微分
  * 自动微分
    * 复杂函数拆分+链式法则
    * 计算图
    * 两种模式：前向模式和后向模式
    * 动态计算图和静态计算图

#### 优化问题

* 神经网络的参数学习的难点
  * 非凸优化问题
  * 梯度消失问题





0.3930： 两层，relu+sigmoid

0.3821





## 卷积神经网络（CNN）

#### 概述

* 全连接前馈神经网络的问题

  * 参数太多
  * 没有考虑 局部不变性特征

* 感受野机制

* 卷积神经网络一般由 卷积层、汇聚层、全连接层交叉堆叠而成的前馈神经网络

  特征：局部连接、权重共享、汇聚  -》 所以具有一定程度上的平移、缩放、旋转不变性

#### 卷积

* 一维卷积：滤波器/卷积核； 滑动窗口大小K；简单移动平均

  常用在信号处理

* 二维卷积：常用在图像处理

  * 均值滤波
  * 高斯滤波器：平滑去噪
  * 也可提取边缘特征
  * 特征映射

* 互相关

* 卷积的变种

  * 步长、零填充
  * 卷积类型：窄卷积、宽卷积、等宽卷积

* 卷积的数学性质

  * 可交换性
  * 导数计算

#### 卷积神经网络

* 卷积代替全连接：局部连接、权重共享 -> 参数大大减小

  * 一个卷积核只捕捉输入数据里的一种特定局部特征

* 卷积层：用图像处理里的二维卷积举例

  一个输出特征映射由D个输入特征映射经二维卷积并加上同一个偏置后，再作用于激活函数后得到。

* 汇聚层(Pooling layer/Subsampling layer):为了减少每一个特征映射的大小，即二维矩阵大小

  * 最大汇聚
  * 平均汇聚

* 卷积网络的整体结构：图5.9

#### 参数学习

* 同样利用误差项来进行误差反向传播算法

#### 几种典型的卷积神经网络

* LeNet-5:手写数字识别
* AlexNet：ImageNet图像分类
* Inception网络：Inception模块，一个卷积层包含多个大小不同的卷积操作，并拼接起来作为输出特征映射
  * GoogLeNet
* 残差网络ResNet：把目标函数拆分成 恒等函数和残差函数

#### 其他卷积方式

* 转置卷积：

  * 仿射变换：高维到低维的映射，转换矩阵W

    用$W^T$实现低维到高维的映射

  * 同理，卷积操作也可写为仿射变换的形式

  * 将低维特征映射到高维特征的卷积操作成为转置卷积

* 微步卷积：在输入特征间插入0来间接减小步长

* 空洞卷积：

  * 增加输出单元的感受野的方式

    * 增加卷积核大小

    * 增加层数

      以上两种方式会增加参数个数

    * 卷积前进行汇聚操作：会丢失信息

    * 空洞卷积：不增加参数数量，且增加输出单元感受野



## 循环神经网络

#### 概述

* 有限自动状态机、时序数据的处理——》循环神经网络（具有短期记忆，可以接受自身信息）
* 随时间反向传播算法、长程依赖问题 ->门控机制
* 广义的记忆网络模型：递归神经网络、图网络

#### 给网络增加记忆能力

* 延时神经网络TDNN

* 有外部输入的非线性自回归模型

  * 自回归模型AR
  * 有外部输入的非线性自回归模型NARX

* 循环神经网络RNN

  * 输入序列

  * 更新带反馈边的隐藏层的活性值：公式6.4,图6.1

    活性值也称为状态或隐状态

* 动力系统

#### 简单循环网络SRN

* 公式6.5，分别由状态-状态权重矩阵，状态-输入权重矩阵

* 时间维度上权值共享的神经网络

* 循环神经网络的计算能力

  * 通用近似定理

    证明见书p137

  * 图灵完备

#### 应用到机器学习

问题类型

* 序列到类别模式
  * 解决序列数据的分类问题，输入为序列，输出为类别，如文本分类
  * 正常模式和按时间进行平均采样模式
* 同步的序列到序列模式
  * 用于序列标注任务，如词性标注
* 异步的序列到序列模式
  * 编码器-解码器模式，输入与输出序列无严格对应关系，如机器翻译任务
  * 一般模式：先编码，后解码，共两个循环神经网络

#### 参数学习

仍是梯度下降方法，以同步的序列到序列模式为例，但考虑有递归调用函数f()，所以与前馈神经网络有区别

* 随时间反向传播算法BPTT
* 实时循环学习算法RTRL：即前向传播，类比Ch4里的自动微分里的前向模式
* 两种算法比较：更新参数的频度不同

#### 长程依赖问题

* 梯度爆炸问题
  * 权重衰减
  * 梯度截断
* 梯度消失问题
  * 令U=I
  * 或6.50式，但导致新的问题
    * 梯度爆炸
    * 记忆容量问题

#### 基于门控的循环神经网络(Gated RNN)

* 在公式6.50基础上引入门控机制来控制信息的累积速度，包括有选择的加入新的信息，有选择的遗忘之前累积的信息
* 长短期记忆网络(LSTM)
  * 引入新的内部状态$c_t$
  * 门控机制：控制信息传递的路径，包括输入门、遗忘门、输出门
  * 计算过程：p146底部
  * 记忆：长期、短期、长短期
  * 梯度弥散问题：参数设置需设的大
* LSTM网络的各种变体
  * 无遗忘门的LSTM网络：会使得记忆单元不断增大
  * peephole连接：依赖关系变化
  * 耦合输入门和遗忘门
* 门控循环单元网络（GRU）
  * 不要新的记忆单元，只用更新门+重置门

#### 深层循环神经网络

* 深度增加主要指同一时刻网络输入到输出之间的路径$x_t->y_t$
* 堆叠循环神经网络SRNN，也称为循环多层感知机RMLP
* 双向循环神经网络Bi-RNN: 多了一层时间逆序操作

#### 扩展到图结构

* 递归神经网络RecNN：在有向无循环图上的扩展，一般结构为树状的层次结构

  * 主要用来建模自然语言句子的语义，给定一个句子的语法结构，可以用递归神经网络来按照句法组合关系来合成一个句子的语义
  * 树结构的长短期记忆模型：引入门控机制来改进长距离依赖问题


##### 图神经网络GNN

* 有向/无向

* 公式6.79， 6.80, 是同步更新方式所有结构同时接受信息并更新自己的状态

  但对有向图而言，异步更新更有效率

* 读出函数

* 诸如图卷积网络GCN，图注意力网络GAT，消息传递神经网络MPNN

* ![image-20210403220431316](D:\科大\大三下\深度学习导论\image\image-20210403220431316.png)

* NN4G![image-20210403220640293](D:\科大\大三下\深度学习导论\image\image-20210403220640293.png)

![image-20210403220728615](D:\科大\大三下\深度学习导论\image\image-20210403220728615.png)

* DCNN

  ![image-20210403221039292](D:\科大\大三下\深度学习导论\image\image-20210403221039292.png)

  d(3,.)=2表示与该结点相距为2的结点(如与v3相距为2的结点有v1和v3)![image-20210403221241457](D:\科大\大三下\深度学习导论\image\image-20210403221241457.png)

* DGC，与DCNN很像，除了下面

  ![image-20210403221325607](D:\科大\大三下\深度学习导论\image\image-20210403221325607.png)

* MoNET

  ![image-20210403221601328](D:\科大\大三下\深度学习导论\image\image-20210403221601328.png)

* GraphSAGE

  ![image-20210403221651750](D:\科大\大三下\深度学习导论\image\image-20210403221651750.png)

  ![image-20210403221952327](D:\科大\大三下\深度学习导论\image\image-20210403221952327.png)

* GAT（Graph Attention Network）

  ![image-20210403222213111](D:\科大\大三下\深度学习导论\image\image-20210403222213111.png)





## 网络优化与正则化

* 应用神经网络模型的难点
  * 优化问题
  * 泛化问题：过拟合 -> 加入正则化
* 风险最小化准则（书p29-30）
  * 经验风险最小化：训练集上的平均损失
  * 结构风险最小化：加入正则化项后的损失函数

#### 网络优化

* 网络优化定义：指寻找一个神经网络模型来使得经验或结构风险最小化的过程，包括模型选择以及参数学习等
* 神经网络模型的风险最小化是一个非凸优化问题
* 网络结构多样性
* 高维变量的非凸优化
  * 低维空间里非凸优化主要难点：选择初始化参数以及逃离局部最优点
  * 高维空间里：多为 鞍点 -> 随机梯度下降的重要性
* 平坦最小值与尖锐最小值：前者鲁棒性更好
* 局部最小解的等价性
* 神经网络优化的改善方法
  * 目标：找到更好的局部最小值和提高优化效率
  * 方法
    1. 使用更有效的优化算法
    2. 使用更好的参数初始化方法、数据预处理方法
    3. 修改网络结构来得到更好的优化地形
    4. 使用更好的超参数优化方法

#### 优化算法

* 优化算法大体为两类：
  * 调整学习率，使得优化更稳定
  * 梯度估计修正，优化训练速度

* 主要是梯度下降法
  * 批量梯度下降
  * 随机梯度下降
  * 小批量梯度下降：7.2.1
    * 影响因素: 批量大小、学习率、梯度估计

* 批量大小选择：

  * 与学习率设置成 线性缩放规则
  * 回合Epoch与迭代Iteration的关系与定义；
  * 批量大小、学习率与回合、迭代的图示
  * 批量大小也与模型泛化能力有关：批量越大越易收敛到尖锐最小值，越小则对应平坦最小值
  * 见书p161-162

* 学习率调整

  * 学习率衰减/学习率退火：多种衰减函数，见书

  * 学习率预热

  * 周期性学习率调整：周期性增大学习率来更好的逃离鞍点或尖锐最小值

    * 循环学习率
    * 带热重启的随机梯度下降(SGDR)

  * 自适应调整学习率

    * AdaGrad：用每个参数偏导数的累积来平衡学习率

      缺点：多次迭代后学习率会变得很小，若仍未找到最优点，那么后续很难继续找到最优点

    * RMSprop算法：可以有效缓解AdaGrad里学习率的过早衰减问题，引入了指数衰减移动平均

    * AdaDelta算法：在RMSprop的基础上用每次参数更新的差值的平方的指数衰减移动平均代替初始学习率，在一定程度上平抑了学习率的波动

* 梯度估计修正

  * 随机小批量梯度下降法里，批量较小时，损失会呈现震荡的方式下降，即每次迭代的梯度估计与整个训练集上的最优梯度并不一致，有一定随机性  ----->> 考虑用一段时间的平均梯度来代替当前时刻的随机梯度，缓解随机性问题（或增大批量大小来缓解随机性

  * 动量法

    * 即用累积梯度的方式来平衡原来的参数更新值
    * 每个参数的实际更新差值取决于最近一段时间内梯度的加权平均值

  * Nesterov加速梯度（NAG）

    * 与动量法的区别：分两步计算

  * Adam算法：动量法和RMSprop算法的结合

    Nadam算法：Nesterov算法+RMSprop算法

  * 梯度截断：缓解梯度爆炸问题

    * 按值截断
    * 按模截断

* 优化算法总结：p170底部式子

#### 参数初始化

* 参数初始化的3种方式：
  * 预训练初始化：用已在大规模数据上训练过的模型的参数作参数初始值，然后在目标任务上学习(精调Fine-Tuning)
  * 随机初始化：对称权重现象（若全部参数初始化为0）；通常只用在神经网络的权值矩阵上
  * 固定值初始化

下面介绍3种常用的随机初始化方法

* 基于固定方差的参数初始化
  * 高斯分布或均匀分布的初始化
    * 一般固定方差作为预设值，然后在分布里采样为参数初始化
    * 设置方差很关键：过大过小都有问题
    * 一般要配合逐层归一化使用
* 基于方差缩放的参数初始化
  * 根据神经元的性质来差异化设置随机初始化区间
    * 神经元输入连接多，相应输入连接上的权重要小一些
    * 基本思路即根据神经元的连接数量来自适应地调整初始化分布的方差
  * Xavier方法
    * 为了输入信号的方差在经过神经元后不被过分放大或削弱；同时对称的要满足反向传播时不被过分放大或削弱，能得到参数的理想方差，此后再利用高斯分布或均匀分布来随机初始化
  * He初始化
* 正交初始化方法
  * 基于方差的两种方法都是对权重矩阵里每个参数独立采样，所以仍可能有梯度消失或爆炸的问题
  * 考虑权重矩阵整体使得前向、反向传播过程都有范数保持性
  * 步骤：高斯分布初始化权重矩阵；奇异值分解得到正交矩阵
  * 常用在循环神经网络的循环边上的权重矩阵上，并且设置缩放系数

#### 数据预处理

* 算法的尺度不变性

* 神经网络理论上具有尺度不变性，通过参数调整来适应，但是这样会增加训练难度

* 所以输入特征的尺度差异较大时，会对参数初始化、梯度下降法效率造成影响

* 归一化定义：p177

  归一化方法：

  * 最小最大值归一化
  * 标准化（Z值归一化）：将每一维特征都调整为均值为0，方差为1

  * 白化：用来降低输入数据特征之间的冗余性，主要方式是PCA

#### 逐层归一化

* 目的：在神经网络的中间层做数据归一化

  好处：

  * 更好的尺度不变性 ：内部协变量偏移
  * 更平滑的优化地形以及使得大部分神经层输入处于不饱和区域

* 批量归一化(BN)

  * 归一化操作一般在仿射变换之后，激活函数之前

    并且要求效率要高，所以不用白化等方法，而用标准化（且只用小批量来对均值方差做近似)

  * 问题：标准归一化后取值集中到0附近，使得Sigmoid型激活函数时会弱化网络非线性能力

    * 考虑附加缩放和平移操作

  * 计算参数梯度时要考虑均值和方差

  * 逐层归一化不仅可以提高优化效率，同时也是一种隐形的正则化方法

* 层归一化（LN)

  * 批量归一化是对中间层的单个神经元进行归一化，用的是多个样本的相同位置数据
  * 层归一化则是用同一层所有神经元的一个样本数据来做归一化，多用于循环神经网络

* 权重归一化

  * 不对输入做归一化，改为对连接权重做归一化
  * 因为权重共享的缘故，所以权重数量往往比神经元数量少，所以计算开销更小

* 局部响应归一化

  * 多用于卷积
  * 是对邻近的特征映射做局部归一化，且应用于激活函数之后
  * 对标生物神经元里的侧抑制。最大汇聚也有侧抑制作用，但和局部归一化有区别

#### 超参数优化

* 常见的超参数
  * 网络结构：神经元间连接关系、层数、每层神经元数量、激活函数类型
  * 优化参数：优化方法（如梯度估计、参数初始化、数据预处理、逐层归一化等方式的选择）、学习率、小批量的样本数量
  * 正则化系数
* 超参数优化的困难
  * 组合优化问题，无法梯度下降求解
  * 评估一组超参数配置的时间代价很高

常用方法如下

* 网格搜索：遍历所有组合；连续值的超参数需根据参数特点来离散化
* 随机搜索：对超参数进行随机组合

以上两种方法都没有利用不同超参数组合之间的相关性，下面介绍一些自适应的超参数优化方法

* 贝叶斯优化

  * 根据当前已经试验的超参数组合，来预测下一个可能带来最大收益的组合

  * 时序模型优化SMBO

    * 通过定义收益函数（如期望改善），在已经试验的超参数组合基础上选出下一个最好的超参数组合，这个组合对应的收益最大，能使得建模更接近真实分布

    * 算法见书p185

* 动态资源分配

  * 考虑在较早阶段就估计出一组配置的效果是否会比较差，从而早期停止

    如使用一组超参数的学习曲线（因为目前多用随机梯度下降法优化）

  * 可归结为多臂赌博机问题、最优臂问题

  * 逐次减半算法：书p186， N值的选取的分析

* 神经架构搜索

  * 神经网络架构的确定：通过神经网络来自动实现网络架构的设计
  * 利用元学习、强化学习

#### 网络正则化

* 正则化定义：是一类通过限制模型复杂度，从而避免过拟合，提高泛化能力的方法，比如引入约束、增加先验、提前停止等
* 神经网络里的过度参数化问题

常用方法

* L1和L2正则化
  * 即通过约束参数的L1或者L2范数来减小模型在训练数据集上的过拟合现象
  * 等价于带约束条件的优化问题
  * 不同范数约束下的特点：L1约束时最终参数多为稀疏向量
  * 弹性网络正则化
* 权重衰减
* 提前停止
  * 利用验证集来代替期望错误，当验证集上错误率不再下降，则停止迭代
* 丢弃法
  * 掩蔽函数，丢弃掩码
  * 对丢弃法的解释：
    * 集成学习角度
    * 贝叶斯学习角度
  * 循环神经网络上的丢弃法：变分丢弃法
* 数据增强
  * 目前主要应用在图像数据上
    * 旋转、翻转、缩放、平移、加噪声
* 标签平滑
  * 防止因训练数据集标签存在错误标注而导致过拟合
  * 硬目标、软目标
  * 若按照类别相关性赋予其他标签不同的概率：教师网络+学生网络 -> 知识蒸馏

## 注意力机制与外部记忆

* 为减少计算复杂度而引入局部连接、权重共享、汇聚操作等
* 网络容量
* 注意力和记忆机制

#### 认知神经学中的注意力

* 自上而下的有意识的注意力：聚焦式注意力
* 自下而上的无意识的注意力：基于显著性的注意力

#### 注意力机制

* 最大汇聚、门控机制都可视为基于显著性的注意力
* 聚焦式注意力：如阅读理解里只选取与提问相关的片段来处理
  * 注意力机制的计算
    * 在所有输入信息上计算注意力分布
    * 根据注意力分布来计算输入信息的加权平均
  * 查询向量、打分函数
  * 软性注意力机制
* 注意力机制的变体
  * 硬性注意力
  * 键值对注意力
  * 多头注意力：多个查询并行地从输入信息中选取多组信息
  * 结构化注意力：输入信息本身具有层次结构
  * 指针网络：只用计算注意力分布，将其作为软性指针来指出相关信息的位置
    * 即p140的异步的序列到序列模式
    * 输入为向量序列，输出为这些向量的部分下标序列

#### 自注意力模型

* 建立变长输入序列间的长距离依赖关系：既能获取远距离的信息交互，又能根据输入序列长度动态生成权重

* 模型采用 查询-键-值模式(QKV)

* 注意事项：p205 最后一段

* Transformer详解：ppt 48 以及 https://zhuanlan.zhihu.com/p/90033981文章

  * 重点理解：

    * Encoder 一次性把句子里所有词输入(每个词提前编码为一个向量)，然后输出为每个词对应的K和V
  
* 在Decoder里，会把该句子的翻译作为输入(仍是每个词一个向量)，但在做self-attention时，需要masked掉未来的输出，比如在计算位置4对应的词向量的输出A时，只能用位置4的query对位置1,2,3,4的K和V来做查询（且注意到位置1,2,3,4的输入其实对应翻译的0,1,2,3 ,0指的起始符号BOS）。  这样用A作为Encoder-Decoder Attention层的query来对Encoder里传过来的翻译前每个词的K和V做查询，然后得到位置4的翻译结果输出。
  
      decoder中的self-attention层允许decoder中的每个位置都关注decoder层中当前位置之前的所有位置（包括当前位置）。 为了保持解码器的自回归特性，需要防止解码器中的信息向左流动。我们在scaled dot-product attention的内部 ，通过屏蔽softmax输入中所有的非法连接值（设置为 −∞）实现了这一点。
  
    
  

#### 人脑中的记忆

* 信息存储的整体效应：记忆在大脑皮层是分布式存储的
* 人脑中的记忆具有周期性和联想性
* 记忆周期：短期记忆、长期记忆（结构记忆、知识）、工作记忆
* 联想记忆：基于内容寻址的存储CAM
  * 外部记忆在DNN中的实现
    * 结构化的记忆
    * 基于神经动力学的联想记忆

#### 记忆增强神经网络MANN

* 记忆网络的典型结构：书p207
  * 主网络C：也称控制器
  * 外部记忆单元M：分为很多记忆片段
  * 读取模块R
  * 写入模块W
  * 可按内容寻址的注意力机制实现读取和写入

常见的结构化外部记忆模型

* 端到端记忆网络(MemN2N)
  * 外部记忆单元是只读的
  * 主网络根据输入x生成查询向量q，然后利用键值对注意力机制和q去外部记忆里读取信息r，然后合并q和r产生输出
  * 也可有多跳操作：见p209 图8.7
* 神经图灵机（NTM）
  * 外部记忆+控制器
  * 查询向量、删除向量、增加向量、注意力分布机制
  * 读、写操作

#### 基于神经动力学的联想记忆

* 联想记忆模型：通过神经网络的动态演化来进行联想
  * 输入输出的模式在同一空间：自联想模型，用前馈或循环神经网络实现，又称自编码器(AE)
  * 输入输出的模式不在同一空间: 异联想模型，可视为分类器，(适用于大部分机器学习问题
* Hopfield网络
  * 是一种循环神经网络模型，所有神经元都互相连接、且不分层，无隐藏神经元
  * 更新规则：（离散和连续的Hopfield网络)
    * 异步或同步更新
  * 能量函数
    * 能量函数经多次迭代后会达到收敛状态
    * 给定一个外部输入，网络经过演化，会达到某个稳定状态，称为吸引点（一个网络有多个吸引点，代表能量的局部最优点)
  * 联想记忆：吸引点、存储的模式、检索过程
  * 信息存储：赫布规则。存储过程实际上是调整神经元间的连接权重
  * 存储容量：联想记忆模型的存储容量为其能够可靠的存储和检索模式的最大数量
* 使用联想记忆增加网络容量
  * 把联想记忆模型作为部件引入LSTM
  * 或将循环神经网络的部分连接权重作为短期记忆，用联想记忆模型来更新



## 图卷积网络

原文：https://www.yanxishe.com/TextTranslation/2793?from=jianshu

* Graph上的任务

  - 节点分类：预测特定节点的类型。
  - 链接预测：预测两个节点是否有联系
  - 社区检测：识别密集联系的节点群落。
  - 网络相似性：两个(子)网络的相似性有多大？ 

* GCN：基于论文：Paper: [Semi-supervised Classification with Graph Convolutional Networks](https://arxiv.org/pdf/1609.02907.pdf) (2017) [3]

  GCN是一种卷积神经网络，它可以直接在图上工作，并利用图的结构信息。

  它解决的是对图（如引文网络）中的节点（如文档）进行分类的问题，其中仅有一小部分节点有标签（半监督学习）

* GCN的基本思路：对于每个节点，我们从它的所有邻居节点处获取其特征信息，当然也包括它自身的特征。假设我们使用average()函数。我们将对所有的节点进行同样的操作。最后，我们将这些计算得到的平均值输入到神经网络中。

  在下图中，我们有一个引文网络的简单实例。其中每个节点代表一篇研究论文，同时边代表的是引文。我们在这里有一个预处理步骤。在这里我们不使用原始论文作为特征，而是将论文转换成向量（通过使用NLP嵌入，例如tf-idf）。NLP嵌入，例如TF-IDF)。

  让我们考虑下绿色节点。首先，我们得到它的所有邻居的特征值，包括自身节点，接着取平均值。最后通过神经网络返回一个结果向量并将此作为最终结果。

  ![img](https://static.leiphone.com/uploads/new/sns/article/202009/1599636334366628.png)

  GCN的主要思想。我们以绿色节点为例。首先，我们取其所有邻居节点的平均值，包括自身节点。然后，将平均值通过神经网络。请注意，在GCN中，我们仅仅使用一个全连接层。在这个例子中，我们得到2维向量作为输出（全连接层的2个节点）。

  在实际操作中，我们可以使用比average函数更复杂的聚合函数。我们还可以将更多的层叠加在一起，以获得更深的GCN。其中每一层的输出会被视为下一层的输入。

  ![img](https://static.leiphone.com/uploads/new/sns/article/202009/1599636337152463.png)

  2层GCN的例子：第一层的输出是第二层的输入。同样，注意GCN中的神经网络仅仅是一个全连接层（图片来自[2]）。

* 数学原理：首先，我们需要一些注解

  ![img](https://static.leiphone.com/uploads/new/sns/article/202009/1599636340109939.png)我们考虑图G，如下图所示。

  ![img](https://static.leiphone.com/uploads/new/sns/article/202009/1599636335734727.png)从图G中，我们有一个邻接矩阵A和一个度矩阵D。同时我们也有特征矩阵X。

  
  那么我们怎样才能从邻居节点处得到每一个节点的特征值呢？解决方法就在于A和X的相乘。

  看看邻接矩阵的第一行，我们看到节点A与节点E之间有连接，得到的矩阵第一行就是与A相连接的E节点的特征向量（如下图）。同理，得到的矩阵的第二行是D和E的特征向量之和，通过这个方法，我们可以得到所有邻居节点的向量之和。

  ![img](https://static.leiphone.com/uploads/new/sns/article/202009/1599636339582585.png)

  计算 "和向量矩阵 "AX的第一行。

  - 这里还有一些需要改进的地方。

  1. 我们忽略了节点本身的特征。例如，计算得到的矩阵的第一行也应该包含节点A的特征。
  2. 我们不需要使用sum()函数，而是需要取平均值，甚至更好的邻居节点特征向量的加权平均值。那我们为什么不使用sum()函数呢？原因是在使用sum()函数时，度大的节点很可能会生成的大的v向量，而度低的节点往往会得到小的聚集向量，这可能会在以后造成梯度爆炸或梯度消失（例如，使用sigmoid时）。此外，神经网络似乎对输入数据的规模很敏感。因此，我们需要对这些向量进行归一化，以摆脱可能出现的问题。

  在问题（1）中，我们可以通过在A中增加一个单位矩阵I来解决，得到一个新的邻接矩阵Ã。

  ![img](https://static.leiphone.com/uploads/new/sns/article/202009/1599636328706132.png) 

  取*lambda=1*（使得节点本身的特征和邻居一样重要），我们就有Ã=A+I，注意，我们可以把*lambda*当做一个可训练的参数，但现在只要把lambda赋值为1就可以了，即使在论文中，lambda也只是简单的赋值为1。

  ![img](https://static.leiphone.com/uploads/new/sns/article/202009/1599636337384276.png) 通过给每个节点增加一个自循环，我们得到新的邻接矩阵

  对于问题(2): 对于矩阵缩放，我们通常将矩阵乘以对角线矩阵。在当前的情况下，我们要取聚合特征的平均值，或者从数学角度上说，要根据节点度数对*聚合向量矩阵**ÃX*进行缩放。直觉告诉我们这里用来缩放的对角矩阵是和度矩阵D̃有关的东西（为什么是D̃，而不是D？因为我们考虑的是新邻接矩阵*Ã* 的度矩阵D̃，而不再是*A*了）。

* 现在的问题变成了*我们要如何对和向量进行缩放/归一化*？换句话说

  > **我们如何将邻居的信息传递给特定节点？**

  我们从我们的老朋友*average*开始。在这种情况下，D̃的逆矩阵（即，D̃^{-1}）就会用起作用。基本上，D̃的逆矩阵中的每个元素都是对角矩阵D中相应项的倒数。

  ![img](https://static.leiphone.com/uploads/new/sns/article/202009/1599636337384522.png)例如，节点A的度数为2，所以我们将节点A的聚合向量乘以1/2，而节点E的度数为5，我们应该将E的聚合向量乘以1/5，以此类推。

  因此，通过D̃取反和X的乘法，我们可以取所有邻居节点的特征向量（包括自身节点）的平均值。

  

  ![img](https://static.leiphone.com/uploads/new/sns/article/202009/1599636338960217.png) 

  到目前为止一切都很好。但是你可能会问*加权平均()怎么样？*直觉上，如果我们对高低度的节点区别对待，应该会更好。

  ![img](https://static.leiphone.com/uploads/new/sns/article/202009/1599636334457390.png)

  ![img](https://static.leiphone.com/uploads/new/sns/article/202009/1599636330297147.png)

   但我们只是按行缩放，但忽略了对应的列（虚线框）。 

  ![img](https://static.leiphone.com/uploads/new/sns/article/202009/1599636332288474.png)

  ![img](https://static.leiphone.com/uploads/new/sns/article/202009/1599636336222725.png)  

  为列增加一个新的缩放器。

  新的缩放方法给我们提供了 "加权 "的平均值。我们在这里做的是给低度的节点加更多的权重，以减少高度节点的影响。这个加权平均的想法是，我们假设低度节点会对邻居节点产生更大的影响，而高度节点则会产生较低的影响，因为它们的影响力分散在太多的邻居节点上。

  ![img](https://static.leiphone.com/uploads/new/sns/article/202009/1599636340163238.png)在节点B处聚合邻接节点特征时，我们为节点B本身分配最大的权重（度数为3），为节点E分配最小的权重（度数为5）。

  ![img](https://static.leiphone.com/uploads/new/sns/article/202009/1599636335960051.png)

  ![img](https://static.leiphone.com/uploads/new/sns/article/202009/1599636335541999.png) 因为我们归一化了两次，所以将"-1 "改为"-1/2"

  ![img](https://static.leiphone.com/uploads/new/sns/article/202009/1599636335215956.png) ![img](https://static.leiphone.com/uploads/new/sns/article/202009/1599636338181056.png)

  例如，我们有一个多分类问题，有10个类，*F* 被设置为10。在第2层有了10个维度的向量后，我们将这些向量通过一个softmax函数进行预测。

  Loss函数的计算方法很简单，就是通过对所有有标签的例子的交叉熵误差来计算，其中*Y_{l}*是有标签的节点的集合。  

  ![img](https://static.leiphone.com/uploads/new/sns/article/202009/1599636329874326.png)

* 层数

  ## #layers的含义

  层数是指节点特征能够传输的最远距离。例如，在1层的GCN中，每个节点只能从其邻居那里获得信息。每个节点收集信息的过程是独立进行的，对所有节点来说都是在同一时间进行的。

  当在第一层的基础上再叠加一层时，我们重复收集信息的过程，但这一次，邻居节点已经有了自己的邻居的信息（来自上一步）。这使得层数成为每个节点可以走的最大跳步。所以，这取决于我们认为一个节点应该从网络中获取多远的信息，我们可以为#layers设置一个合适的数字。但同样，在图中，通常我们不希望走得太远。设置为6-7跳，我们就几乎可以得到整个图，但是这就使得聚合的意义不大。

  ![img](https://static.leiphone.com/uploads/new/sns/article/202009/1599636335287944.png) 例： 收集目标节点 i 的两层信息的过程

* **GCN应该叠加几层？**

  在论文中，作者还分别对浅层和深层的GCN进行了一些实验。在下图中，我们可以看到，使用2层或3层的模型可以得到最好的结果。此外，对于深层的GCN（超过7层），反而往往得到不好的性能（虚线蓝色）。一种解决方案是借助隐藏层之间的残余连接（紫色线）。

  ![img](https://static.leiphone.com/uploads/new/sns/article/202009/1599636336826408.png) 不同层数#的性能。图片来自论文[3]

* 备注：

  - GCNs用于图上的半监督学习。

  - GCNs同时使用节点特征和结构进行训练

  - GCN的主要思想是取所有邻居节点特征（包括自身节点）的加权平均值。度低的节点获得更大的权重。之后，我们将得到的特征向量通过神经网络进行训练。

  - 我们可以堆叠更多的层数来使GCN更深。考虑深度GCNs的残差连接。通常，我们会选择2层或3层的GCN。

  - 数学笔记：当看到对角线矩阵时，要想到矩阵缩放。

  - 这里有[一个使用StellarGraph库的GCN演示[5\]](https://stellargraph.readthedocs.io/en/stable/demos/node-classification/gcn-node-classification.html)。该仓库还提供了许多其他GNN的算法。

   论文作者的说明

  > 该框架目前仅限于无向图（加权或不加权）。但是，可以通过将原始有向图表示为一个无向的两端图，并增加代表原始图中边的节点，来处理有向边和边特征。